# å¤š LLM ä½¿ç”¨æŒ‡å—

> å¦‚ä½•é…ç½®å’Œä½¿ç”¨ Qwenã€DeepSeekã€Doubao ä¸‰å¤§æ¨¡å‹åä½œ

## 1. è®¾è®¡ç†å¿µ

Creative AutoGPT é‡‡ç”¨**å¤š LLM æ™ºèƒ½åˆ†å·¥**ç­–ç•¥ï¼Œè®©ä¸åŒæ¨¡å‹å‘æŒ¥å„è‡ªä¼˜åŠ¿ï¼š

```
ğŸ§  Qwen (Aliyun)     â†’ æ€»è§ˆè§„åˆ’ã€é•¿æœŸè®°å¿†
ğŸ” DeepSeek          â†’ é€»è¾‘æ¨ç†ã€è´¨é‡è¯„ä¼°
âœ¨ Doubao (ç«å±±å¼•æ“) â†’ åˆ›æ„æ–‡ç¬”ã€å†…å®¹ç”Ÿæˆ
```

---

## 2. å¿«é€Ÿå¼€å§‹

### 2.1 è·å– API Keys

#### Qwen (é˜¿é‡Œäº‘é€šä¹‰åƒé—®)

1. è®¿é—® [é˜¿é‡Œäº‘ç™¾ç‚¼å¹³å°](https://bailian.console.aliyun.com/)
2. å¼€é€šæœåŠ¡å¹¶åˆ›å»º API Key
3. å¤åˆ¶ API Key

#### DeepSeek

1. è®¿é—® [DeepSeek å¹³å°](https://platform.deepseek.com/)
2. æ³¨å†Œå¹¶å……å€¼ï¼ˆæ€§ä»·æ¯”æé«˜ï¼Œå»ºè®®å……å€¼ ï¿¥50ï¼‰
3. åˆ›å»º API Key

#### Doubao (ç«å±±å¼•æ“è±†åŒ…)

1. è®¿é—® [ç«å±±å¼•æ“æ§åˆ¶å°](https://console.volcengine.com/ark)
2. å¼€é€šæ¨¡å‹æ¨ç†æœåŠ¡
3. åˆ›å»º API Key

---

### 2.2 é…ç½® API Keys

ç¼–è¾‘ `.env` æ–‡ä»¶ï¼š

```bash
# Qwen (Aliyun)
ALIYUN_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
ALIYUN_MODEL=qwen-max                # æ¨èæ¨¡å‹

# DeepSeek
DEEPSEEK_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
DEEPSEEK_BASE_URL=https://api.deepseek.com/v1
DEEPSEEK_MODEL=deepseek-chat         # æ¨èæ¨¡å‹

# Doubao (ç«å±±å¼•æ“)
ARK_API_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
ARK_BASE_URL=https://ark.cn-beijing.volces.com/api/v3
ARK_MODEL=doubao-pro-32k            # æ¨èæ¨¡å‹

# å¯ç”¨é…ç½®
ENABLE_QWEN=true
ENABLE_DEEPSEEK=true
ENABLE_DOUBAO=true
```

---

## 3. ä»»åŠ¡åˆ†å·¥è¯¦è§£

### 3.1 ä»»åŠ¡è·¯ç”±è¡¨

| ä»»åŠ¡ç±»å‹ | åˆ†é…æ¨¡å‹ | åŸå›  |
|---------|---------|------|
| **å¤§çº²** | Qwen | éœ€è¦å…¨å±€è§†é‡å’Œé•¿ä¸Šä¸‹æ–‡ |
| **é£æ ¼å…ƒç´ ** | Qwen | éœ€è¦ä¿æŒå…¨ä¹¦é£æ ¼ä¸€è‡´æ€§ |
| **äººç‰©è®¾è®¡** | Qwen | éœ€è¦è®°ä½æ‰€æœ‰äººç‰©å…³ç³»ç½‘ |
| **ä¸–ç•Œè§‚è§„åˆ™** | Qwen | éœ€è¦æ„å»ºå®Œæ•´è®¾å®šä½“ç³» |
| **äº‹ä»¶** | DeepSeek | éœ€è¦é€»è¾‘æ¨ç†å’Œå› æœé“¾ |
| **åœºæ™¯ç‰©å“å†²çª** | DeepSeek | éœ€è¦ç»“æ„åŒ–æ€è€ƒ |
| **è¯„ä¼°** | DeepSeek | éœ€è¦å®¢è§‚åˆ†æèƒ½åŠ› |
| **ä¸€è‡´æ€§æ£€æŸ¥** | DeepSeek | éœ€è¦æ£€æµ‹é€»è¾‘æ¼æ´ |
| **ç« èŠ‚å†…å®¹** | Doubao | éœ€è¦ä¼˜ç¾æ–‡ç¬”å’Œåˆ›æ„ |
| **ä¿®è®¢æ¶¦è‰²** | Doubao | éœ€è¦æ–‡å­¦åˆ›ä½œèƒ½åŠ› |
| **å¯¹è¯ç”Ÿæˆ** | Doubao | éœ€è¦ç”ŸåŠ¨è‡ªç„¶çš„å¯¹è¯ |

---

### 3.2 æ¨¡å‹ç‰¹æ€§å¯¹æ¯”

#### ğŸ§  Qwen - æ€»è§ˆè®°å¿†ä¸“å®¶

**æ ¸å¿ƒä¼˜åŠ¿**
- âœ… é•¿ä¸Šä¸‹æ–‡çª—å£ï¼ˆ200K+ tokensï¼‰
- âœ… å¼ºå¤§çš„é•¿æœŸè®°å¿†èƒ½åŠ›
- âœ… å…¨å±€ä¸€è‡´æ€§æŠŠæ§å¥½

**é€‚åˆåœºæ™¯**
```python
# ç”Ÿæˆå¤§çº²
outline = await generate_outline(
    style="ç„å¹»ä¿®ä»™",
    theme="å°‘å¹´æˆé•¿",
    chapter_count=500,
    llm="qwen"  # ä½¿ç”¨ Qwen
)

# è®¾è®¡äººç‰©
characters = await design_characters(
    count=25,
    relationships_complex=True,
    llm="qwen"  # éœ€è¦è®°ä½æ‰€æœ‰å…³ç³»
)
```

**å‚æ•°å»ºè®®**
```python
{
    "model": "qwen-max",
    "temperature": 0.7,      # åˆ›é€ æ€§é€‚ä¸­
    "max_tokens": 4000,      # å¤§çº²é€šå¸¸è¾ƒé•¿
    "top_p": 0.9
}
```

---

#### ğŸ” DeepSeek - é€»è¾‘ç»“æ„ä¸“å®¶

**æ ¸å¿ƒä¼˜åŠ¿**
- âœ… å¼ºå¤§çš„é€»è¾‘æ¨ç†èƒ½åŠ›
- âœ… å› æœå…³ç³»åˆ†æå‡†ç¡®
- âœ… **æ€§ä»·æ¯”æé«˜**ï¼ˆé‡è¦ï¼ï¼‰

**é€‚åˆåœºæ™¯**
```python
# è®¾è®¡äº‹ä»¶é“¾
events = await design_events(
    chapter_range=(1, 10),
    ensure_causality=True,
    llm="deepseek"  # é€»è¾‘æ¨ç†å¼º
)

# è¯„ä¼°å†…å®¹è´¨é‡
evaluation = await evaluate_content(
    content=chapter_content,
    criteria=["structure", "consistency"],
    llm="deepseek"  # å®¢è§‚åˆ†æ
)
```

**å‚æ•°å»ºè®®**
```python
{
    "model": "deepseek-chat",
    "temperature": 0.5,      # é€»è¾‘ä»»åŠ¡éœ€è¦ç¨³å®š
    "max_tokens": 2000,
    "top_p": 0.85
}
```

---

#### âœ¨ Doubao - åˆ›æ„æ–‡ç¬”ä¸“å®¶

**æ ¸å¿ƒä¼˜åŠ¿**
- âœ… æ–‡å­¦åˆ›ä½œèƒ½åŠ›å¼º
- âœ… æ–‡ç¬”ä¼˜ç¾æµç•…
- âœ… å¯¹è¯ç”ŸåŠ¨è‡ªç„¶

**é€‚åˆåœºæ™¯**
```python
# ç”Ÿæˆç« èŠ‚å†…å®¹
chapter = await generate_chapter(
    outline="å°‘å¹´åˆå…¥ä¿®ä»™ç•Œ...",
    target_words=3000,
    style="ä¼˜ç¾æŠ’æƒ…",
    llm="doubao"  # æ–‡ç¬”æœ€å¥½
)

# æ¶¦è‰²ä¿®è®¢
polished = await polish_content(
    content=draft_content,
    improvements=["å¢å¼ºæå†™", "ä¸°å¯Œå¯¹è¯"],
    llm="doubao"  # åˆ›æ„å¼º
)
```

**å‚æ•°å»ºè®®**
```python
{
    "model": "doubao-pro-32k",
    "temperature": 0.8,      # åˆ›ä½œéœ€è¦é«˜åˆ›é€ æ€§
    "max_tokens": 4000,
    "top_p": 0.95
}
```

---

## 4. é…ç½®ç¤ºä¾‹

### 4.1 é¡¹ç›®é…ç½®æ–‡ä»¶

**config/llm_config.yaml**

```yaml
llm:
  # Qwen é…ç½®
  qwen:
    enabled: true
    provider: aliyun
    model: qwen-max
    api_key_env: ALIYUN_API_KEY
    default_params:
      temperature: 0.7
      max_tokens: 4000
      top_p: 0.9
    rate_limit:
      rpm: 60          # æ¯åˆ†é’Ÿè¯·æ±‚æ•°
      tpm: 200000      # æ¯åˆ†é’Ÿ tokens
    timeout: 120       # ç§’
    
  # DeepSeek é…ç½®
  deepseek:
    enabled: true
    provider: openai_compatible
    model: deepseek-chat
    base_url: https://api.deepseek.com/v1
    api_key_env: DEEPSEEK_API_KEY
    default_params:
      temperature: 0.5
      max_tokens: 2000
      top_p: 0.85
    rate_limit:
      rpm: 100
      tpm: 500000
    timeout: 90
    
  # Doubao é…ç½®
  doubao:
    enabled: true
    provider: volcengine
    model: doubao-pro-32k
    base_url: https://ark.cn-beijing.volces.com/api/v3
    api_key_env: ARK_API_KEY
    default_params:
      temperature: 0.8
      max_tokens: 4000
      top_p: 0.95
    rate_limit:
      rpm: 60
      tpm: 150000
    timeout: 150

# ä»»åŠ¡è·¯ç”±é…ç½®
routing:
  # è§„åˆ’ç±»ä»»åŠ¡
  planning:
    - outline
    - style_elements
    - character_design
    - worldview
    default_llm: qwen
    
  # é€»è¾‘ç±»ä»»åŠ¡
  logic:
    - events
    - scenes
    - conflicts
    - evaluation
    - consistency_check
    default_llm: deepseek
    
  # åˆ›ä½œç±»ä»»åŠ¡
  creation:
    - chapter_content
    - revision
    - polish
    - dialogue
    default_llm: doubao
```

---

### 4.2 Python ä»£ç é…ç½®

**ä½¿ç”¨é…ç½®æ–‡ä»¶**

```python
from creative_autogpt.config import LLMConfig

# åŠ è½½é…ç½®
config = LLMConfig.from_yaml("config/llm_config.yaml")

# è·å–ç‰¹å®šæ¨¡å‹é…ç½®
qwen_config = config.get_llm_config("qwen")
print(f"Qwen Model: {qwen_config.model}")
print(f"Temperature: {qwen_config.default_params.temperature}")
```

**åŠ¨æ€é…ç½®**

```python
from creative_autogpt.utils.llm_client import MultiLLMClient

# åˆ›å»ºå®¢æˆ·ç«¯
client = MultiLLMClient()

# ä¸ºç‰¹å®šä¼šè¯è‡ªå®šä¹‰é…ç½®
client.configure_session(
    session_id="sess_123",
    overrides={
        "qwen": {
            "temperature": 0.8,  # æé«˜åˆ›é€ æ€§
        },
        "deepseek": {
            "enabled": False,    # ä¸´æ—¶ç¦ç”¨
        }
    }
)
```

---

## 5. ä½¿ç”¨ç¤ºä¾‹

### 5.1 åŸºç¡€ä½¿ç”¨

```python
from creative_autogpt.core.loop_engine import LoopEngine
from creative_autogpt.modes.novel import NovelMode

# åˆå§‹åŒ–å¼•æ“
engine = LoopEngine(
    mode=NovelMode(),
    session_id="sess_123"
)

# åˆ›å»ºå°è¯´
result = await engine.run(
    goal={
        "style": "ç„å¹»ä¿®ä»™",
        "theme": "å°‘å¹´æˆé•¿",
        "target_words": 100000,
        "chapter_count": 50
    }
)

# å¼•æ“ä¼šè‡ªåŠ¨è·¯ç”±ä»»åŠ¡åˆ°åˆé€‚çš„ LLM
# - å¤§çº² â†’ Qwen
# - äº‹ä»¶ â†’ DeepSeek
# - ç« èŠ‚ â†’ Doubao
```

---

### 5.2 æ‰‹åŠ¨æŒ‡å®šæ¨¡å‹

```python
from creative_autogpt.utils.llm_client import MultiLLMClient

client = MultiLLMClient()

# å¼ºåˆ¶ä½¿ç”¨ç‰¹å®šæ¨¡å‹
outline = await client.generate(
    task_type="outline",
    prompt="åˆ›ä½œä¸€ä¸ªç„å¹»å°è¯´å¤§çº²...",
    llm="qwen",              # æ˜ç¡®æŒ‡å®š
    temperature=0.8
)

# å°è¯•å¤šä¸ªæ¨¡å‹ï¼ˆå®¹é”™ï¼‰
chapter = await client.generate_with_fallback(
    task_type="chapter",
    prompt="å†™ç¬¬ä¸€ç« ...",
    preferred_llm="doubao",
    fallback_llms=["qwen", "deepseek"]  # å¤±è´¥æ—¶å°è¯•å¤‡é€‰
)
```

---

### 5.3 æ‰¹é‡ä»»åŠ¡

```python
from creative_autogpt.utils.llm_client import MultiLLMClient
import asyncio

client = MultiLLMClient()

# å¹¶å‘è°ƒç”¨ä¸åŒæ¨¡å‹
tasks = [
    client.generate(task_type="outline", llm="qwen", ...),
    client.generate(task_type="events", llm="deepseek", ...),
    client.generate(task_type="chapter", llm="doubao", ...)
]

results = await asyncio.gather(*tasks)
```

---

## 6. æˆæœ¬ä¼˜åŒ–

### 6.1 æˆæœ¬å¯¹æ¯”

| æ¨¡å‹ | è¾“å…¥ä»·æ ¼ | è¾“å‡ºä»·æ ¼ | ç›¸å¯¹æˆæœ¬ |
|------|---------|---------|---------|
| **Qwen Max** | ï¿¥0.04/1K | ï¿¥0.12/1K | ä¸­ç­‰ |
| **DeepSeek** | ï¿¥0.001/1K | ï¿¥0.002/1K | **æä½** â­ |
| **Doubao Pro** | ï¿¥0.008/1K | ï¿¥0.008/1K | ä½ |

### 6.2 ä¼˜åŒ–ç­–ç•¥

**ç­–ç•¥ 1ï¼šé«˜é¢‘ä»»åŠ¡ç”¨ DeepSeek**

```python
# è¯„ä¼°ä»»åŠ¡éå¸¸é¢‘ç¹ï¼Œä½¿ç”¨ DeepSeek
routing_config = {
    "evaluation": "deepseek",      # æ¯ä¸ªä»»åŠ¡éƒ½è¯„ä¼°
    "consistency_check": "deepseek" # é¢‘ç¹æ£€æŸ¥
}
```

**ç­–ç•¥ 2ï¼šé™åˆ¶ token ä½¿ç”¨**

```python
# æ§åˆ¶è¾“å‡ºé•¿åº¦
client.generate(
    task_type="chapter",
    llm="doubao",
    max_tokens=3500,     # é™åˆ¶æœ€å¤§ tokens
    target_words=3000    # æ˜ç¡®å­—æ•°è¦æ±‚
)
```

**ç­–ç•¥ 3ï¼šç¼“å­˜å¸¸ç”¨ç»“æœ**

```python
from creative_autogpt.utils.cache import LLMCache

cache = LLMCache()

# å°è¯•ä»ç¼“å­˜è·å–
cached_result = cache.get(prompt_hash)
if cached_result:
    return cached_result

# æœªå‘½ä¸­ç¼“å­˜æ‰è°ƒç”¨ LLM
result = await client.generate(...)
cache.set(prompt_hash, result)
```

**ç­–ç•¥ 4ï¼šæ™ºèƒ½é‡è¯•**

```python
# å…ˆç”¨ä¾¿å®œçš„æ¨¡å‹ï¼Œå¤±è´¥å†ç”¨è´µçš„
result = await client.generate_with_fallback(
    task_type="chapter",
    preferred_llm="deepseek",     # å…ˆç”¨ DeepSeekï¼ˆä¾¿å®œï¼‰
    fallback_llms=["doubao"],     # å¤±è´¥å†ç”¨ Doubao
    quality_threshold=7.0          # è´¨é‡é˜ˆå€¼
)
```

---

## 7. ç›‘æ§ä¸è°ƒè¯•

### 7.1 æŸ¥çœ‹ LLM è°ƒç”¨ç»Ÿè®¡

```python
from creative_autogpt.utils.monitoring import LLMMonitor

monitor = LLMMonitor()

# è·å–ç»Ÿè®¡
stats = monitor.get_stats(session_id="sess_123")

print(f"Qwen è°ƒç”¨æ¬¡æ•°: {stats['qwen']['calls']}")
print(f"DeepSeek æ€»æˆæœ¬: ï¿¥{stats['deepseek']['total_cost']:.2f}")
print(f"Doubao å¹³å‡å“åº”æ—¶é—´: {stats['doubao']['avg_latency']:.2f}s")
```

---

### 7.2 å®æ—¶ç›‘æ§

**WebSocket äº‹ä»¶**

```javascript
// å‰ç«¯ç›‘å¬ LLM è°ƒç”¨äº‹ä»¶
ws.on('llm.llm_call_completed', (data) => {
  console.log(`${data.provider} å®Œæˆè°ƒç”¨`);
  console.log(`è€—æ—¶: ${data.elapsed_time}s`);
  console.log(`Tokens: ${data.tokens_used.total_tokens}`);
  console.log(`æˆæœ¬: $${data.cost.toFixed(4)}`);
});
```

---

### 7.3 æ—¥å¿—æŸ¥çœ‹

```bash
# æŸ¥çœ‹ LLM è°ƒç”¨æ—¥å¿—
tail -f logs/app.log | grep "LLM_CALL"

# ç¤ºä¾‹è¾“å‡º
# 2026-01-23 10:05:30 | INFO | LLM_CALL | qwen | outline | START
# 2026-01-23 10:07:30 | INFO | LLM_CALL | qwen | outline | SUCCESS | 120s | 3500 tokens | ï¿¥0.42
```

---

## 8. æ•…éšœå¤„ç†

### 8.1 æ¨¡å‹ä¸å¯ç”¨

**é—®é¢˜**ï¼šæŸä¸ªæ¨¡å‹ API å¤±è´¥

**è‡ªåŠ¨å®¹é”™**

```python
# ç³»ç»Ÿä¼šè‡ªåŠ¨ä½¿ç”¨å¤‡é€‰æ¨¡å‹
result = await client.generate(
    task_type="chapter",
    llm="doubao",
    auto_fallback=True  # å¯ç”¨è‡ªåŠ¨å®¹é”™
)

# å¦‚æœ Doubao å¤±è´¥ï¼Œä¼šå°è¯•ï¼š
# 1. Qwen (åŒæ ·æ“…é•¿åˆ›ä½œ)
# 2. DeepSeek (æœ€åçš„å¤‡é€‰)
```

---

### 8.2 é¢‘ç‡é™åˆ¶

**é—®é¢˜**ï¼šè¶…å‡º API è°ƒç”¨é¢‘ç‡é™åˆ¶

**è§£å†³æ–¹æ¡ˆ**

```python
# é…ç½®é¢‘ç‡é™åˆ¶å™¨
client.configure_rate_limiter(
    "qwen",
    max_rpm=60,        # æ¯åˆ†é’Ÿæœ€å¤š 60 æ¬¡
    max_tpm=200000,    # æ¯åˆ†é’Ÿæœ€å¤š 200K tokens
    strategy="wait"    # è¶…é™æ—¶ç­‰å¾…ï¼ˆè€Œéå¤±è´¥ï¼‰
)
```

---

### 8.3 è´¨é‡ä¸è¾¾æ ‡

**é—®é¢˜**ï¼šç”Ÿæˆå†…å®¹è´¨é‡ä¸ç¬¦åˆé¢„æœŸ

**åˆ‡æ¢æ¨¡å‹**

```python
# å¦‚æœ DeepSeek ç”Ÿæˆçš„ç« èŠ‚è´¨é‡ä¸å¤Ÿ
# å¯ä»¥ä¸´æ—¶æ”¹ç”¨ Doubao
session_config = {
    "chapter_content": "doubao"  # è¦†ç›–é»˜è®¤è·¯ç”±
}

engine.update_routing(session_config)
```

---

## 9. æœ€ä½³å®è·µ

### 9.1 ä»»åŠ¡åˆ†é…åŸåˆ™

âœ… **DO**
- å¤§çº²ã€è®¾å®š â†’ Qwenï¼ˆéœ€è¦å…¨å±€è§†é‡ï¼‰
- è¯„ä¼°ã€æ£€æŸ¥ â†’ DeepSeekï¼ˆå®¢è§‚ä¸”ä¾¿å®œï¼‰
- ç« èŠ‚ã€æ¶¦è‰² â†’ Doubaoï¼ˆæ–‡ç¬”æœ€å¥½ï¼‰

âŒ **DON'T**
- ä¸è¦ç”¨ Qwen ç”Ÿæˆæ‰€æœ‰å†…å®¹ï¼ˆæˆæœ¬é«˜ï¼‰
- ä¸è¦ç”¨ DeepSeek å†™ç« èŠ‚ï¼ˆæ–‡ç¬”ä¸å¦‚ Doubaoï¼‰
- ä¸è¦å¿½ç•¥æ¨¡å‹ä¼˜åŠ¿ï¼Œéšæ„åˆ†é…

---

### 9.2 å‚æ•°è°ƒä¼˜

```python
# è§„åˆ’ç±»ä»»åŠ¡ï¼ˆQwenï¼‰
planning_params = {
    "temperature": 0.7,    # é€‚ä¸­
    "top_p": 0.9,
    "max_tokens": 4000
}

# é€»è¾‘ç±»ä»»åŠ¡ï¼ˆDeepSeekï¼‰
logic_params = {
    "temperature": 0.5,    # è¾ƒä½ï¼Œä¿è¯ç¨³å®š
    "top_p": 0.85,
    "max_tokens": 2000
}

# åˆ›ä½œç±»ä»»åŠ¡ï¼ˆDoubaoï¼‰
creation_params = {
    "temperature": 0.8,    # è¾ƒé«˜ï¼Œå¢å¼ºåˆ›é€ åŠ›
    "top_p": 0.95,
    "max_tokens": 4000
}
```

---

### 9.3 æˆæœ¬æ§åˆ¶

```python
# é¢„ç®—æ§åˆ¶å™¨
from creative_autogpt.utils.budget import BudgetController

budget = BudgetController(
    total_budget=100.0,      # æ€»é¢„ç®— ï¿¥100
    alerts=[50.0, 80.0]      # ä½¿ç”¨50%å’Œ80%æ—¶é¢„è­¦
)

# æ‰§è¡Œå‰æ£€æŸ¥
if budget.can_execute(estimated_cost=2.5):
    result = await client.generate(...)
    budget.record_cost(actual_cost=2.3)
else:
    print("é¢„ç®—ä¸è¶³ï¼Œè¯·å……å€¼æˆ–ä¼˜åŒ–é…ç½®")
```

---

## 10. å¸¸è§é—®é¢˜

### Q1: å¯ä»¥åªç”¨ä¸€ä¸ªæ¨¡å‹å—ï¼Ÿ

**A**: å¯ä»¥ï¼Œä½†ä¸æ¨èã€‚

```python
# åªç”¨ Doubaoï¼ˆé€‚åˆå°é¡¹ç›®ï¼‰
config = {
    "default_llm": "doubao",
    "enable_routing": False
}

# ä½†ä¼šæŸå¤±ï¼š
# - Qwen çš„é•¿ä¸Šä¸‹æ–‡ä¼˜åŠ¿
# - DeepSeek çš„æˆæœ¬ä¼˜åŠ¿
```

---

### Q2: å¦‚ä½•åˆ¤æ–­æ¨¡å‹åˆ†å·¥æ˜¯å¦åˆç†ï¼Ÿ

**A**: æŸ¥çœ‹è¯„ä¼°æŠ¥å‘Š

```bash
# ç”ŸæˆæŠ¥å‘Š
python scripts/analyze_llm_usage.py --session sess_123

# è¾“å‡ºç¤ºä¾‹
# âœ… Qwen: 12æ¬¡è°ƒç”¨ï¼Œè´¨é‡è¯„åˆ† 8.5
# âœ… DeepSeek: 50æ¬¡è°ƒç”¨ï¼Œè´¨é‡è¯„åˆ† 8.2ï¼Œæˆæœ¬ä»… ï¿¥0.15
# âš ï¸ Doubao: 45æ¬¡è°ƒç”¨ï¼Œè´¨é‡è¯„åˆ† 8.8ï¼Œä½†3æ¬¡é‡è¯•
```

---

### Q3: æ¨¡å‹è¿”å›å†…å®¹ä¸ç¬¦åˆè¦æ±‚æ€ä¹ˆåŠï¼Ÿ

**A**: è°ƒæ•´ Prompt æˆ–åˆ‡æ¢æ¨¡å‹

```python
# æ–¹æ¡ˆ1ï¼šä¼˜åŒ– Prompt
result = await client.generate(
    prompt="""
ä½ æ˜¯ä¸€ä½ä¸“ä¸šå°è¯´ä½œå®¶ã€‚
è¦æ±‚ï¼š
1. æ–‡ç¬”ä¼˜ç¾æµç•…
2. æƒ…èŠ‚ç´§å‡‘
3. å­—æ•°æ§åˆ¶åœ¨3000å­—
""",
    llm="doubao"
)

# æ–¹æ¡ˆ2ï¼šåˆ‡æ¢æ¨¡å‹
result = await client.generate(
    prompt=same_prompt,
    llm="qwen"  # æ¢ä¸ªæ¨¡å‹è¯•è¯•
)
```

---

## 11. å‚è€ƒèµ„æº

- [å¤š LLM æ¶æ„æ–‡æ¡£](../architecture/MULTI_LLM.md)
- [API æ–‡æ¡£](../api/REST_API.md)
- [æˆæœ¬ä¼˜åŒ–æŒ‡å—](./COST_OPTIMIZATION.md)

---

*ç‰ˆæœ¬: 1.0*  
*æœ€åæ›´æ–°: 2026-01-23*
