# æŠ€æœ¯å®ç°æ–‡æ¡£

> Creative AutoGPT è¯¦ç»†æŠ€æœ¯å®ç°è§„èŒƒ

## 1. ä»£ç ç»“æ„

### 1.1 é¡¹ç›®ç›®å½•ç»“æ„

```
creative_autogpt/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ creative_autogpt/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ core/                    # æ ¸å¿ƒæ¨¡å—
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ loop_engine.py       # æ‰§è¡Œå¼•æ“
â”‚       â”‚   â”œâ”€â”€ task_planner.py      # ä»»åŠ¡è§„åˆ’
â”‚       â”‚   â”œâ”€â”€ evaluator.py         # è´¨é‡è¯„ä¼°
â”‚       â”‚   â””â”€â”€ vector_memory.py     # å‘é‡è®°å¿†
â”‚       â”œâ”€â”€ modes/                   # å†™ä½œæ¨¡å¼
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ base.py              # åŸºç¡€æ¨¡å¼
â”‚       â”‚   â”œâ”€â”€ novel.py             # å°è¯´æ¨¡å¼
â”‚       â”‚   â”œâ”€â”€ script.py            # å‰§æœ¬æ¨¡å¼
â”‚       â”‚   â””â”€â”€ larp.py              # å‰§æœ¬æ€æ¨¡å¼
â”‚       â”œâ”€â”€ plugins/                 # æ’ä»¶ç³»ç»Ÿ
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ base.py              # æ’ä»¶åŸºç±»
â”‚       â”‚   â”œâ”€â”€ character.py         # äººç‰©æ’ä»¶
â”‚       â”‚   â”œâ”€â”€ worldview.py         # ä¸–ç•Œè§‚æ’ä»¶
â”‚       â”‚   â”œâ”€â”€ event.py             # äº‹ä»¶æ’ä»¶
â”‚       â”‚   â””â”€â”€ manager.py           # æ’ä»¶ç®¡ç†å™¨
â”‚       â”œâ”€â”€ prompts/                 # æç¤ºè¯ç³»ç»Ÿ
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ manager.py           # æç¤ºè¯ç®¡ç†å™¨
â”‚       â”‚   â”œâ”€â”€ enhancer.py          # ğŸ†• æ™ºèƒ½æç¤ºè¯å¢å¼ºå™¨
â”‚       â”‚   â”œâ”€â”€ templates/           # æ¨¡æ¿æ–‡ä»¶
â”‚       â”‚   â””â”€â”€ styles/              # é£æ ¼é…ç½®
â”‚       â”œâ”€â”€ utils/                   # å·¥å…·ç±»
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ llm_client.py        # LLMå®¢æˆ·ç«¯
â”‚       â”‚   â”œâ”€â”€ cache.py             # ç¼“å­˜
â”‚       â”‚   â”œâ”€â”€ logger.py            # æ—¥å¿—
â”‚       â”‚   â””â”€â”€ validators.py        # éªŒè¯å™¨
â”‚       â”œâ”€â”€ storage/                 # å­˜å‚¨å±‚
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ session.py           # ä¼šè¯å­˜å‚¨
â”‚       â”‚   â”œâ”€â”€ vector_store.py      # å‘é‡å­˜å‚¨
â”‚       â”‚   â””â”€â”€ file_store.py        # æ–‡ä»¶å­˜å‚¨
â”‚       â””â”€â”€ api/                     # APIå±‚
â”‚           â”œâ”€â”€ __init__.py
â”‚           â”œâ”€â”€ main.py              # FastAPIä¸»æ–‡ä»¶
â”‚           â”œâ”€â”€ routes/              # è·¯ç”±
â”‚           â”‚   â”œâ”€â”€ sessions.py
â”‚           â”‚   â”œâ”€â”€ tasks.py
â”‚           â”‚   â””â”€â”€ websocket.py
â”‚           â”œâ”€â”€ schemas/             # æ•°æ®æ¨¡å‹
â”‚           â”‚   â”œâ”€â”€ session.py
â”‚           â”‚   â”œâ”€â”€ task.py
â”‚           â”‚   â””â”€â”€ response.py
â”‚           â””â”€â”€ dependencies.py      # ä¾èµ–æ³¨å…¥
â”‚
â”œâ”€â”€ prompts/                         # æç¤ºè¯æ¨¡æ¿
â”‚   â”œâ”€â”€ base/
â”‚   â”‚   â”œâ”€â”€ system.txt
â”‚   â”‚   â””â”€â”€ constraints.txt
â”‚   â”œâ”€â”€ tasks/
â”‚   â”‚   â”œâ”€â”€ outline.jinja2
â”‚   â”‚   â”œâ”€â”€ character.jinja2
â”‚   â”‚   â””â”€â”€ chapter.jinja2
â”‚   â””â”€â”€ styles/
â”‚       â”œâ”€â”€ xuanhuan.yaml
â”‚       â””â”€â”€ wuxia.yaml
â”‚
â”œâ”€â”€ frontend/                        # å‰ç«¯ä»£ç 
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ pages/                   # é¡µé¢
â”‚   â”‚   â”œâ”€â”€ components/              # ç»„ä»¶
â”‚   â”‚   â”œâ”€â”€ stores/                  # çŠ¶æ€ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ api/                     # APIè°ƒç”¨
â”‚   â”‚   â””â”€â”€ utils/                   # å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ public/
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ tests/                           # æµ‹è¯•
â”‚   â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ integration/
â”‚   â””â”€â”€ e2e/
â”‚
â”œâ”€â”€ scripts/                         # è„šæœ¬
â”‚   â”œâ”€â”€ init_db.py
â”‚   â”œâ”€â”€ migrate.py
â”‚   â””â”€â”€ analyze_llm_usage.py
â”‚
â”œâ”€â”€ data/                            # æ•°æ®ç›®å½•ï¼ˆgitignoreï¼‰
â”‚   â”œâ”€â”€ sessions/
â”‚   â”œâ”€â”€ chroma/
â”‚   â””â”€â”€ exports/
â”‚
â”œâ”€â”€ logs/                            # æ—¥å¿—ï¼ˆgitignoreï¼‰
â”‚
â”œâ”€â”€ docs/                            # æ–‡æ¡£
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ requirements-dev.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

---

## 2. æ ¸å¿ƒæ¨¡å—å®ç°

### 2.1 LoopEngine (æ‰§è¡Œå¼•æ“)

**æ–‡ä»¶**: `src/creative_autogpt/core/loop_engine.py`

```python
"""
æ ¸å¿ƒæ‰§è¡Œå¼•æ“
è´Ÿè´£åè°ƒæ‰€æœ‰æ¨¡å—å®Œæˆå°è¯´åˆ›ä½œ
"""

from typing import Dict, Any, List, Optional
from loguru import logger
import asyncio

from creative_autogpt.modes.base import Mode
from creative_autogpt.core.task_planner import TaskPlanner
from creative_autogpt.core.evaluator import EvaluationEngine
from creative_autogpt.core.vector_memory import VectorMemoryManager
from creative_autogpt.utils.llm_client import MultiLLMClient
from creative_autogpt.storage.session import SessionStorage


class LoopEngine:
    """å†™ä½œ Agent çš„æ ¸å¿ƒæ‰§è¡Œå¼•æ“"""
    
    def __init__(
        self,
        session_id: str,
        mode: Mode,
        llm_client: MultiLLMClient,
        memory: VectorMemoryManager,
        evaluator: EvaluationEngine,
        storage: SessionStorage,
        config: Dict[str, Any] = None
    ):
        self.session_id = session_id
        self.mode = mode
        self.llm_client = llm_client
        self.memory = memory
        self.evaluator = evaluator
        self.storage = storage
        self.config = config or {}
        
        # ä»»åŠ¡è§„åˆ’å™¨
        self.planner = TaskPlanner(mode=mode, config=config)
        
        # æ‰§è¡ŒçŠ¶æ€
        self.is_running = False
        self.is_paused = False
        self.current_task = None
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_tasks": 0,
            "completed_tasks": 0,
            "failed_tasks": 0,
            "total_time": 0,
        }
    
    async def run(self, goal: Dict[str, Any]) -> Dict[str, Any]:
        """
        ä¸»æ‰§è¡Œå¾ªç¯
        
        Args:
            goal: åˆ›ä½œç›®æ ‡ï¼ŒåŒ…å«é£æ ¼ã€ä¸»é¢˜ã€å­—æ•°ç­‰
            
        Returns:
            æ‰§è¡Œç»“æœ
        """
        logger.info(f"Session {self.session_id} starting execution")
        logger.debug(f"Goal: {goal}")
        
        try:
            # 1. ç”Ÿæˆä»»åŠ¡è®¡åˆ’
            tasks = await self.planner.plan(goal)
            self.stats["total_tasks"] = len(tasks)
            logger.info(f"Generated {len(tasks)} tasks")
            
            # ä¿å­˜ä»»åŠ¡é˜Ÿåˆ—
            await self.storage.save_tasks(tasks)
            
            # 2. æ‰§è¡Œä»»åŠ¡é˜Ÿåˆ—
            self.is_running = True
            
            for task in tasks:
                if not self.is_running:
                    logger.info("Execution stopped by user")
                    break
                
                if self.is_paused:
                    logger.info("Execution paused, waiting...")
                    await self._wait_for_resume()
                
                # æ‰§è¡Œå•ä¸ªä»»åŠ¡
                await self._execute_task(task)
            
            # 3. ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
            result = await self._generate_final_report()
            
            logger.info(f"Session {self.session_id} completed successfully")
            return result
            
        except Exception as e:
            logger.error(f"Session {self.session_id} failed: {str(e)}")
            raise
        finally:
            self.is_running = False
    
    async def _execute_task(self, task: Dict[str, Any]) -> None:
        """
        æ‰§è¡Œå•ä¸ªä»»åŠ¡
        
        Args:
            task: ä»»åŠ¡ä¿¡æ¯
        """
        self.current_task = task
        task_id = task["task_id"]
        task_type = task["task_type"]
        
        logger.info(f"Executing task {task_id}: {task_type}")
        
        try:
            # 1. æ„å»º Prompt
            prompt = await self.mode.build_prompt(task, self.memory)
            
            # 2. è°ƒç”¨ LLM ç”Ÿæˆå†…å®¹
            llm_name = self._route_to_llm(task_type)
            result = await self.llm_client.generate(
                prompt=prompt,
                task_type=task_type,
                llm=llm_name,
                **task.get("llm_params", {})
            )
            
            # 3. è¯„ä¼°ç»“æœè´¨é‡
            evaluation = await self.evaluator.evaluate(
                task_type=task_type,
                content=result.content,
                criteria=task.get("evaluation_criteria"),
                llm_client=self.llm_client
            )
            
            # 4. åˆ¤æ–­æ˜¯å¦éœ€è¦é‡å†™
            if not evaluation.passed:
                result = await self._attempt_rewrite(
                    task=task,
                    result=result,
                    evaluation=evaluation
                )
            
            # 5. ä¿å­˜ç»“æœåˆ°è®°å¿†
            await self.memory.store(
                content=result.content,
                task_id=task_id,
                task_type=task_type,
                metadata={
                    "evaluation": evaluation.to_dict(),
                    "llm_used": llm_name,
                    "tokens_used": result.tokens_used
                }
            )
            
            # 6. ä¿å­˜åˆ°å­˜å‚¨
            await self.storage.save_task_result(
                task_id=task_id,
                result=result,
                evaluation=evaluation
            )
            
            # 7. æ›´æ–°ç»Ÿè®¡
            self.stats["completed_tasks"] += 1
            await self._update_progress()
            
            logger.info(f"Task {task_id} completed successfully")
            
        except Exception as e:
            logger.error(f"Task {task_id} failed: {str(e)}")
            self.stats["failed_tasks"] += 1
            
            # ä¿å­˜é”™è¯¯ä¿¡æ¯
            await self.storage.save_task_error(
                task_id=task_id,
                error=str(e)
            )
            
            # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦ç»§ç»­
            if not self.config.get("continue_on_error", False):
                raise
    
    def _route_to_llm(self, task_type: str) -> str:
        """
        æ ¹æ®ä»»åŠ¡ç±»å‹è·¯ç”±åˆ°åˆé€‚çš„ LLM
        
        Args:
            task_type: ä»»åŠ¡ç±»å‹
            
        Returns:
            LLM åç§°
        """
        routing_map = {
            # è§„åˆ’ç±» â†’ Qwen
            "outline": "qwen",
            "style_elements": "qwen",
            "character_design": "qwen",
            "worldview": "qwen",
            
            # é€»è¾‘ç±» â†’ DeepSeek
            "events": "deepseek",
            "scenes": "deepseek",
            "evaluation": "deepseek",
            "consistency_check": "deepseek",
            
            # åˆ›ä½œç±» â†’ Doubao
            "chapter_content": "doubao",
            "revision": "doubao",
            "polish": "doubao",
        }
        
        return routing_map.get(task_type, "doubao")  # é»˜è®¤ Doubao
    
    async def _attempt_rewrite(
        self,
        task: Dict[str, Any],
        result: Any,
        evaluation: Any,
        max_retries: int = 3
    ) -> Any:
        """
        å°è¯•é‡å†™ä¸åˆæ ¼çš„å†…å®¹
        
        Args:
            task: ä»»åŠ¡ä¿¡æ¯
            result: åŸå§‹ç»“æœ
            evaluation: è¯„ä¼°ç»“æœ
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            
        Returns:
            é‡å†™åçš„ç»“æœ
        """
        logger.warning(f"Task {task['task_id']} failed evaluation, attempting rewrite")
        
        for attempt in range(max_retries):
            logger.info(f"Rewrite attempt {attempt + 1}/{max_retries}")
            
            # æ„å»ºæ”¹è¿›åçš„ Prompt
            improved_prompt = await self.mode.build_improved_prompt(
                task=task,
                previous_result=result.content,
                evaluation_feedback=evaluation.feedback
            )
            
            # é‡æ–°ç”Ÿæˆ
            llm_name = self._route_to_llm(task["task_type"])
            new_result = await self.llm_client.generate(
                prompt=improved_prompt,
                task_type=task["task_type"],
                llm=llm_name,
                temperature=self.config.get("rewrite_temperature", 0.8)
            )
            
            # é‡æ–°è¯„ä¼°
            new_evaluation = await self.evaluator.evaluate(
                task_type=task["task_type"],
                content=new_result.content,
                criteria=task.get("evaluation_criteria"),
                llm_client=self.llm_client
            )
            
            if new_evaluation.passed:
                logger.info(f"Rewrite successful on attempt {attempt + 1}")
                return new_result
        
        # æ‰€æœ‰é‡è¯•å¤±è´¥ï¼Œè¿”å›æœ€å¥½çš„ç»“æœ
        logger.warning(f"All rewrite attempts failed for task {task['task_id']}")
        return result
    
    async def _update_progress(self) -> None:
        """æ›´æ–°æ‰§è¡Œè¿›åº¦"""
        progress = {
            "total_tasks": self.stats["total_tasks"],
            "completed_tasks": self.stats["completed_tasks"],
            "failed_tasks": self.stats["failed_tasks"],
            "percentage": (
                self.stats["completed_tasks"] / self.stats["total_tasks"] * 100
                if self.stats["total_tasks"] > 0 else 0
            )
        }
        
        # ä¿å­˜è¿›åº¦
        await self.storage.update_progress(progress)
        
        # å‘é€ WebSocket äº‹ä»¶
        await self._emit_event("progress_updated", progress)
    
    async def _wait_for_resume(self) -> None:
        """ç­‰å¾…æ¢å¤æ‰§è¡Œ"""
        while self.is_paused and self.is_running:
            await asyncio.sleep(1)
    
    async def _emit_event(self, event_type: str, data: Dict[str, Any]) -> None:
        """å‘é€ WebSocket äº‹ä»¶ï¼ˆéœ€è¦å®ç°ï¼‰"""
        # TODO: å®ç° WebSocket äº‹ä»¶å‘é€
        pass
    
    async def _generate_final_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        return {
            "session_id": self.session_id,
            "status": "completed",
            "stats": self.stats,
            "summary": await self._generate_summary()
        }
    
    async def _generate_summary(self) -> str:
        """ç”Ÿæˆåˆ›ä½œæ€»ç»“"""
        # TODO: ä½¿ç”¨ LLM ç”Ÿæˆæ•´ä½“æ€»ç»“
        return f"Session {self.session_id} completed with {self.stats['completed_tasks']} tasks"
    
    # === æ§åˆ¶æ–¹æ³• ===
    
    def pause(self) -> None:
        """æš‚åœæ‰§è¡Œ"""
        logger.info(f"Session {self.session_id} pausing")
        self.is_paused = True
    
    def resume(self) -> None:
        """æ¢å¤æ‰§è¡Œ"""
        logger.info(f"Session {self.session_id} resuming")
        self.is_paused = False
    
    def stop(self) -> None:
        """åœæ­¢æ‰§è¡Œ"""
        logger.info(f"Session {self.session_id} stopping")
        self.is_running = False
        self.is_paused = False
```

---

### 2.2 PromptEnhancer (æ™ºèƒ½æç¤ºè¯å¢å¼ºå™¨) ğŸ†•

> è®©ä¸æ‡‚æç¤ºè¯çš„ç”¨æˆ·ä¹Ÿèƒ½è½»æ¾ä½¿ç”¨ç³»ç»Ÿï¼

**æ–‡ä»¶**: `src/creative_autogpt/prompts/enhancer.py`

```python
"""
æ™ºèƒ½æç¤ºè¯å¢å¼ºå™¨
å°†ç”¨æˆ·çš„ç®€å•æè¿°è‡ªåŠ¨æ‰©å±•ä¸ºå®Œæ•´çš„ç»“æ„åŒ–é…ç½®

è®¾è®¡ç›®æ ‡ï¼š
- ç”¨æˆ·åªéœ€ä¸€å¥è¯æè¿°ï¼Œç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆä¸“ä¸šçº§é…ç½®
- é™ä½ä½¿ç”¨é—¨æ§›ï¼Œè®©éæŠ€æœ¯ç”¨æˆ·ä¹Ÿèƒ½è½»æ¾åˆ›ä½œ
- ä½¿ç”¨ DeepSeek è¿›è¡Œæ‰©å±•ï¼ˆæˆæœ¬æä½ï¼Œçº¦ Â¥0.001/æ¬¡ï¼‰
"""

from typing import Dict, Any, Optional
from dataclasses import dataclass, asdict
from loguru import logger
import json
import re


@dataclass
class EnhancedPrompt:
    """å¢å¼ºåçš„æç¤ºè¯é…ç½®"""
    # åŸºæœ¬ä¿¡æ¯
    style: str                      # é£æ ¼ç±»å‹ï¼ˆç„å¹»/æ­¦ä¾ /éƒ½å¸‚/ç§‘å¹»ç­‰ï¼‰
    theme: str                      # ä¸»é¢˜ï¼ˆä¸€å¥è¯æ¦‚æ‹¬ï¼‰
    target_words: int               # ç›®æ ‡å­—æ•°
    chapter_count: int              # ç« èŠ‚æ•°
    
    # äººç‰©è®¾å®š
    protagonist: Dict[str, Any]     # ä¸»è§’è®¾å®š
    
    # ä¸–ç•Œè§‚
    world_setting: Dict[str, Any]   # ä¸–ç•Œè§‚è®¾å®š
    
    # æƒ…èŠ‚ä¸é£æ ¼
    plot_elements: list             # æƒ…èŠ‚è¦ç´ 
    style_elements: Dict[str, Any]  # é£æ ¼å…ƒç´ 
    
    # çº¦æŸ
    constraints: list               # çº¦æŸæ¡ä»¶
    special_requirements: list      # ç‰¹æ®Šè¦æ±‚
    
    # å…ƒä¿¡æ¯
    raw_input: str                  # åŸå§‹ç”¨æˆ·è¾“å…¥
    confidence: float               # æ‰©å±•ç½®ä¿¡åº¦ï¼ˆ0-1ï¼‰
    
    def to_config(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸º LoopEngine å¯ç”¨çš„é…ç½®"""
        return {
            "style": self.style,
            "theme": self.theme,
            "structure": {
                "target_words": self.target_words,
                "chapter_count": self.chapter_count,
                "words_per_chapter": self.target_words // self.chapter_count
            },
            "characters": {
                "protagonist": self.protagonist
            },
            "world": self.world_setting,
            "plot": {
                "elements": self.plot_elements
            },
            "style_config": self.style_elements,
            "constraints": self.constraints,
            "requirements": self.special_requirements
        }


class PromptEnhancer:
    """
    æ™ºèƒ½æç¤ºè¯å¢å¼ºå™¨
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. è§£æç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æè¿°
    2. è¯†åˆ«å…³é”®è¦ç´ ï¼ˆé£æ ¼ã€ä¸»é¢˜ã€äººç‰©ç­‰ï¼‰
    3. æ‰©å±•ä¸ºå®Œæ•´çš„ç»“æ„åŒ–é…ç½®
    4. æ”¯æŒç”¨æˆ·ç¡®è®¤å’Œè¿­ä»£è°ƒæ•´
    
    ä½¿ç”¨ç¤ºä¾‹:
        enhancer = PromptEnhancer(llm_client)
        
        # ç®€å•ä½¿ç”¨ï¼šç”¨æˆ·ä¸€å¥è¯
        enhanced = await enhancer.enhance("å†™ä¸€ä¸ªç„å¹»å°è¯´ï¼Œä¸»è§’åºŸæé€†è¢­ï¼Œ100ä¸‡å­—")
        
        # ç”¨æˆ·å¯ä»¥æŸ¥çœ‹é…ç½®å¹¶è°ƒæ•´
        if enhanced.confidence < 0.8:
            enhanced = await enhancer.refine(enhanced, "ä¸»è§’è¦æ˜¯å¥³çš„")
        
        # è½¬æ¢ä¸ºç³»ç»Ÿé…ç½®
        config = enhanced.to_config()
    """
    
    # æ‰©å±•æç¤ºè¯æ¨¡æ¿
    ENHANCE_PROMPT_TEMPLATE = '''ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å°è¯´ç­–åˆ’ä¸“å®¶ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„ç®€å•æè¿°ï¼Œæ‰©å±•ä¸ºå®Œæ•´çš„å°è¯´åˆ›ä½œé…ç½®ã€‚

## ç”¨æˆ·æè¿°
{user_input}

## å†™ä½œæ¨¡å¼
{mode}

## ä½ çš„ä»»åŠ¡
åˆ†æç”¨æˆ·æè¿°ï¼Œæ¨æ–­å¹¶è¡¥å……ä»¥ä¸‹ä¿¡æ¯ã€‚å¦‚æœç”¨æˆ·æ²¡æœ‰æ˜ç¡®è¯´æ˜ï¼Œè¯·æ ¹æ®æè¿°åˆç†æ¨æ–­ã€‚

è¯·ä»¥ JSON æ ¼å¼è¾“å‡ºï¼š
```json
{{
  "style": "é£æ ¼ç±»å‹",
  "theme": "æ ¸å¿ƒä¸»é¢˜ï¼ˆä¸€å¥è¯ï¼‰",
  "target_words": ç›®æ ‡å­—æ•°,
  "chapter_count": ç« èŠ‚æ•°,
  
  "protagonist": {{
    "name": "å§“åï¼ˆå¯ä¸ºç©ºï¼‰",
    "gender": "æ€§åˆ«",
    "age": "å¹´é¾„èŒƒå›´",
    "personality": "æ€§æ ¼ç‰¹ç‚¹",
    "background": "èƒŒæ™¯è®¾å®š",
    "growth_arc": "æˆé•¿å¼§çº¿"
  }},
  
  "world_setting": {{
    "type": "ä¸–ç•Œç±»å‹",
    "era": "æ—¶ä»£èƒŒæ™¯",
    "power_system": "åŠ›é‡ä½“ç³»",
    "key_locations": ["åœ°ç‚¹1", "åœ°ç‚¹2"],
    "factions": ["åŠ¿åŠ›1", "åŠ¿åŠ›2"]
  }},
  
  "plot_elements": ["æƒ…èŠ‚è¦ç´ 1", "æƒ…èŠ‚è¦ç´ 2", "æƒ…èŠ‚è¦ç´ 3"],
  
  "style_elements": {{
    "tone": "æ•´ä½“åŸºè°ƒ",
    "pacing": "èŠ‚å¥é£æ ¼",
    "description_style": "æå†™é£æ ¼",
    "dialogue_style": "å¯¹è¯é£æ ¼"
  }},
  
  "constraints": ["åˆ›ä½œçº¦æŸ1", "åˆ›ä½œçº¦æŸ2"],
  "special_requirements": ["ç‰¹æ®Šè¦æ±‚"],
  "confidence": 0.0åˆ°1.0
}}
```

æ³¨æ„ï¼š
1. å¦‚æœæè¿°ç®€å•ï¼Œåˆç†æ¨æ–­è¡¥å……
2. ä¿æŒä¸ç”¨æˆ·æ„å›¾ä¸€è‡´
3. confidenceï¼šæè¿°è¯¦ç»†åˆ™æ¥è¿‘1.0ï¼Œéœ€è¦å¤§é‡æ¨æ–­åˆ™è¾ƒä½
4. åªè¾“å‡º JSON
'''

    REFINE_PROMPT_TEMPLATE = '''å½“å‰é…ç½®ï¼š
{current_config}

ç”¨æˆ·è°ƒæ•´æ„è§ï¼š
{user_feedback}

è¯·æ ¹æ®ç”¨æˆ·åé¦ˆè°ƒæ•´é…ç½®ï¼Œä¿æŒå…¶ä»–éƒ¨åˆ†ä¸å˜ï¼Œè¾“å‡ºå®Œæ•´ JSONã€‚
'''
    
    def __init__(
        self,
        llm_client,
        config: Dict[str, Any] = None
    ):
        """
        åˆå§‹åŒ–å¢å¼ºå™¨
        
        Args:
            llm_client: LLM å®¢æˆ·ç«¯
            config: é…ç½®é€‰é¡¹
        """
        self.llm_client = llm_client
        self.config = config or {}
        
        # ä½¿ç”¨ DeepSeekï¼ˆæ€§ä»·æ¯”æœ€é«˜ï¼‰
        self.enhancer_llm = self.config.get("llm", "deepseek")
        
        # è‡ªåŠ¨ç¡®è®¤é˜ˆå€¼
        self.auto_confirm_threshold = self.config.get(
            "auto_confirm_threshold", 0.8
        )
    
    async def enhance(
        self,
        user_input: str,
        mode: str = "novel"
    ) -> EnhancedPrompt:
        """
        å°†ç”¨æˆ·ç®€å•æè¿°æ‰©å±•ä¸ºå®Œæ•´é…ç½®
        
        Args:
            user_input: ç”¨æˆ·çš„ç®€å•æè¿°ï¼ˆå¦‚"å†™ä¸€ä¸ªç„å¹»å°è¯´ï¼Œ100ä¸‡å­—"ï¼‰
            mode: å†™ä½œæ¨¡å¼ (novel/script/larp)
            
        Returns:
            EnhancedPrompt: æ‰©å±•åçš„å®Œæ•´é…ç½®
        
        ç¤ºä¾‹:
            enhanced = await enhancer.enhance(
                "å†™ä¸ªéƒ½å¸‚ä¿®ä»™ï¼Œä¸»è§’é‡ç”Ÿå›é«˜ä¸­ï¼Œæœ‰ç³»ç»Ÿé‡‘æ‰‹æŒ‡"
            )
            print(f"é£æ ¼: {enhanced.style}")  # éƒ½å¸‚ä¿®ä»™
            print(f"ç½®ä¿¡åº¦: {enhanced.confidence}")  # 0.85
        """
        logger.info(f"ğŸ”® Enhancing user input: {user_input[:50]}...")
        
        # æ„å»ºå¢å¼ºæç¤ºè¯
        prompt = self.ENHANCE_PROMPT_TEMPLATE.format(
            user_input=user_input,
            mode=mode
        )
        
        # è°ƒç”¨ LLM æ‰©å±•
        response = await self.llm_client.generate(
            prompt=prompt,
            task_type="prompt_enhance",
            llm=self.enhancer_llm,
            temperature=0.7  # é€‚åº¦åˆ›é€ æ€§
        )
        
        # è§£æå“åº”
        enhanced = self._parse_response(response.content, user_input)
        
        logger.info(
            f"âœ… Enhancement complete: "
            f"style={enhanced.style}, "
            f"words={enhanced.target_words:,}, "
            f"confidence={enhanced.confidence:.0%}"
        )
        
        return enhanced
    
    async def refine(
        self,
        enhanced: EnhancedPrompt,
        user_feedback: str
    ) -> EnhancedPrompt:
        """
        æ ¹æ®ç”¨æˆ·åé¦ˆè°ƒæ•´é…ç½®
        
        Args:
            enhanced: å½“å‰é…ç½®
            user_feedback: ç”¨æˆ·çš„è°ƒæ•´æ„è§
            
        Returns:
            è°ƒæ•´åçš„é…ç½®
        
        ç¤ºä¾‹:
            enhanced = await enhancer.refine(
                enhanced,
                "ä¸»è§’æ”¹æˆå¥³çš„ï¼Œå¢åŠ æ„Ÿæƒ…çº¿"
            )
        """
        logger.info(f"ğŸ”„ Refining with feedback: {user_feedback[:50]}...")
        
        prompt = self.REFINE_PROMPT_TEMPLATE.format(
            current_config=json.dumps(asdict(enhanced), ensure_ascii=False, indent=2),
            user_feedback=user_feedback
        )
        
        response = await self.llm_client.generate(
            prompt=prompt,
            task_type="prompt_refine",
            llm=self.enhancer_llm,
            temperature=0.5  # è¾ƒä½åˆ›é€ æ€§ï¼Œä¿æŒä¸€è‡´
        )
        
        return self._parse_response(response.content, enhanced.raw_input)
    
    def _parse_response(self, response: str, raw_input: str) -> EnhancedPrompt:
        """è§£æ LLM çš„ JSON å“åº”"""
        try:
            # å°è¯•ç›´æ¥è§£æ
            data = json.loads(response)
        except json.JSONDecodeError:
            # å°è¯•æå– JSON å—
            json_match = re.search(r'```json\s*(.*?)\s*```', response, re.DOTALL)
            if json_match:
                data = json.loads(json_match.group(1))
            else:
                # å°è¯•æå–ä»»ä½• JSON å¯¹è±¡
                json_match = re.search(r'\{.*\}', response, re.DOTALL)
                if json_match:
                    data = json.loads(json_match.group(0))
                else:
                    raise ValueError("Failed to parse LLM response as JSON")
        
        return EnhancedPrompt(
            style=data.get("style", "ç„å¹»"),
            theme=data.get("theme", ""),
            target_words=data.get("target_words", 100000),
            chapter_count=data.get("chapter_count", 50),
            protagonist=data.get("protagonist", {}),
            world_setting=data.get("world_setting", {}),
            plot_elements=data.get("plot_elements", []),
            style_elements=data.get("style_elements", {}),
            constraints=data.get("constraints", []),
            special_requirements=data.get("special_requirements", []),
            raw_input=raw_input,
            confidence=data.get("confidence", 0.5)
        )
    
    def should_auto_confirm(self, enhanced: EnhancedPrompt) -> bool:
        """åˆ¤æ–­æ˜¯å¦å¯ä»¥è‡ªåŠ¨ç¡®è®¤ï¼ˆæ— éœ€ç”¨æˆ·äº¤äº’ï¼‰"""
        return enhanced.confidence >= self.auto_confirm_threshold


# ä¾¿æ·å‡½æ•°
async def smart_enhance(
    user_input: str,
    llm_client,
    auto_confirm: bool = True
) -> Dict[str, Any]:
    """
    æ™ºèƒ½å¢å¼ºå¹¶è¿”å›é…ç½®
    
    Args:
        user_input: ç”¨æˆ·æè¿°
        llm_client: LLM å®¢æˆ·ç«¯
        auto_confirm: é«˜ç½®ä¿¡åº¦æ—¶è‡ªåŠ¨ç¡®è®¤
        
    Returns:
        å¯ç›´æ¥ç”¨äº LoopEngine çš„é…ç½®
    """
    enhancer = PromptEnhancer(llm_client)
    enhanced = await enhancer.enhance(user_input)
    
    if auto_confirm and enhancer.should_auto_confirm(enhanced):
        logger.info("âœ… High confidence, auto-confirming configuration")
        return enhanced.to_config()
    
    return enhanced
```

**ä½¿ç”¨æµç¨‹**ï¼š

```
ç”¨æˆ·: "å†™ä¸€ä¸ªç„å¹»å°è¯´ï¼Œä¸»è§’åºŸæé€†è¢­ï¼Œ100ä¸‡å­—"
          â”‚
          â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚PromptEnhancerâ”‚  â† ä½¿ç”¨ DeepSeekï¼Œæˆæœ¬çº¦ Â¥0.001
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ EnhancedPrompt:                     â”‚
    â”‚   style: "ç„å¹»ä¿®ä»™"                  â”‚
    â”‚   theme: "åºŸæé€†è¢­æˆä»™å¸"            â”‚
    â”‚   target_words: 1,000,000           â”‚
    â”‚   protagonist: {...}                â”‚
    â”‚   world_setting: {...}              â”‚
    â”‚   confidence: 0.85                  â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ ç”¨æˆ·ç¡®è®¤/è°ƒæ•´â”‚  â† å¦‚æœ confidence >= 0.8 å¯è·³è¿‡
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ LoopEngine  â”‚  â† å¼€å§‹æ­£å¼åˆ›ä½œ
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 2.3 MultiLLMClient (å¤šæ¨¡å‹å®¢æˆ·ç«¯)

**æ–‡ä»¶**: `src/creative_autogpt/utils/llm_client.py`

```python
"""
å¤š LLM å®¢æˆ·ç«¯
ç»Ÿä¸€ç®¡ç† Qwenã€DeepSeekã€Doubao ç­‰æ¨¡å‹çš„è°ƒç”¨
"""

from typing import Dict, Any, Optional, List
from dataclasses import dataclass
from loguru import logger
import asyncio
import os

from openai import AsyncOpenAI
import dashscope


@dataclass
class LLMResponse:
    """LLM å“åº”"""
    content: str
    tokens_used: Dict[str, int]
    llm_used: str
    cost: float
    latency: float


class MultiLLMClient:
    """å¤š LLM å®¢æˆ·ç«¯"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        
        # åˆå§‹åŒ–å„ä¸ª LLM å®¢æˆ·ç«¯
        self.clients = {}
        
        # Qwen (é€šè¿‡ DashScope)
        if self.config.get("qwen", {}).get("enabled", True):
            dashscope.api_key = os.getenv("ALIYUN_API_KEY")
            self.clients["qwen"] = "dashscope"  # æ ‡è®°ä½¿ç”¨ DashScope
        
        # DeepSeek (OpenAI å…¼å®¹)
        if self.config.get("deepseek", {}).get("enabled", True):
            self.clients["deepseek"] = AsyncOpenAI(
                api_key=os.getenv("DEEPSEEK_API_KEY"),
                base_url=os.getenv("DEEPSEEK_BASE_URL", "https://api.deepseek.com/v1")
            )
        
        # Doubao (OpenAI å…¼å®¹)
        if self.config.get("doubao", {}).get("enabled", True):
            self.clients["doubao"] = AsyncOpenAI(
                api_key=os.getenv("ARK_API_KEY"),
                base_url=os.getenv("ARK_BASE_URL", "https://ark.cn-beijing.volces.com/api/v3")
            )
    
    async def generate(
        self,
        prompt: str,
        task_type: str,
        llm: str,
        temperature: float = None,
        max_tokens: int = None,
        **kwargs
    ) -> LLMResponse:
        """
        ç”Ÿæˆå†…å®¹
        
        Args:
            prompt: æç¤ºè¯
            task_type: ä»»åŠ¡ç±»å‹
            llm: ä½¿ç”¨çš„ LLM åç§°
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ tokens
            
        Returns:
            LLM å“åº”
        """
        import time
        start_time = time.time()
        
        logger.info(f"Calling {llm} for task {task_type}")
        
        try:
            if llm == "qwen":
                response = await self._call_qwen(prompt, temperature, max_tokens, **kwargs)
            elif llm == "deepseek":
                response = await self._call_deepseek(prompt, temperature, max_tokens, **kwargs)
            elif llm == "doubao":
                response = await self._call_doubao(prompt, temperature, max_tokens, **kwargs)
            else:
                raise ValueError(f"Unknown LLM: {llm}")
            
            latency = time.time() - start_time
            logger.info(f"{llm} responded in {latency:.2f}s")
            
            return LLMResponse(
                content=response["content"],
                tokens_used=response["tokens"],
                llm_used=llm,
                cost=self._calculate_cost(llm, response["tokens"]),
                latency=latency
            )
            
        except Exception as e:
            logger.error(f"LLM call failed: {str(e)}")
            raise
    
    async def _call_qwen(
        self,
        prompt: str,
        temperature: Optional[float],
        max_tokens: Optional[int],
        **kwargs
    ) -> Dict[str, Any]:
        """è°ƒç”¨ Qwen"""
        from dashscope import Generation
        
        model = self.config.get("qwen", {}).get("model", "qwen-max")
        temperature = temperature or self.config.get("qwen", {}).get("temperature", 0.7)
        max_tokens = max_tokens or self.config.get("qwen", {}).get("max_tokens", 4000)
        
        response = await asyncio.to_thread(
            Generation.call,
            model=model,
            prompt=prompt,
            temperature=temperature,
            max_tokens=max_tokens,
            **kwargs
        )
        
        if response.status_code == 200:
            return {
                "content": response.output.text,
                "tokens": {
                    "prompt_tokens": response.usage.input_tokens,
                    "completion_tokens": response.usage.output_tokens,
                    "total_tokens": response.usage.total_tokens
                }
            }
        else:
            raise Exception(f"Qwen API error: {response.message}")
    
    async def _call_deepseek(
        self,
        prompt: str,
        temperature: Optional[float],
        max_tokens: Optional[int],
        **kwargs
    ) -> Dict[str, Any]:
        """è°ƒç”¨ DeepSeek"""
        client = self.clients["deepseek"]
        model = self.config.get("deepseek", {}).get("model", "deepseek-chat")
        temperature = temperature or self.config.get("deepseek", {}).get("temperature", 0.5)
        max_tokens = max_tokens or self.config.get("deepseek", {}).get("max_tokens", 2000)
        
        response = await client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            temperature=temperature,
            max_tokens=max_tokens,
            **kwargs
        )
        
        return {
            "content": response.choices[0].message.content,
            "tokens": {
                "prompt_tokens": response.usage.prompt_tokens,
                "completion_tokens": response.usage.completion_tokens,
                "total_tokens": response.usage.total_tokens
            }
        }
    
    async def _call_doubao(
        self,
        prompt: str,
        temperature: Optional[float],
        max_tokens: Optional[int],
        **kwargs
    ) -> Dict[str, Any]:
        """è°ƒç”¨ Doubao"""
        client = self.clients["doubao"]
        model = self.config.get("doubao", {}).get("model", "doubao-pro-32k")
        temperature = temperature or self.config.get("doubao", {}).get("temperature", 0.8)
        max_tokens = max_tokens or self.config.get("doubao", {}).get("max_tokens", 4000)
        
        response = await client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            temperature=temperature,
            max_tokens=max_tokens,
            **kwargs
        )
        
        return {
            "content": response.choices[0].message.content,
            "tokens": {
                "prompt_tokens": response.usage.prompt_tokens,
                "completion_tokens": response.usage.completion_tokens,
                "total_tokens": response.usage.total_tokens
            }
        }
    
    def _calculate_cost(self, llm: str, tokens: Dict[str, int]) -> float:
        """è®¡ç®—æˆæœ¬ï¼ˆäººæ°‘å¸ï¼‰"""
        # ä»·æ ¼è¡¨ï¼ˆå…ƒ/1K tokensï¼‰
        pricing = {
            "qwen": {"input": 0.04, "output": 0.12},
            "deepseek": {"input": 0.001, "output": 0.002},
            "doubao": {"input": 0.008, "output": 0.008}
        }
        
        if llm not in pricing:
            return 0.0
        
        input_cost = tokens["prompt_tokens"] / 1000 * pricing[llm]["input"]
        output_cost = tokens["completion_tokens"] / 1000 * pricing[llm]["output"]
        
        return input_cost + output_cost
```

---

## 3. API å®ç°

### 3.1 FastAPI ä¸»æ–‡ä»¶

**æ–‡ä»¶**: `src/creative_autogpt/api/main.py`

```python
"""
FastAPI ä¸»åº”ç”¨
"""

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from loguru import logger

from creative_autogpt.api.routes import sessions, tasks, websocket
from creative_autogpt.api.dependencies import get_settings


# åˆ›å»ºåº”ç”¨
app = FastAPI(
    title="Creative AutoGPT API",
    description="AI-powered creative writing system",
    version="1.0.0"
)

# CORS é…ç½®
settings = get_settings()
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.CORS_ORIGINS.split(","),
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ³¨å†Œè·¯ç”±
app.include_router(sessions.router, prefix="/api/v1/sessions", tags=["sessions"])
app.include_router(tasks.router, prefix="/api/v1/tasks", tags=["tasks"])
app.include_router(websocket.router, prefix="/ws", tags=["websocket"])


@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨æ—¶æ‰§è¡Œ"""
    logger.info("Starting Creative AutoGPT API")
    # TODO: åˆå§‹åŒ–æ•°æ®åº“è¿æ¥ã€å‘é‡å­˜å‚¨ç­‰


@app.on_event("shutdown")
async def shutdown_event():
    """åº”ç”¨å…³é—­æ—¶æ‰§è¡Œ"""
    logger.info("Shutting down Creative AutoGPT API")
    # TODO: å…³é—­æ•°æ®åº“è¿æ¥ç­‰


@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "name": "Creative AutoGPT API",
        "version": "1.0.0",
        "docs": "/docs"
    }


@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "healthy"}
```

---

### 3.2 ä¼šè¯è·¯ç”±

**æ–‡ä»¶**: `src/creative_autogpt/api/routes/sessions.py`

```python
"""
ä¼šè¯ç®¡ç†è·¯ç”±
"""

from fastapi import APIRouter, HTTPException, Depends
from typing import List

from creative_autogpt.api.schemas.session import (
    SessionCreate,
    SessionResponse,
    SessionList
)
from creative_autogpt.storage.session import SessionStorage


router = APIRouter()


@router.post("/", response_model=SessionResponse, status_code=201)
async def create_session(
    session_data: SessionCreate,
    storage: SessionStorage = Depends()
):
    """åˆ›å»ºæ–°ä¼šè¯"""
    try:
        session = await storage.create_session(session_data.dict())
        return session
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/", response_model=SessionList)
async def list_sessions(
    page: int = 1,
    page_size: int = 20,
    status: str = None,
    storage: SessionStorage = Depends()
):
    """è·å–ä¼šè¯åˆ—è¡¨"""
    try:
        sessions = await storage.list_sessions(
            page=page,
            page_size=page_size,
            status=status
        )
        return sessions
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/{session_id}", response_model=SessionResponse)
async def get_session(
    session_id: str,
    storage: SessionStorage = Depends()
):
    """è·å–ä¼šè¯è¯¦æƒ…"""
    session = await storage.get_session(session_id)
    if not session:
        raise HTTPException(status_code=404, detail="Session not found")
    return session


@router.delete("/{session_id}")
async def delete_session(
    session_id: str,
    storage: SessionStorage = Depends()
):
    """åˆ é™¤ä¼šè¯"""
    success = await storage.delete_session(session_id)
    if not success:
        raise HTTPException(status_code=404, detail="Session not found")
    return {"message": "Session deleted successfully"}
```

---

## 4. æ•°æ®æ¨¡å‹

### 4.1 ä¼šè¯æ¨¡å‹

**æ–‡ä»¶**: `src/creative_autogpt/api/schemas/session.py`

```python
"""
ä¼šè¯æ•°æ®æ¨¡å‹
"""

from pydantic import BaseModel, Field
from typing import Dict, Any, List, Optional
from datetime import datetime
from enum import Enum


class SessionStatus(str, Enum):
    """ä¼šè¯çŠ¶æ€"""
    CREATED = "created"
    CONFIGURED = "configured"
    RUNNING = "running"
    PAUSED = "paused"
    COMPLETED = "completed"
    FAILED = "failed"
    ARCHIVED = "archived"


class SessionMode(str, Enum):
    """å†™ä½œæ¨¡å¼"""
    NOVEL = "novel"
    SCRIPT = "script"
    LARP = "larp"


class SessionConfig(BaseModel):
    """ä¼šè¯é…ç½®"""
    style: str = Field(..., description="é£æ ¼ç±»å‹")
    theme: str = Field(..., description="ä¸»é¢˜")
    target_words: int = Field(..., gt=0, description="ç›®æ ‡å­—æ•°")
    chapter_count: int = Field(..., gt=0, description="ç« èŠ‚æ•°é‡")
    words_per_chapter: Optional[int] = Field(None, description="æ¯ç« å­—æ•°")
    
    llm_config: Dict[str, Any] = Field(default_factory=dict, description="LLMé…ç½®")
    

class SessionMetadata(BaseModel):
    """ä¼šè¯å…ƒæ•°æ®"""
    title: str
    author: Optional[str] = None
    description: Optional[str] = None
    tags: List[str] = Field(default_factory=list)


class SessionCreate(BaseModel):
    """åˆ›å»ºä¼šè¯è¯·æ±‚"""
    mode: SessionMode
    config: SessionConfig
    metadata: SessionMetadata


class SessionProgress(BaseModel):
    """ä¼šè¯è¿›åº¦"""
    total_tasks: int
    completed_tasks: int
    failed_tasks: int
    percentage: float


class SessionStats(BaseModel):
    """ä¼šè¯ç»Ÿè®¡"""
    total_words: int = 0
    chapters_completed: int = 0
    llm_calls: Dict[str, int] = Field(default_factory=dict)


class SessionResponse(BaseModel):
    """ä¼šè¯å“åº”"""
    session_id: str
    mode: SessionMode
    status: SessionStatus
    config: SessionConfig
    metadata: SessionMetadata
    progress: Optional[SessionProgress] = None
    stats: Optional[SessionStats] = None
    created_at: datetime
    updated_at: datetime
    
    class Config:
        orm_mode = True


class SessionList(BaseModel):
    """ä¼šè¯åˆ—è¡¨å“åº”"""
    total: int
    page: int
    page_size: int
    sessions: List[SessionResponse]
```

---

## 5. æ•°æ®åº“è®¾è®¡

### 5.1 SQLAlchemy æ¨¡å‹

**æ–‡ä»¶**: `src/creative_autogpt/storage/models.py`

```python
"""
æ•°æ®åº“æ¨¡å‹
"""

from sqlalchemy import Column, String, Integer, DateTime, JSON, Text, ForeignKey, Enum
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import relationship
from datetime import datetime
import enum


Base = declarative_base()


class SessionStatus(str, enum.Enum):
    CREATED = "created"
    RUNNING = "running"
    PAUSED = "paused"
    COMPLETED = "completed"
    FAILED = "failed"


class Session(Base):
    """ä¼šè¯è¡¨"""
    __tablename__ = "sessions"
    
    session_id = Column(String(50), primary_key=True)
    mode = Column(String(20), nullable=False)
    status = Column(Enum(SessionStatus), default=SessionStatus.CREATED)
    
    config = Column(JSON, nullable=False)
    metadata = Column(JSON, nullable=False)
    
    total_tasks = Column(Integer, default=0)
    completed_tasks = Column(Integer, default=0)
    failed_tasks = Column(Integer, default=0)
    
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    # å…³ç³»
    tasks = relationship("Task", back_populates="session", cascade="all, delete-orphan")
    checkpoints = relationship("Checkpoint", back_populates="session")


class Task(Base):
    """ä»»åŠ¡è¡¨"""
    __tablename__ = "tasks"
    
    task_id = Column(String(50), primary_key=True)
    session_id = Column(String(50), ForeignKey("sessions.session_id"), nullable=False)
    
    task_type = Column(String(50), nullable=False)
    status = Column(String(20), default="pending")
    
    prompt = Column(Text)
    result_content = Column(Text)
    
    llm_used = Column(String(20))
    tokens_used = Column(Integer, default=0)
    cost = Column(Float, default=0.0)
    
    evaluation_score = Column(Float)
    evaluation_passed = Column(Boolean)
    
    retry_count = Column(Integer, default=0)
    
    created_at = Column(DateTime, default=datetime.utcnow)
    completed_at = Column(DateTime)
    
    # å…³ç³»
    session = relationship("Session", back_populates="tasks")


class Checkpoint(Base):
    """æ£€æŸ¥ç‚¹è¡¨"""
    __tablename__ = "checkpoints"
    
    checkpoint_id = Column(String(50), primary_key=True)
    session_id = Column(String(50), ForeignKey("sessions.session_id"), nullable=False)
    
    description = Column(String(200))
    completed_tasks = Column(Integer)
    state_snapshot = Column(JSON)
    
    created_at = Column(DateTime, default=datetime.utcnow)
    
    # å…³ç³»
    session = relationship("Session", back_populates="checkpoints")
```

---

**ç”±äºæ–‡æ¡£éå¸¸é•¿ï¼Œç»§ç»­åˆ›å»ºå…¶ä»–æŠ€æœ¯ç»†èŠ‚...**

---

*ç‰ˆæœ¬: 1.0*  
*æœ€åæ›´æ–°: 2026-01-23*
