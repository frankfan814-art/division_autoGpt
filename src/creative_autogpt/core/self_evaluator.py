"""
Self Evaluator - è‡ªæˆ‘è¯„ä¼°ç³»ç»Ÿ

ç”¨ LLM è¯„ä¼°ç”Ÿæˆçš„å†…å®¹è´¨é‡ï¼Œå¹¶ç»™å‡ºæ”¹è¿›å»ºè®®ã€‚
æ”¯æŒè®°å½•è¯„ä¼°åŽ†å²ï¼Œç”¨äºŽæç¤ºè¯è‡ªæˆ‘è¿­ä»£ä¼˜åŒ–ã€‚
"""

import json
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

from loguru import logger


@dataclass
class EvaluationResult:
    """è¯„ä¼°ç»“æžœ"""
    task_type: str
    overall_score: float  # 0-100
    dimensions: Dict[str, float]  # å„ç»´åº¦è¯„åˆ†
    strengths: List[str]  # ä¼˜ç‚¹
    weaknesses: List[str]  # ç¼ºç‚¹
    suggestions: List[str]  # æ”¹è¿›å»ºè®®
    prompt_improvements: List[str]  # æç¤ºè¯æ”¹è¿›å»ºè®®
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "task_type": self.task_type,
            "overall_score": self.overall_score,
            "dimensions": self.dimensions,
            "strengths": self.strengths,
            "weaknesses": self.weaknesses,
            "suggestions": self.suggestions,
            "prompt_improvements": self.prompt_improvements,
            "timestamp": self.timestamp,
        }


class SelfEvaluator:
    """
    å†…å®¹è‡ªæˆ‘è¯„ä¼°å™¨
    
    åŠŸèƒ½ï¼š
    1. è¯„ä¼°ç”Ÿæˆå†…å®¹çš„è´¨é‡
    2. ç»™å‡ºæ”¹è¿›å»ºè®®
    3. æä¾›æç¤ºè¯ä¼˜åŒ–å»ºè®®
    4. è®°å½•è¯„ä¼°åŽ†å²ç”¨äºŽå­¦ä¹ 
    """
    
    # è¯„ä¼°ç»´åº¦å®šä¹‰
    EVALUATION_DIMENSIONS = {
        "coherence": "å…³è”æ€§ - ä¸Žå‰ç½®ä»»åŠ¡çš„å…³è”æ˜¯å¦ç´§å¯†ï¼Œæ˜¯å¦æœåŠ¡äºŽæ•´ä½“æ•…äº‹",
        "readability": "å¯è¯»æ€§ - å†…å®¹æ˜¯å¦é€šä¿—æ˜“æ‡‚ã€ç™½è¯æ–‡",
        "storytelling": "æ•…äº‹æ€§ - æ˜¯å¦æœ‰å¸å¼•åŠ›ã€æœ‰ä»£å…¥æ„Ÿã€åƒä¸ªä¼šè®²æ•…äº‹çš„äººå†™çš„",
        "consistency": "ä¸€è‡´æ€§ - ä¸Žå‰æ–‡è®¾å®šæ˜¯å¦ä¸€è‡´ï¼Œæœ‰æ²¡æœ‰è‡ªç›¸çŸ›ç›¾",
        "creativity": "åˆ›æ„æ€§ - æ˜¯å¦æœ‰æ–°æ„ã€ä¸è½ä¿—å¥—ã€æœ‰ç‹¬ç‰¹çš„æƒ³æ³•",
        "completeness": "å®Œæ•´æ€§ - æ˜¯å¦è¦†ç›–äº†è¦æ±‚çš„æ‰€æœ‰å†…å®¹",
        "structure": "ç»“æž„æ€§ - ç»„ç»‡æ˜¯å¦æ¸…æ™°ã€é€»è¾‘æ˜¯å¦é€šé¡º",
        "literary": "æ–‡å­¦æ€§ - æ˜¯å¦åƒå°è¯´è€Œä¸æ˜¯è®ºæ–‡ï¼Œæœ‰æ¸©åº¦æœ‰ç”»é¢",
    }
    
    # ä»»åŠ¡ç±»åž‹ç‰¹å®šçš„è¯„ä¼°é‡ç‚¹
    TASK_EVALUATION_FOCUS = {
        "åˆ›æ„è„‘æš´": ["creativity", "storytelling", "completeness"],
        "æ•…äº‹æ ¸å¿ƒ": ["coherence", "storytelling", "completeness"],
        "é£Žæ ¼å…ƒç´ ": ["coherence", "readability", "completeness", "literary"],
        "ä¸»é¢˜ç¡®è®¤": ["coherence", "readability", "storytelling", "completeness"],
        "äººç‰©è®¾è®¡": ["coherence", "creativity", "completeness", "consistency", "literary"],
        "ä¸–ç•Œè§‚è§„åˆ™": ["coherence", "readability", "creativity", "completeness"],
        "å¤§çº²": ["coherence", "structure", "completeness", "storytelling"],
        "ç« èŠ‚å¤§çº²": ["coherence", "structure", "consistency", "completeness"],
        "ç« èŠ‚å†…å®¹": ["coherence", "readability", "storytelling", "creativity", "consistency", "literary"],
    }
    
    def __init__(
        self,
        llm_client=None,
        history_dir: Optional[str] = None,
    ):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            llm_client: LLMå®¢æˆ·ç«¯ï¼Œç”¨äºŽè¯„ä¼°
            history_dir: è¯„ä¼°åŽ†å²å­˜å‚¨ç›®å½•
        """
        self.llm_client = llm_client
        
        # è®¾ç½®åŽ†å²ç›®å½•
        if history_dir:
            self.history_dir = Path(history_dir)
        else:
            self.history_dir = Path.cwd() / "data" / "evaluation_history"
        
        self.history_dir.mkdir(parents=True, exist_ok=True)
        
        # åŠ è½½åŽ†å²è¯„ä¼°æ•°æ®
        self.evaluation_history: List[EvaluationResult] = []
        self._load_history()
        
        logger.info(f"SelfEvaluator initialized, history dir: {self.history_dir}")
    
    def _load_history(self) -> None:
        """åŠ è½½åŽ†å²è¯„ä¼°æ•°æ®"""
        history_file = self.history_dir / "evaluation_history.json"
        if history_file.exists():
            try:
                with open(history_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    for item in data:
                        self.evaluation_history.append(EvaluationResult(**item))
                logger.info(f"Loaded {len(self.evaluation_history)} evaluation records")
            except Exception as e:
                logger.warning(f"Failed to load evaluation history: {e}")
    
    def _save_history(self) -> None:
        """ä¿å­˜è¯„ä¼°åŽ†å²"""
        history_file = self.history_dir / "evaluation_history.json"
        try:
            with open(history_file, "w", encoding="utf-8") as f:
                json.dump(
                    [r.to_dict() for r in self.evaluation_history[-1000:]],  # åªä¿ç•™æœ€è¿‘1000æ¡
                    f,
                    ensure_ascii=False,
                    indent=2,
                )
        except Exception as e:
            logger.warning(f"Failed to save evaluation history: {e}")
    
    async def evaluate(
        self,
        content: str,
        task_type: str,
        context: Optional[Dict[str, Any]] = None,
        goal: Optional[Dict[str, Any]] = None,
    ) -> EvaluationResult:
        """
        è¯„ä¼°ç”Ÿæˆçš„å†…å®¹
        
        Args:
            content: ç”Ÿæˆçš„å†…å®¹
            task_type: ä»»åŠ¡ç±»åž‹
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            goal: åˆ›ä½œç›®æ ‡
            
        Returns:
            EvaluationResult è¯„ä¼°ç»“æžœ
        """
        if not self.llm_client:
            return self._basic_evaluation(content, task_type)
        
        # æž„å»ºè¯„ä¼°æç¤ºè¯
        prompt = self._build_evaluation_prompt(content, task_type, context, goal)
        
        try:
            response = await self.llm_client.generate(
                prompt=prompt,
                task_type="è¯„ä¼°",
                temperature=0.3,  # ä½Žæ¸©åº¦ï¼Œæ›´ç¨³å®šçš„è¯„ä¼°
                max_tokens=2000,
            )
            
            result = self._parse_evaluation_response(response.content, task_type)
            
            # ä¿å­˜åˆ°åŽ†å²
            self.evaluation_history.append(result)
            self._save_history()
            
            return result
            
        except Exception as e:
            logger.error(f"Evaluation failed: {e}")
            return self._basic_evaluation(content, task_type)
    
    def _build_evaluation_prompt(
        self,
        content: str,
        task_type: str,
        context: Optional[Dict[str, Any]] = None,
        goal: Optional[Dict[str, Any]] = None,
    ) -> str:
        """æž„å»ºè¯„ä¼°æç¤ºè¯ - é¡¶çº§ä½œå®¶è§†è§’"""
        
        # èŽ·å–è¯¥ä»»åŠ¡ç±»åž‹çš„è¯„ä¼°é‡ç‚¹
        focus_dimensions = self.TASK_EVALUATION_FOCUS.get(
            task_type, 
            list(self.EVALUATION_DIMENSIONS.keys())
        )
        
        dimensions_desc = "\n".join([
            f"- {dim}: {self.EVALUATION_DIMENSIONS[dim]}"
            for dim in focus_dimensions
        ])
        
        goal_info = ""
        if goal:
            goal_info = f"""
### åˆ›ä½œç›®æ ‡
- ç±»åž‹: {goal.get('genre', 'æœªçŸ¥')}
- å­—æ•°: {goal.get('word_count', 'æœªçŸ¥')}
- ç« èŠ‚: {goal.get('chapter_count', 'æœªçŸ¥')}
- é£Žæ ¼: {goal.get('style', 'æœªçŸ¥')}
"""

        # æž„å»ºå‰ç½®ä»»åŠ¡ä¸Šä¸‹æ–‡ï¼ˆç”¨äºŽæ£€æŸ¥å…³è”æ€§ï¼‰
        context_summary = ""
        if context and isinstance(context, dict):
            recent = context.get('recent_results', [])
            if recent:
                context_summary = "\n### å‰ç½®ä»»åŠ¡æˆæžœ\n"
                for r in recent[-3:]:  # æœ€è¿‘3ä¸ªä»»åŠ¡
                    context_summary += f"- **{r.get('task_type', 'æœªçŸ¥')}**: {r.get('content', '')[:200]}...\n"

        prompt = f"""## é¡¶çº§ä½œå®¶è¯„ä¼°

ðŸŽ­ **è§’è‰²è®¾å®š**ï¼šä½ çŽ°åœ¨æ˜¯ä¸€ä½èŽ·å¾—è¿‡èŒ…ç›¾æ–‡å­¦å¥–ã€é›¨æžœå¥–çš„é¡¶çº§ä½œå®¶ï¼ŒåŒæ—¶ä¹Ÿæ˜¯èµ„æ·±å‡ºç‰ˆç¤¾ç¼–è¾‘ã€‚
ä½ è§è¿‡å¤ªå¤šå¹³åº¸çš„ä½œå“ï¼Œå¯¹å¥½ä½œå“æœ‰æžé«˜çš„æ ‡å‡†ã€‚

### ä½ çš„è¯„ä¼°åŽŸåˆ™

ä½œä¸ºé¡¶çº§ä½œå®¶ï¼Œä½ çŸ¥é“ï¼š
1. **äººç‰©æ˜¯çµé­‚**ï¼šæ²¡æœ‰æ´»çš„äººç‰©ï¼Œæ•…äº‹å°±æ˜¯æ­»çš„
2. **æ•…äº‹æ ¸å¿ƒè¦æ¸…æ™°**ï¼šè¯»è€…åœ¨ä»»ä½•æ—¶å€™éƒ½è¦çŸ¥é“"è¿™ä¸ªæ•…äº‹è®²ä»€ä¹ˆ"
3. **æ¯ä¸ªä»»åŠ¡éƒ½è¦æœåŠ¡æ•´ä½“**ï¼šå„éƒ¨åˆ†ä¹‹é—´å¿…é¡»ç´§å¯†å…³è”ï¼Œä¸èƒ½å„è‡ªä¸ºæ”¿
4. **é€šä¿—ä¸ç­‰äºŽè‚¤æµ…**ï¼šæœ€å¥½çš„æ•…äº‹äººäººéƒ½çœ‹å¾—æ‡‚ï¼Œä½†æœ‰æ·±åº¦
5. **æ‹’ç»å­¦æœ¯è…”**ï¼šå°è¯´ä¸æ˜¯è®ºæ–‡ï¼Œè¦æœ‰æ¸©åº¦ã€æœ‰ç”»é¢

### ä»»åŠ¡ç±»åž‹
{task_type}
{goal_info}
{context_summary}

### å¾…è¯„ä¼°å†…å®¹
```
{content[:5000]}  
```
{f'ï¼ˆå†…å®¹è¿‡é•¿ï¼Œå·²æˆªæ–­ï¼Œå…±{len(content)}å­—ï¼‰' if len(content) > 5000 else ''}

### è¯„ä¼°ç»´åº¦
{dimensions_desc}

### è¯„ä¼°è¦ç‚¹

**ä½œä¸ºé¡¶çº§ä½œå®¶ï¼Œä½ è¦ç‰¹åˆ«å…³æ³¨ï¼š**

1. **ä¸Žå‰ç½®ä»»åŠ¡çš„å…³è”æ€§**ï¼ˆéžå¸¸é‡è¦ï¼ï¼‰
   - å½“å‰å†…å®¹æ˜¯å¦å……åˆ†åˆ©ç”¨äº†å‰é¢ä»»åŠ¡çš„æˆæžœï¼Ÿ
   - æ˜¯å¦ä¸Žæ•´ä½“æ•…äº‹æ ¸å¿ƒä¿æŒä¸€è‡´ï¼Ÿ
   - æœ‰æ²¡æœ‰"å¦èµ·ç‚‰ç¶"ã€è„±ç¦»å‰æ–‡çš„é—®é¢˜ï¼Ÿ

2. **å†…å®¹è´¨é‡**
   - æ˜¯å¦æœ‰è¡€æœ‰è‚‰ï¼Œè€Œä¸æ˜¯å¹²å·´å·´çš„æ¸…å•ï¼Ÿ
   - æ˜¯å¦åƒä¸ªä¼šè®²æ•…äº‹çš„äººå†™çš„ï¼Œè€Œä¸æ˜¯AIç”Ÿæˆçš„æ¨¡æ¿ï¼Ÿ
   - è¯»è€…ä¼šè¢«å¸å¼•å—ï¼Ÿ

3. **ä¸“ä¸šæ€§**
   - æ˜¯å¦é¿å…äº†å­¦æœ¯è®ºæ–‡è…”ï¼Ÿ
   - æ˜¯å¦åšåˆ°äº†"é€šä¿—æ˜“æ‡‚"ï¼Ÿ
   - æ˜¯å¦æœ‰å°è¯´å®¶çš„ç¬”è§¦ï¼Ÿ

### è¾“å‡ºæ ¼å¼

è¯·ä»¥ JSON æ ¼å¼è¾“å‡ºï¼š

```json
{{
  "overall_score": 85,
  "dimensions": {{
    "readability": 80,
    "storytelling": 90,
    ...
  }},
  "strengths": [
    "ä¼˜ç‚¹1",
    "ä¼˜ç‚¹2"
  ],
  "weaknesses": [
    "ç¼ºç‚¹1",
    "ç¼ºç‚¹2"
  ],
  "suggestions": [
    "å†…å®¹æ”¹è¿›å»ºè®®1",
    "å†…å®¹æ”¹è¿›å»ºè®®2"
  ],
  "prompt_improvements": [
    "æç¤ºè¯æ”¹è¿›å»ºè®®1ï¼š...",
    "æç¤ºè¯æ”¹è¿›å»ºè®®2ï¼š..."
  ]
}}
```

è¯·ç›´æŽ¥è¾“å‡º JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚
"""
        return prompt
    
    def _parse_evaluation_response(
        self, 
        response: str, 
        task_type: str
    ) -> EvaluationResult:
        """è§£æžè¯„ä¼°å“åº”"""
        try:
            # æå– JSON
            json_start = response.find("{")
            json_end = response.rfind("}") + 1
            
            if json_start >= 0 and json_end > json_start:
                json_str = response[json_start:json_end]
                data = json.loads(json_str)
                
                return EvaluationResult(
                    task_type=task_type,
                    overall_score=data.get("overall_score", 70),
                    dimensions=data.get("dimensions", {}),
                    strengths=data.get("strengths", []),
                    weaknesses=data.get("weaknesses", []),
                    suggestions=data.get("suggestions", []),
                    prompt_improvements=data.get("prompt_improvements", []),
                )
        except Exception as e:
            logger.warning(f"Failed to parse evaluation response: {e}")
        
        return self._basic_evaluation("", task_type)
    
    def _basic_evaluation(self, content: str, task_type: str) -> EvaluationResult:
        """åŸºç¡€è¯„ä¼°ï¼ˆä¸ä½¿ç”¨ LLMï¼‰"""
        # ç®€å•çš„åŸºäºŽè§„åˆ™çš„è¯„ä¼°
        score = 70
        strengths = []
        weaknesses = []
        suggestions = []
        
        # æ£€æŸ¥é•¿åº¦
        if len(content) > 500:
            strengths.append("å†…å®¹æœ‰ä¸€å®šç¯‡å¹…")
        else:
            weaknesses.append("å†…å®¹è¾ƒçŸ­")
            suggestions.append("å¢žåŠ æ›´å¤šç»†èŠ‚")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å­¦æœ¯åŒ–å€¾å‘
        academic_words = ["ç»¼ä¸Šæ‰€è¿°", "æœ¬æ–‡", "ç ”ç©¶è¡¨æ˜Ž", "æ•°æ®æ˜¾ç¤º", "å®žéªŒè¯æ˜Ž"]
        if any(word in content for word in academic_words):
            weaknesses.append("å­˜åœ¨å­¦æœ¯åŒ–è¡¨è¾¾")
            suggestions.append("ä½¿ç”¨æ›´é€šä¿—çš„è¯­è¨€")
        
        return EvaluationResult(
            task_type=task_type,
            overall_score=score,
            dimensions={},
            strengths=strengths,
            weaknesses=weaknesses,
            suggestions=suggestions,
            prompt_improvements=[],
        )
    
    def get_improvement_insights(self, task_type: str) -> Dict[str, Any]:
        """
        æ ¹æ®åŽ†å²è¯„ä¼°æ•°æ®ï¼ŒèŽ·å–æ”¹è¿›æ´žå¯Ÿ
        
        Args:
            task_type: ä»»åŠ¡ç±»åž‹
            
        Returns:
            æ”¹è¿›æ´žå¯Ÿ
        """
        # ç­›é€‰è¯¥ä»»åŠ¡ç±»åž‹çš„åŽ†å²æ•°æ®
        task_history = [
            r for r in self.evaluation_history 
            if r.task_type == task_type
        ]
        
        if not task_history:
            return {
                "message": "æš‚æ— åŽ†å²æ•°æ®",
                "optimization_recommended": False,
            }
        
        # è®¡ç®—å¹³å‡åˆ†
        avg_score = sum(r.overall_score for r in task_history) / len(task_history)
        
        # æ”¶é›†æ‰€æœ‰å¼±ç‚¹å’Œå»ºè®®
        all_weaknesses = []
        all_improvements = []
        for r in task_history[-20:]:  # æœ€è¿‘20æ¡
            all_weaknesses.extend(r.weaknesses)
            all_improvements.extend(r.prompt_improvements)
        
        # ç»Ÿè®¡æœ€å¸¸è§çš„é—®é¢˜
        from collections import Counter
        weakness_counts = Counter(all_weaknesses)
        improvement_counts = Counter(all_improvements)
        
        # åˆ¤æ–­æ˜¯å¦åº”è¯¥è§¦å‘ä¼˜åŒ–
        # æ¡ä»¶ï¼šè‡³å°‘10æ¡è¯„ä¼°è®°å½•ï¼Œä¸”å¹³å‡åˆ†ä½ŽäºŽ75
        optimization_recommended = (
            len(task_history) >= 10 and avg_score < 75
        )
        
        # åˆ¤æ–­è¶‹åŠ¿
        trend = "stable"
        if len(task_history) > 5:
            recent_avg = sum(r.overall_score for r in task_history[-5:]) / 5
            if recent_avg > avg_score + 5:
                trend = "improving"
            elif recent_avg < avg_score - 5:
                trend = "declining"
        
        return {
            "task_type": task_type,
            "total_evaluations": len(task_history),
            "average_score": round(avg_score, 1),
            "common_weaknesses": weakness_counts.most_common(5),
            "common_improvements": improvement_counts.most_common(5),
            "trend": trend,
            "optimization_recommended": optimization_recommended,
        }
