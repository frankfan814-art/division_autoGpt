"""
Evaluation Engine - Quality assessment for generated content

Implements multi-dimensional quality evaluation with configurable criteria.
Uses LLM-based evaluation for semantic quality assessment.
"""

import json
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional

from loguru import logger

from creative_autogpt.utils.llm_client import MultiLLMClient, LLMMessage


class EvaluationCriterion(str, Enum):
    """Evaluation criteria for content quality"""

    COHERENCE = "coherence"  # Logical flow and consistency
    CREATIVITY = "creativity"  # Originality and innovation
    QUALITY = "quality"  # Writing quality and prose
    CONSISTENCY = "consistency"  # Consistency with established context
    GOAL_ALIGNMENT = "goal_alignment"  # Alignment with creation goals
    CHARACTER_VOICE = "character_voice"  # Character voice consistency
    PLOT_PROGRESSION = "plot_progression"  # Story development
    DIALOGUE_QUALITY = "dialogue_quality"  # Dialogue naturalness


@dataclass
class DimensionScore:
    """Score for a single evaluation dimension"""

    dimension: str
    score: float  # 0.0 to 1.0
    reason: str = ""
    suggestions: List[str] = field(default_factory=list)

    def to_dict(self) -> Dict[str, Any]:
        return {
            "dimension": self.dimension,
            "score": self.score,
            "reason": self.reason,
            "suggestions": self.suggestions,
        }


@dataclass
class EvaluationResult:
    """Result of content evaluation"""

    passed: bool
    score: float  # Overall score 0.0 to 1.0
    dimension_scores: Dict[str, DimensionScore] = field(default_factory=dict)
    reasons: List[str] = field(default_factory=list)
    suggestions: List[str] = field(default_factory=list)
    evaluated_at: datetime = field(default_factory=datetime.utcnow)
    evaluator: str = "default"
    metadata: Dict[str, Any] = field(default_factory=dict)

    def to_dict(self) -> Dict[str, Any]:
        return {
            "passed": self.passed,
            "score": self.score,
            "dimension_scores": {
                k: v.to_dict() for k, v in self.dimension_scores.items()
            },
            "reasons": self.reasons,
            "suggestions": self.suggestions,
            "evaluated_at": self.evaluated_at.isoformat(),
            "evaluator": self.evaluator,
            "metadata": self.metadata,
        }


class EvaluationEngine:
    """
    Evaluates generated content quality across multiple dimensions

    Default criteria and weights:
    - Coherence: 20%
    - Creativity: 20%
    - Quality: 20%
    - Consistency: 20%
    - Goal Alignment: 20%

    Can be customized for different content types.
    """

    # Default evaluation criteria with weights
    DEFAULT_CRITERIA: Dict[EvaluationCriterion, float] = {
        EvaluationCriterion.COHERENCE: 0.20,
        EvaluationCriterion.CREATIVITY: 0.20,
        EvaluationCriterion.QUALITY: 0.20,
        EvaluationCriterion.CONSISTENCY: 0.20,
        EvaluationCriterion.GOAL_ALIGNMENT: 0.20,
    }

    # Content-specific criteria overrides
    CONTENT_TYPE_CRITERIA: Dict[str, Dict[EvaluationCriterion, float]] = {
        "ç« èŠ‚å†…å®¹": {
            EvaluationCriterion.COHERENCE: 0.15,
            EvaluationCriterion.CREATIVITY: 0.20,
            EvaluationCriterion.QUALITY: 0.25,
            EvaluationCriterion.CONSISTENCY: 0.20,
            EvaluationCriterion.CHARACTER_VOICE: 0.10,
            EvaluationCriterion.PLOT_PROGRESSION: 0.10,
        },
        "å¯¹è¯æ£€æŸ¥": {
            EvaluationCriterion.DIALOGUE_QUALITY: 0.40,
            EvaluationCriterion.CHARACTER_VOICE: 0.30,
            EvaluationCriterion.CONSISTENCY: 0.20,
            EvaluationCriterion.QUALITY: 0.10,
        },
        "å¤§çº²": {
            EvaluationCriterion.COHERENCE: 0.25,
            EvaluationCriterion.CREATIVITY: 0.25,
            EvaluationCriterion.GOAL_ALIGNMENT: 0.25,
            EvaluationCriterion.PLOT_PROGRESSION: 0.25,
        },
    }

    def __init__(
        self,
        llm_client: Optional[MultiLLMClient] = None,
        passing_threshold: float = 0.7,
        criteria: Optional[Dict[EvaluationCriterion, float]] = None,
    ):
        """
        Initialize evaluation engine

        Args:
            llm_client: LLM client for AI-based evaluation
            passing_threshold: Minimum score to pass (0.0 to 1.0)
            criteria: Custom criteria weights
        """
        self.llm_client = llm_client
        self.passing_threshold = passing_threshold
        self.criteria = criteria or self.DEFAULT_CRITERIA.copy()

        logger.info(
            f"EvaluationEngine initialized (threshold={passing_threshold}, "
            f"criteria={len(self.criteria)})"
        )

    async def evaluate(
        self,
        task_type: str,
        content: str,
        criteria: Optional[Dict[EvaluationCriterion, float]] = None,
        context: Optional[Dict[str, Any]] = None,
        goal: Optional[Dict[str, Any]] = None,
    ) -> EvaluationResult:
        """
        Evaluate content quality

        Args:
            task_type: Type of content being evaluated
            content: The content to evaluate
            criteria: Custom criteria for this evaluation
            context: Additional context for evaluation
            goal: Original creation goals

        Returns:
            EvaluationResult with scores and feedback
        """
        logger.debug(f"Evaluating content for task type: {task_type}")

        # Determine criteria to use
        eval_criteria = criteria or self._get_criteria_for_task_type(task_type)

        if self.llm_client:
            # Use LLM-based evaluation
            result = await self._llm_evaluate(
                task_type=task_type,
                content=content,
                criteria=eval_criteria,
                context=context,
                goal=goal,
            )
        else:
            # Use rule-based evaluation (fallback)
            result = await self._rule_based_evaluate(
                task_type=task_type,
                content=content,
                criteria=eval_criteria,
                context=context,
            )

        logger.info(
            f"Evaluation complete: score={result.score:.3f}, passed={result.passed}"
        )

        return result

    def _get_criteria_for_task_type(
        self,
        task_type: str,
    ) -> Dict[EvaluationCriterion, float]:
        """Get evaluation criteria for a specific task type"""
        return self.CONTENT_TYPE_CRITERIA.get(
            task_type,
            self.DEFAULT_CRITERIA,
        )

    async def _llm_evaluate(
        self,
        task_type: str,
        content: str,
        criteria: Dict[EvaluationCriterion, float],
        context: Optional[Dict[str, Any]] = None,
        goal: Optional[Dict[str, Any]] = None,
    ) -> EvaluationResult:
        """
        Perform LLM-based evaluation

        Uses DeepSeek for logical evaluation (cost-effective)
        """
        # Build evaluation prompt
        prompt = self._build_evaluation_prompt(
            task_type=task_type,
            content=content,
            criteria=criteria,
            context=context,
            goal=goal,
        )

        try:
            # Use DeepSeek for evaluation (logic/reasoning strength)
            response = await self.llm_client.generate(
                prompt=prompt,
                task_type="è¯„ä¼°",  # Route to DeepSeek
                temperature=0.3,  # Lower temperature for consistent evaluation
                max_tokens=2000,
            )

            # Parse evaluation result
            return self._parse_evaluation_response(
                response.content,
                criteria,
                task_type,
            )

        except Exception as e:
            logger.error(f"LLM evaluation failed: {e}, falling back to rule-based")
            return await self._rule_based_evaluate(
                task_type=task_type,
                content=content,
                criteria=criteria,
                context=context,
            )

    def _build_evaluation_prompt(
        self,
        task_type: str,
        content: str,
        criteria: Dict[EvaluationCriterion, float],
        context: Optional[Dict[str, Any]] = None,
        goal: Optional[Dict[str, Any]] = None,
    ) -> str:
        """Build prompt for LLM evaluation"""

        criteria_desc = "\n".join(
            f"- {c.value}: {w * 100:.0f}%æƒé‡"
            for c, w in criteria.items()
        )

        context_section = ""
        if context:
            context_section = f"\n\nä¸Šä¸‹æ–‡ä¿¡æ¯:\n{json.dumps(context, ensure_ascii=False, indent=2)}"

        goal_section = ""
        if goal:
            goal_section = f"\n\nåˆ›ä½œç›®æ ‡:\n{json.dumps(goal, ensure_ascii=False, indent=2)}"

        prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å°è¯´å†…å®¹è¯„ä¼°ä¸“å®¶ã€‚ä½ æ­£åœ¨è¯„ä¼°ä¸€éƒ¨**å°è¯´**çš„{task_type}å†…å®¹è´¨é‡ã€‚

âš ï¸ é‡è¦æé†’ï¼š
- è¿™æ˜¯å°è¯´åˆ›ä½œï¼Œä¸æ˜¯å­¦æœ¯è®ºæ–‡æˆ–ç§‘å­¦ç ”ç©¶æŠ¥å‘Š
- å†…å®¹åº”è¯¥æ˜¯æ•…äº‹æ€§çš„ã€æ–‡å­¦æ€§çš„ã€é¢å‘å¤§ä¼—è¯»è€…çš„
- å¿…é¡»ä½¿ç”¨å°è¯´çš„å™äº‹è¯­è¨€ï¼Œè€Œä¸æ˜¯å­¦æœ¯è®ºæ–‡è¯­è¨€

ğŸ“– ç§‘å¹»å°è¯´ç‰¹æ®Šè§„åˆ™ï¼ˆå‚è€ƒã€Šä¸‰ä½“ã€‹ã€Šæµæµªåœ°çƒã€‹æ ‡å‡†ï¼‰ï¼š
- âœ… å…è®¸ï¼šé€‚åº¦çš„ç§‘å­¦æ¦‚å¿µã€æŠ€æœ¯è®¾å®šã€æœªæ¥ç§‘æŠ€æè¿°
- âœ… å…è®¸ï¼šå¿…è¦çš„ç§‘å­¦æœ¯è¯­ï¼Œä½†å¿…é¡»é€šè¿‡æ•…äº‹æƒ…èŠ‚è‡ªç„¶å‘ˆç°
- âœ… å…è®¸ï¼šç”¨é€šä¿—æ˜“æ‡‚çš„æ–¹å¼è§£é‡Šç§‘å­¦åŸç†ï¼ˆåƒåˆ˜æ…ˆæ¬£çš„å†™æ³•ï¼‰
- âŒ ç¦æ­¢ï¼šå †ç Œå¤æ‚å…¬å¼ã€å­¦æœ¯è®ºæ–‡å¼çš„ç†è®ºæ¨å¯¼
- âŒ ç¦æ­¢ï¼šçº¯æŠ€æœ¯æ–‡æ¡£å¼çš„æè¿°ã€ç¼ºä¹æ•…äº‹æ€§
- âŒ ç¦æ­¢ï¼šé¢å‘ä¸“ä¸šç ”ç©¶è€…çš„å­¦æœ¯å†™ä½œé£æ ¼

æ ¸å¿ƒæ ‡å‡†ï¼šç§‘å­¦è®¾å®šæœåŠ¡äºæ•…äº‹ï¼Œè€Œä¸æ˜¯å±•ç¤ºå­¦æœ¯ç ”ç©¶ã€‚

## è¯„ä¼°æ ‡å‡†
{criteria_desc}

## å¾…è¯„ä¼°å†…å®¹
```
{content[:5000]}  # Limit content length
```
{context_section}
{goal_section}

## è¯„ä¼°è¦æ±‚
è¯·å¯¹æ¯ä¸ªè¯„ä¼°ç»´åº¦è¿›è¡Œæ‰“åˆ†ï¼ˆ0-100åˆ†ï¼‰å¹¶ç»™å‡ºç†ç”±å’Œæ”¹è¿›å»ºè®®ã€‚

ç‰¹åˆ«æ³¨æ„ï¼šå¦‚æœå†…å®¹åŒ…å«ä»¥ä¸‹ç‰¹å¾ï¼Œå¿…é¡»å¤§å¹…é™ä½è¯„åˆ†ï¼ˆ< 30åˆ†ï¼‰ï¼š
- è®ºæ–‡æ ¼å¼ï¼ˆæ‘˜è¦ã€å¼•è¨€ã€æ–¹æ³•è®ºã€å‚è€ƒæ–‡çŒ®ç­‰å­¦æœ¯ç»“æ„ï¼‰
- çº¯ç²¹çš„å…¬å¼æ¨å¯¼ï¼Œæ²¡æœ‰æ•…äº‹æƒ…èŠ‚åŒ…è£¹
- å¤§é‡å †ç Œä¸“ä¸šæœ¯è¯­ï¼Œä¸è§£é‡Šæˆ–ç¡¬æ€§çŒè¾“
- å­¦æœ¯æŠ¥å‘Šçš„è¯­æ°”å’Œç»“æ„
- å®Œå…¨ç¼ºä¹æ•…äº‹æ€§ã€å¯¹è¯ã€åœºæ™¯æå†™
- ä¸æ˜¯é¢å‘æ™®é€šè¯»è€…ï¼Œè€Œæ˜¯é¢å‘ä¸“ä¸šç ”ç©¶è€…

è¯·ä»¥JSONæ ¼å¼è¿”å›è¯„ä¼°ç»“æœ:
```json
{{
  "dimension_scores": {{
    "coherence": {{"score": 85, "reason": "...", "suggestions": ["..."]}},
    "creativity": {{"score": 75, "reason": "...", "suggestions": ["..."]}},
    ...
  }},
  "overall_reasons": ["...", "..."],
  "suggestions": ["...", "..."]
}}
```

è¯·ç¡®ä¿:
1. æ¯ä¸ªç»´åº¦çš„åˆ†æ•°åœ¨0-100ä¹‹é—´
2. ç†ç”±å…·ä½“æ˜ç¡®
3. å»ºè®®å…·æœ‰å¯æ“ä½œæ€§
"""

        return prompt

    def _parse_evaluation_response(
        self,
        response: str,
        criteria: Dict[EvaluationCriterion, float],
        task_type: str,
    ) -> EvaluationResult:
        """Parse LLM evaluation response"""

        try:
            # Extract JSON from response
            json_start = response.find("{")
            json_end = response.rfind("}") + 1

            if json_start >= 0 and json_end > json_start:
                json_str = response[json_start:json_end]
                data = json.loads(json_str)
            else:
                raise ValueError("No JSON found in response")

            # Build dimension scores
            dimension_scores = {}
            total_score = 0.0
            total_weight = 0.0

            for criterion, weight in criteria.items():
                criterion_key = criterion.value
                if criterion_key in data.get("dimension_scores", {}):
                    score_data = data["dimension_scores"][criterion_key]
                    score = score_data.get("score", 70) / 100.0  # Convert to 0-1

                    dimension_scores[criterion_key] = DimensionScore(
                        dimension=criterion_key,
                        score=score,
                        reason=score_data.get("reason", ""),
                        suggestions=score_data.get("suggestions", []),
                    )

                    total_score += score * weight
                    total_weight += weight
                else:
                    # Use default score if missing
                    dimension_scores[criterion_key] = DimensionScore(
                        dimension=criterion_key,
                        score=0.7,
                        reason="æœªè¯„ä¼°",
                        suggestions=[],
                    )
                    total_score += 0.7 * weight
                    total_weight += weight

            # Calculate overall score
            overall_score = total_score / total_weight if total_weight > 0 else 0.7

            # Collect reasons and suggestions
            all_reasons = data.get("overall_reasons", [])
            all_suggestions = data.get("suggestions", [])

            for dim_score in dimension_scores.values():
                if dim_score.reason:
                    all_reasons.append(f"{dim_score.dimension}: {dim_score.reason}")
                all_suggestions.extend(dim_score.suggestions)

            return EvaluationResult(
                passed=overall_score >= self.passing_threshold,
                score=overall_score,
                dimension_scores=dimension_scores,
                reasons=all_reasons,
                suggestions=all_suggestions,
                evaluator="llm_deepseek",
                metadata={"task_type": task_type},
            )

        except Exception as e:
            logger.error(f"Failed to parse evaluation response: {e}")
            # Return a default result
            return EvaluationResult(
                passed=True,  # Pass on error to avoid blocking
                score=0.7,
                dimension_scores={
                    c.value: DimensionScore(
                        dimension=c.value,
                        score=0.7,
                        reason="è¯„ä¼°è§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤åˆ†æ•°",
                        suggestions=[],
                    )
                    for c in criteria.keys()
                },
                reasons=["è¯„ä¼°è§£æå¤±è´¥"],
                suggestions=["è¯·é‡è¯•è¯„ä¼°"],
                evaluator="llm_deepseek_fallback",
            )

    async def _rule_based_evaluate(
        self,
        task_type: str,
        content: str,
        criteria: Dict[EvaluationCriterion, float],
        context: Optional[Dict[str, Any]] = None,
    ) -> EvaluationResult:
        """
        Perform rule-based evaluation (fallback)

        Uses simple heuristics to assess content quality
        """
        dimension_scores = {}
        total_score = 0.0
        total_weight = 0.0

        content_length = len(content)
        word_count = len(content.split())

        # Coherence: Check for reasonable length and structure
        coherence_score = min(1.0, min(content_length / 500, 1.0))
        if content_length < 100:
            coherence_score = 0.5
        dimension_scores[EvaluationCriterion.COHERENCE.value] = DimensionScore(
            dimension=EvaluationCriterion.COHERENCE.value,
            score=coherence_score,
            reason=f"å†…å®¹é•¿åº¦: {content_length}å­—ç¬¦",
            suggestions=["å†…å®¹è¿‡çŸ­ï¼Œå»ºè®®æ‰©å±•" if content_length < 200 else "é•¿åº¦é€‚ä¸­"],
        )
        total_score += coherence_score * criteria.get(EvaluationCriterion.COHERENCE, 0.2)
        total_weight += criteria.get(EvaluationCriterion.COHERENCE, 0.2)

        # Creativity: Check for variety in vocabulary
        unique_words = len(set(content.split()))
        vocabulary_diversity = unique_words / max(word_count, 1)
        creativity_score = min(1.0, vocabulary_diversity * 1.5)
        dimension_scores[EvaluationCriterion.CREATIVITY.value] = DimensionScore(
            dimension=EvaluationCriterion.CREATIVITY.value,
            score=creativity_score,
            reason=f"è¯æ±‡å¤šæ ·æ€§: {vocabulary_diversity:.2f}",
            suggestions=["å¢åŠ è¯æ±‡ä¸°å¯Œåº¦" if vocabulary_diversity < 0.5 else "è¯æ±‡ä¸°å¯Œåº¦è‰¯å¥½"],
        )
        total_score += creativity_score * criteria.get(EvaluationCriterion.CREATIVITY, 0.2)
        total_weight += criteria.get(EvaluationCriterion.CREATIVITY, 0.2)

        # Quality: Check for basic grammar indicators
        quality_score = 0.8  # Default good score
        issues = []
        if content.count("ã€‚") < content_length / 200:
            issues.append("å¥å­ç»“å°¾æ ‡ç‚¹å¯èƒ½ä¸è¶³")
            quality_score -= 0.1
        if content.count("\n") < content_length / 1000:
            issues.append("æ®µè½åˆ’åˆ†å¯èƒ½ä¸è¶³")
            quality_score -= 0.1
        dimension_scores[EvaluationCriterion.QUALITY.value] = DimensionScore(
            dimension=EvaluationCriterion.QUALITY.value,
            score=max(0.5, quality_score),
            reason="åŸºç¡€æ ¼å¼æ£€æŸ¥",
            suggestions=issues if issues else ["æ ¼å¼è‰¯å¥½"],
        )
        total_score += quality_score * criteria.get(EvaluationCriterion.QUALITY, 0.2)
        total_weight += criteria.get(EvaluationCriterion.QUALITY, 0.2)

        # Consistency and goal alignment: Default scores
        for criterion in [EvaluationCriterion.CONSISTENCY, EvaluationCriterion.GOAL_ALIGNMENT]:
            if criterion in criteria:
                default_score = 0.7
                dimension_scores[criterion.value] = DimensionScore(
                    dimension=criterion.value,
                    score=default_score,
                    reason="åŸºäºè§„åˆ™çš„é»˜è®¤è¯„ä¼°",
                    suggestions=["å»ºè®®ä½¿ç”¨LLMè¿›è¡Œæ›´å‡†ç¡®çš„è¯„ä¼°"],
                )
                total_score += default_score * criteria[criterion]
                total_weight += criteria[criterion]

        # Calculate overall score
        overall_score = total_score / total_weight if total_weight > 0 else 0.7

        all_reasons = [f"{d.dimension}: {d.reason}" for d in dimension_scores.values()]
        all_suggestions = []
        for d in dimension_scores.values():
            all_suggestions.extend(d.suggestions)

        return EvaluationResult(
            passed=overall_score >= self.passing_threshold,
            score=overall_score,
            dimension_scores=dimension_scores,
            reasons=all_reasons,
            suggestions=all_suggestions,
            evaluator="rule_based",
            metadata={"task_type": task_type},
        )

    def set_passing_threshold(self, threshold: float) -> None:
        """Update the passing threshold"""
        self.passing_threshold = max(0.0, min(1.0, threshold))
        logger.info(f"Updated passing threshold to {self.passing_threshold}")

    def set_criteria(
        self,
        criteria: Dict[EvaluationCriterion, float],
    ) -> None:
        """Update evaluation criteria"""
        # Validate weights sum to approximately 1.0
        total_weight = sum(criteria.values())
        if abs(total_weight - 1.0) > 0.1:
            logger.warning(
                f"Criteria weights sum to {total_weight}, "
                f"expected ~1.0. Normalizing..."
            )
            # Normalize
            self.criteria = {
                k: v / total_weight for k, v in criteria.items()
            }
        else:
            self.criteria = criteria.copy()

        logger.info(f"Updated evaluation criteria: {list(self.criteria.keys())}")
