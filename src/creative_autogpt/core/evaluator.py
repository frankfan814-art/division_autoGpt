"""
Evaluation Engine - Quality assessment for generated content

Implements multi-dimensional quality evaluation with configurable criteria.
Uses LLM-based evaluation for semantic quality assessment.
"""

import json
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional

from loguru import logger

from creative_autogpt.utils.llm_client import MultiLLMClient


class EvaluationCriterion(str, Enum):
    """Evaluation criteria for content quality"""

    COHERENCE = "coherence"  # Logical flow and consistency
    CREATIVITY = "creativity"  # Originality and innovation
    QUALITY = "quality"  # Writing quality and prose
    CONSISTENCY = "consistency"  # Consistency with established context
    GOAL_ALIGNMENT = "goal_alignment"  # Alignment with creation goals
    CHARACTER_VOICE = "character_voice"  # Character voice consistency
    PLOT_PROGRESSION = "plot_progression"  # Story development
    DIALOGUE_QUALITY = "dialogue_quality"  # Dialogue naturalness


@dataclass
class DimensionScore:
    """Score for a single evaluation dimension"""

    dimension: str
    score: float  # 0.0 to 1.0
    reason: str = ""
    suggestions: List[str] = field(default_factory=list)

    def to_dict(self) -> Dict[str, Any]:
        return {
            "dimension": self.dimension,
            "score": self.score,
            "reason": self.reason,
            "suggestions": self.suggestions,
        }


@dataclass
class EvaluationResult:
    """Result of content evaluation"""

    passed: bool
    score: float  # Overall score 0.0 to 1.0 (deprecated, use quality_score and consistency_score)
    dimension_scores: Dict[str, DimensionScore] = field(default_factory=dict)
    reasons: List[str] = field(default_factory=list)
    suggestions: List[str] = field(default_factory=list)
    evaluated_at: datetime = field(default_factory=datetime.utcnow)
    evaluator: str = "default"
    metadata: Dict[str, Any] = field(default_factory=dict)

    # ğŸ”¥ æ–°å¢ï¼šåˆ†åˆ«è·Ÿè¸ªè´¨é‡è¯„åˆ†å’Œä¸€è‡´æ€§è¯„åˆ†
    quality_score: float = 0.0  # æ–‡å­¦è´¨é‡è¯„åˆ† (0.0 - 1.0)
    consistency_score: float = 0.0  # é€»è¾‘ä¸€è‡´æ€§è¯„åˆ† (0.0 - 1.0)

    # ğŸ”¥ æ–°å¢ï¼šè¯¦ç»†çš„é—®é¢˜åˆ†ç±»
    quality_issues: List[str] = field(default_factory=list)  # è´¨é‡é—®é¢˜åˆ—è¡¨
    consistency_issues: List[str] = field(default_factory=list)  # ä¸€è‡´æ€§é—®é¢˜åˆ—è¡¨

    def to_dict(self) -> Dict[str, Any]:
        return {
            "passed": self.passed,
            "score": self.score,
            "quality_score": self.quality_score,  # æ–°å¢
            "consistency_score": self.consistency_score,  # æ–°å¢
            "dimension_scores": {
                k: v.to_dict() for k, v in self.dimension_scores.items()
            },
            "reasons": self.reasons,
            "suggestions": self.suggestions,
            "quality_issues": self.quality_issues,  # æ–°å¢
            "consistency_issues": self.consistency_issues,  # æ–°å¢
            "evaluated_at": self.evaluated_at.isoformat(),
            "evaluator": self.evaluator,
            "metadata": self.metadata,
        }


class EvaluationEngine:
    """
    Evaluates generated content quality across multiple dimensions

    Default criteria and weights:
    - Coherence: 20%
    - Creativity: 20%
    - Quality: 20%
    - Consistency: 20%
    - Goal Alignment: 20%

    Can be customized for different content types.
    """

    # Default evaluation criteria with weights
    DEFAULT_CRITERIA: Dict[EvaluationCriterion, float] = {
        EvaluationCriterion.COHERENCE: 0.20,
        EvaluationCriterion.CREATIVITY: 0.20,
        EvaluationCriterion.QUALITY: 0.20,
        EvaluationCriterion.CONSISTENCY: 0.20,
        EvaluationCriterion.GOAL_ALIGNMENT: 0.20,
    }

    # Content-specific criteria overrides
    CONTENT_TYPE_CRITERIA: Dict[str, Dict[EvaluationCriterion, float]] = {
        "ç« èŠ‚å†…å®¹": {
            EvaluationCriterion.COHERENCE: 0.15,
            EvaluationCriterion.CREATIVITY: 0.20,
            EvaluationCriterion.QUALITY: 0.25,
            EvaluationCriterion.CONSISTENCY: 0.20,
            EvaluationCriterion.CHARACTER_VOICE: 0.10,
            EvaluationCriterion.PLOT_PROGRESSION: 0.10,
        },
        "å¯¹è¯æ£€æŸ¥": {
            EvaluationCriterion.DIALOGUE_QUALITY: 0.40,
            EvaluationCriterion.CHARACTER_VOICE: 0.30,
            EvaluationCriterion.CONSISTENCY: 0.20,
            EvaluationCriterion.QUALITY: 0.10,
        },
        "å¤§çº²": {
            EvaluationCriterion.COHERENCE: 0.25,
            EvaluationCriterion.CREATIVITY: 0.25,
            EvaluationCriterion.GOAL_ALIGNMENT: 0.25,
            EvaluationCriterion.PLOT_PROGRESSION: 0.25,
        },
    }

    def __init__(
        self,
        llm_client: Optional[MultiLLMClient] = None,
        passing_threshold: float = 0.7,  # ğŸ”¥ é»˜è®¤é˜ˆå€¼ 0.7 (7/10) - ç¬¦åˆæ–‡æ¡£è¦æ±‚
        criteria: Optional[Dict[EvaluationCriterion, float]] = None,
    ):
        """
        Initialize evaluation engine

        Args:
            llm_client: LLM client for AI-based evaluation
            passing_threshold: Minimum score to pass (0.0 to 1.0)
            criteria: Custom criteria weights
        """
        self.llm_client = llm_client
        self.passing_threshold = passing_threshold
        self.criteria = criteria or self.DEFAULT_CRITERIA.copy()

        logger.info(
            f"EvaluationEngine initialized (threshold={passing_threshold}, "
            f"criteria={len(self.criteria)})"
        )

    async def evaluate(
        self,
        task_type: str,
        content: str,
        criteria: Optional[Dict[EvaluationCriterion, float]] = None,
        context: Optional[Dict[str, Any]] = None,
        goal: Optional[Dict[str, Any]] = None,
        predecessor_contents: Optional[Dict[str, str]] = None,
        chapter_context: Optional[str] = None,
    ) -> EvaluationResult:
        """
        Evaluate content quality

        Args:
            task_type: Type of content being evaluated
            content: The content to evaluate
            criteria: Custom criteria for this evaluation
            context: Additional context for evaluation
            goal: Original creation goals
            predecessor_contents: Contents from previous tasks for cross-task consistency check
            chapter_context: Previous chapter contents for continuity check

        Returns:
            EvaluationResult with scores and feedback
        """
        logger.debug(f"Evaluating content for task type: {task_type}")

        # Determine criteria to use
        eval_criteria = criteria or self._get_criteria_for_task_type(task_type)

        if self.llm_client:
            # Use LLM-based evaluation
            result = await self._llm_evaluate(
                task_type=task_type,
                content=content,
                criteria=eval_criteria,
                context=context,
                goal=goal,
                predecessor_contents=predecessor_contents,
                chapter_context=chapter_context,
            )
        else:
            # Use rule-based evaluation (fallback)
            result = await self._rule_based_evaluate(
                task_type=task_type,
                content=content,
                criteria=eval_criteria,
                context=context,
            )

        logger.info(
            f"Evaluation complete: score={result.score:.3f}, passed={result.passed}"
        )

        return result

    def _get_criteria_for_task_type(
        self,
        task_type: str,
    ) -> Dict[EvaluationCriterion, float]:
        """Get evaluation criteria for a specific task type"""
        return self.CONTENT_TYPE_CRITERIA.get(
            task_type,
            self.DEFAULT_CRITERIA,
        )

    async def _llm_evaluate(
        self,
        task_type: str,
        content: str,
        criteria: Dict[EvaluationCriterion, float],
        context: Optional[Dict[str, Any]] = None,
        goal: Optional[Dict[str, Any]] = None,
        predecessor_contents: Optional[Dict[str, str]] = None,
        chapter_context: Optional[str] = None,
    ) -> EvaluationResult:
        """
        Perform LLM-based evaluation

        Uses DeepSeek for logical evaluation (cost-effective)
        """
        # Build evaluation prompt
        prompt = self._build_evaluation_prompt(
            task_type=task_type,
            content=content,
            criteria=criteria,
            context=context,
            goal=goal,
            predecessor_contents=predecessor_contents,
            chapter_context=chapter_context,
        )

        try:
            # Use DeepSeek for evaluation (logic/reasoning strength)
            response = await self.llm_client.generate(
                prompt=prompt,
                task_type="è¯„ä¼°",  # Route to DeepSeek
                temperature=0.3,  # Lower temperature for consistent evaluation
                max_tokens=2000,
            )

            # Parse evaluation result
            return self._parse_evaluation_response(
                response.content,
                criteria,
                task_type,
            )

        except Exception as e:
            logger.error(f"LLM evaluation failed: {e}, falling back to rule-based")
            return await self._rule_based_evaluate(
                task_type=task_type,
                content=content,
                criteria=criteria,
                context=context,
            )

    def _build_evaluation_prompt(
        self,
        task_type: str,
        content: str,
        criteria: Dict[EvaluationCriterion, float],
        context: Optional[Dict[str, Any]] = None,
        goal: Optional[Dict[str, Any]] = None,
        predecessor_contents: Optional[Dict[str, str]] = None,
        chapter_context: Optional[str] = None,
    ) -> str:
        """Build prompt for LLM evaluation"""

        criteria_desc = "\n".join(
            f"- {c.value}: {w * 100:.0f}%æƒé‡"
            for c, w in criteria.items()
        )

        context_section = ""
        if context:
            context_section = f"\n\nä¸Šä¸‹æ–‡ä¿¡æ¯:\n{json.dumps(context, ensure_ascii=False, indent=2)}"

        goal_section = ""
        if goal:
            goal_section = f"\n\nåˆ›ä½œç›®æ ‡:\n{json.dumps(goal, ensure_ascii=False, indent=2)}"

        # ğŸ”¥ æ–°å¢ï¼šå‰ç½®ä»»åŠ¡å†…å®¹ï¼ˆç”¨äºè·¨ä»»åŠ¡ä¸€è‡´æ€§æ£€æŸ¥ï¼‰
        predecessor_section = ""
        if predecessor_contents:
            priority_list = ["äººç‰©è®¾è®¡", "ä¸–ç•Œè§‚è§„åˆ™", "é£æ ¼å…ƒç´ ", "å¤§çº²", "ä¼ç¬”åˆ—è¡¨", "äº‹ä»¶", "åœºæ™¯ç‰©å“å†²çª"]
            predecessor_section = "\n\n### å‰é¢ä»»åŠ¡çš„æ ¸å¿ƒæˆæœï¼ˆå¿…é¡»ä¸¥æ ¼ä¿æŒä¸€è‡´ï¼‰\n"
            for pred_type in priority_list:
                if pred_type in predecessor_contents:
                    pred_content = predecessor_contents[pred_type]
                    max_len = 4000 if pred_type in ["äººç‰©è®¾è®¡", "å¤§çº²"] else 2000
                    predecessor_section += f"\n#### {pred_type}\n```\n{pred_content[:max_len]}{'...' if len(pred_content) > max_len else ''}\n```\n"

        # ğŸ”¥ æ–°å¢ï¼šç« èŠ‚ä¸Šä¸‹æ–‡ï¼ˆç”¨äºç« èŠ‚è¿è´¯æ€§æ£€æŸ¥ï¼‰
        chapter_context_section = ""
        if chapter_context:
            chapter_context_section = f"\n\n### å‰é¢ç« èŠ‚å†…å®¹ï¼ˆç« èŠ‚è¿è´¯æ€§æ£€æŸ¥ï¼‰\n{chapter_context}"

        # ğŸ”¥ æ ¹æ®ä»»åŠ¡ç±»å‹æä¾›å…·ä½“çš„è¯„ä¼°è¦ç‚¹
        task_specific_criteria = self._get_task_evaluation_criteria(task_type)

        prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å°è¯´å†…å®¹è¯„ä¼°ä¸“å®¶ã€‚ä½ æ­£åœ¨è¯„ä¼°ä¸€éƒ¨**å°è¯´**çš„{task_type}å†…å®¹è´¨é‡ã€‚

âš ï¸ é‡è¦æé†’ï¼š
- è¿™æ˜¯å°è¯´åˆ›ä½œï¼Œä¸æ˜¯å­¦æœ¯è®ºæ–‡æˆ–ç§‘å­¦ç ”ç©¶æŠ¥å‘Š
- å†…å®¹åº”è¯¥æ˜¯æ•…äº‹æ€§çš„ã€æ–‡å­¦æ€§çš„ã€é¢å‘å¤§ä¼—è¯»è€…çš„
- å¿…é¡»ä½¿ç”¨å°è¯´çš„å™äº‹è¯­è¨€ï¼Œè€Œä¸æ˜¯å­¦æœ¯è®ºæ–‡è¯­è¨€

ğŸ“– ç§‘å¹»å°è¯´ç‰¹æ®Šè§„åˆ™ï¼ˆå‚è€ƒã€Šä¸‰ä½“ã€‹ã€Šæµæµªåœ°çƒã€‹æ ‡å‡†ï¼‰ï¼š
- âœ… å…è®¸ï¼šé€‚åº¦çš„ç§‘å­¦æ¦‚å¿µã€æŠ€æœ¯è®¾å®šã€æœªæ¥ç§‘æŠ€æè¿°
- âœ… å…è®¸ï¼šå¿…è¦çš„ç§‘å­¦æœ¯è¯­ï¼Œä½†å¿…é¡»é€šè¿‡æ•…äº‹æƒ…èŠ‚è‡ªç„¶å‘ˆç°
- âœ… å…è®¸ï¼šç”¨é€šä¿—æ˜“æ‡‚çš„æ–¹å¼è§£é‡Šç§‘å­¦åŸç†ï¼ˆåƒåˆ˜æ…ˆæ¬£çš„å†™æ³•ï¼‰
- âŒ ç¦æ­¢ï¼šå †ç Œå¤æ‚å…¬å¼ã€å­¦æœ¯è®ºæ–‡å¼çš„ç†è®ºæ¨å¯¼
- âŒ ç¦æ­¢ï¼šçº¯æŠ€æœ¯æ–‡æ¡£å¼çš„æè¿°ã€ç¼ºä¹æ•…äº‹æ€§
- âŒ ç¦æ­¢ï¼šé¢å‘ä¸“ä¸šç ”ç©¶è€…çš„å­¦æœ¯å†™ä½œé£æ ¼

æ ¸å¿ƒæ ‡å‡†ï¼šç§‘å­¦è®¾å®šæœåŠ¡äºæ•…äº‹ï¼Œè€Œä¸æ˜¯å±•ç¤ºå­¦æœ¯ç ”ç©¶ã€‚

## è¯„ä¼°æ ‡å‡†
{criteria_desc}

## {task_type}å…·ä½“è¯„ä¼°è¦ç‚¹
{task_specific_criteria}

## å¾…è¯„ä¼°å†…å®¹
```
{content[:8000]}
```
{context_section}
{goal_section}
{predecessor_section}
{chapter_context_section}

## è¯„ä¼°è¦æ±‚

### ğŸ” ç”¨æˆ·è¾“å…¥ä¸€è‡´æ€§æ£€æŸ¥ï¼ˆä»…é’ˆå¯¹åˆ›æ„è„‘æš´ï¼‰

å¦‚æœå‰ç½®å†…å®¹ä¸­åŒ…å«ã€ç”¨æˆ·è¾“å…¥ã€‘ï¼Œè¯´æ˜è¿™æ˜¯åˆ›æ„è„‘æš´ä»»åŠ¡ï¼Œè¯·åŠ¡å¿…æ£€æŸ¥è„‘æš´ç»“æœæ˜¯å¦**ä¸¥æ ¼éµå¾ª**ç”¨æˆ·çš„åŸå§‹è¾“å…¥ï¼š

1. **ç±»å‹/æµæ´¾ä¸€è‡´æ€§**ï¼š
   - ç‚¹å­æ˜¯å¦ç¬¦åˆç”¨æˆ·æŒ‡å®šçš„ã€ç±»å‹/æµæ´¾ã€‘ï¼Ÿ
   - å¦‚æœç”¨æˆ·è¦æ±‚"ç§‘å¹»"ï¼Œç‚¹å­æ˜¯å¦åŒ…å«ç§‘å¹»å…ƒç´ ï¼Ÿ

2. **é£æ ¼ä¸€è‡´æ€§**ï¼š
   - ç‚¹å­æ˜¯å¦ç¬¦åˆç”¨æˆ·æŒ‡å®šçš„ã€å†™ä½œé£æ ¼ã€‘ï¼Ÿ
   - å¦‚æœç”¨æˆ·è¦æ±‚"é»‘æš—é£æ ¼"ï¼Œç‚¹å­æ˜¯å¦ä½“ç°è¿™ç§é£æ ¼ï¼Ÿ

3. **åˆ›ä½œè¦æ±‚ä¸€è‡´æ€§**ï¼š
   - ç‚¹å­æ˜¯å¦æ»¡è¶³ç”¨æˆ·çš„ã€åˆ›ä½œè¦æ±‚ã€‘ï¼Ÿ
   - ç”¨æˆ·è¦æ±‚çš„å…³é”®è¦ç´ æ˜¯å¦éƒ½ä½“ç°åœ¨ç‚¹å­ä¸­ï¼Ÿ

4. **è§„æ¨¡åŒ¹é…åº¦**ï¼š
   - ç‚¹å­çš„è§„æ¨¡æ˜¯å¦é€‚åˆã€ç›®æ ‡å­—æ•°ã€‘å’Œã€ç« èŠ‚æ•°é‡ã€‘ï¼Ÿ
   - å¦‚æœç”¨æˆ·è¦æ±‚100ä¸‡å­—ï¼Œç‚¹å­æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ‰©å±•ç©ºé—´ï¼Ÿ

âš ï¸ **ç”¨æˆ·è¾“å…¥ä¸€è‡´æ€§è¯„åˆ¤æ ‡å‡†**ï¼š
- **ä¸¥é‡é—®é¢˜**ï¼šå®Œå…¨è¿èƒŒç”¨æˆ·æ˜ç¡®è¦æ±‚çš„ç±»å‹/é£æ ¼/æ ¸å¿ƒè®¾å®š
- **ä¸­ç­‰é—®é¢˜**ï¼šéƒ¨åˆ†åç¦»ç”¨æˆ·è¦æ±‚ï¼Œä½†ä»å¯è°ƒæ•´
- **å°é—®é¢˜**ï¼šç»†èŠ‚ä¸Šä¸å¤Ÿè´´åˆç”¨æˆ·è¦æ±‚

---

### ğŸ” è·¨ä»»åŠ¡ä¸€è‡´æ€§æ£€æŸ¥ï¼ˆé‡è¦ï¼ï¼‰

å¦‚æœæä¾›äº†ã€å‰é¢ä»»åŠ¡çš„æ ¸å¿ƒæˆæœã€‘ï¼Œè¯·åŠ¡å¿…æ£€æŸ¥å½“å‰å†…å®¹ä¸å‰é¢ä»»åŠ¡çš„**ä¸¥æ ¼ä¸€è‡´æ€§**ï¼š

1. **å¤§çº²ä¸€è‡´æ€§**ï¼š
   - æ˜¯å¦ç´§æ‰£ã€å¤§çº²ã€‘ä¸­å®šä¹‰çš„ä¸»è§’ç›®æ ‡å’Œæ ¸å¿ƒå†²çªï¼Ÿ
   - æ˜¯å¦æœåŠ¡äºå¤§çº²çš„æ ¸å¿ƒæƒ…æ„Ÿé’©å­ï¼Ÿ

2. **äººç‰©ä¸€è‡´æ€§**ï¼š
   - å¦‚æœæ¶‰åŠäººç‰©ï¼Œæ˜¯å¦ä½¿ç”¨äº†ã€äººç‰©è®¾è®¡ã€‘ä¸­å·²æœ‰çš„è§’è‰²ï¼Ÿ
   - äººç‰©çš„æ€§æ ¼ã€èƒŒæ™¯ã€ç›®æ ‡æ˜¯å¦ä¸è®¾è®¡ä¸€è‡´ï¼Ÿ
   - æœ‰æ²¡æœ‰å‡­ç©ºå‡ºç°çš„æ–°è§’è‰²ï¼ˆåº”è¯¥é¿å…ï¼‰ï¼Ÿ

3. **ä¸–ç•Œè§‚ä¸€è‡´æ€§**ï¼š
   - æ˜¯å¦ç¬¦åˆã€ä¸–ç•Œè§‚è§„åˆ™ã€‘ä¸­çš„è®¾å®šï¼Ÿ
   - æœ‰æ²¡æœ‰è¿åå·²è®¾å®šçš„è§„åˆ™ï¼Ÿ
   - æ–°å¢çš„è®¾å®šæ˜¯å¦ä¸å·²æœ‰è®¾å®šå†²çªï¼Ÿ

4. **é£æ ¼ä¸€è‡´æ€§**ï¼š
   - å†™ä½œé£æ ¼æ˜¯å¦ç¬¦åˆã€é£æ ¼å…ƒç´ ã€‘çš„è¦æ±‚ï¼Ÿ
   - è¯­è¨€è°ƒæ€§æ˜¯å¦ç»Ÿä¸€ï¼Ÿ

5. **ä¸»é¢˜ä¸€è‡´æ€§**ï¼š
   - æ˜¯å¦å›´ç»•ã€å¤§çº²ã€‘çš„æ ¸å¿ƒä¸»é¢˜å±•å¼€ï¼Ÿ
   - æœ‰æ²¡æœ‰åç¦»ä¸»é¢˜ã€è·‘é¢˜çš„å†…å®¹ï¼Ÿ

### ğŸ“– ç« èŠ‚è¿è´¯æ€§æ£€æŸ¥ï¼ˆå¦‚æœæä¾›äº†å‰é¢ç« èŠ‚å†…å®¹ï¼‰

å¦‚æœæä¾›äº†ã€å‰é¢ç« èŠ‚å†…å®¹ã€‘ï¼Œè¯·åŠ¡å¿…æ£€æŸ¥ç« èŠ‚ä¹‹é—´çš„è¿è´¯æ€§ï¼š

1. **å¼€å¤´è¡”æ¥**ï¼š
   - æœ¬ç« å¼€å¤´æ˜¯å¦è‡ªç„¶è¡”æ¥ä¸Šä¸€ç« ç»“å°¾ï¼Ÿ
   - æœ‰æ²¡æœ‰çªå…€çš„è½¬æ¢æˆ–å‰²è£‚æ„Ÿï¼Ÿ

2. **äººç‰©çŠ¶æ€å»¶ç»­**ï¼š
   - äººç‰©ä½ç½®ã€æƒ…ç»ªã€çŠ¶æ€æ˜¯å¦ä¸ä¸Šä¸€ç« ç»“å°¾ä¸€è‡´ï¼Ÿ
   - æœ‰æ²¡æœ‰å‡ºç°ä¸åˆç†çš„çŠ¶æ€å˜åŒ–ï¼Ÿ

3. **æ—¶é—´çº¿è¿è´¯**ï¼š
   - æ—¶é—´çº¿æ˜¯å¦åˆç†ï¼Ÿ
   - æœ‰æ²¡æœ‰æ—¶é—´è·³è·ƒæˆ–çŸ›ç›¾ï¼Ÿ

4. **æ•´ä½“è¿è´¯æ€§**ï¼š
   - æœ¬ç« æ˜¯å¦åƒç‹¬ç«‹çŸ­ç¯‡ï¼Œä¸å‰é¢è„±èŠ‚ï¼Ÿ
   - æ˜¯å¦ä¿æŒäº†å™äº‹çš„è¿ç»­æ€§ï¼Ÿ

âš ï¸ **ä¸€è‡´æ€§æ£€æŸ¥è¯„åˆ¤æ ‡å‡†**ï¼ˆä¸¥æ ¼è¦æ±‚ï¼‰ï¼š
- **9-10åˆ†**ï¼šå®Œç¾ä¸€è‡´ï¼Œæ²¡æœ‰ä»»ä½•ç‘•ç–µï¼Œè¿‘ä¹å®Œç¾çš„ä½œå“
- **8-9åˆ†**ï¼šé«˜åº¦ä¸€è‡´ï¼Œæœ‰æä¸ªåˆ«å°ç‘•ç–µä½†ä¸å½±å“é˜…è¯»ï¼ˆä¼˜ç§€ä½œå“æ°´å¹³ï¼‰
- **7-8åˆ†**ï¼šåŸºæœ¬ä¸€è‡´ï¼Œæœ‰ä¸€äº›å°é—®é¢˜ä½†ä¸ä¸¥é‡ï¼ˆåˆæ ¼ä½œå“æ°´å¹³ï¼‰
- **6-7åˆ†**ï¼šæœ‰æ˜æ˜¾ä¸€è‡´æ€§é—®é¢˜ï¼Œéœ€è¦ä¿®æ”¹ï¼ˆå‹‰å¼ºåŠæ ¼ï¼‰
- **6åˆ†ä»¥ä¸‹**ï¼šä¸€è‡´æ€§å¾ˆå·®ï¼Œå¿…é¡»é‡å†™ï¼ˆä¸é€šè¿‡ï¼‰

---

## è¯„ä¼°æ ¼å¼è¦æ±‚

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¿”å›è¯„ä¼°ç»“æœï¼ˆ**å¿…é¡»ä¸¥æ ¼éµå¾ªæ­¤æ ¼å¼**ï¼‰ï¼š

### ç»¼åˆè´¨é‡è¯„åˆ†ï¼šX/10
ï¼ˆç»™å‡ºä¸€ä¸ª0-10çš„åˆ†æ•°ï¼Œ7åˆ†ä»¥ä¸Šä¸ºé€šè¿‡ï¼Œ8åˆ†ä»¥ä¸Šä¸ºä¼˜ç§€ï¼‰

### ä¸€è‡´æ€§è¯„åˆ†ï¼šX/10
ï¼ˆç»™å‡ºä¸€ä¸ª0-10çš„åˆ†æ•°ï¼Œ7åˆ†ä»¥ä¸Šä¸ºé€šè¿‡ï¼Œ8åˆ†ä»¥ä¸Šä¸ºä¼˜ç§€ï¼‰

### ä¸€ã€æ–‡å­¦è´¨é‡è¯„åˆ†

| ç»´åº¦ | è¯„åˆ† | ä¼˜ç‚¹ | ä¸è¶³ |
|------|------|------|------|
| è¿è´¯æ€§ | X/10 | ... | ... |
| åˆ›é€ æ€§ | X/10 | ... | ... |
| æ–‡ç¬” | X/10 | ... | ... |

**è´¨é‡é—®é¢˜æ€»ç»“**ï¼š
- é—®é¢˜1ï¼šå…·ä½“æè¿°...
- é—®é¢˜2ï¼šå…·ä½“æè¿°...
- é—®é¢˜3ï¼šå…·ä½“æè¿°...

### äºŒã€é€»è¾‘ä¸€è‡´æ€§æ£€æŸ¥

ğŸ”´ **ä¸¥é‡é—®é¢˜**ï¼ˆå¿…é¡»ä¿®å¤ï¼‰ï¼š
- å…·ä½“æè¿°ä¸¥é‡çš„é€»è¾‘çŸ›ç›¾æˆ–ä¸ä¸€è‡´ä¹‹å¤„...

ğŸŸ¡ **ä¸­ç­‰é—®é¢˜**ï¼ˆå»ºè®®ä¿®å¤ï¼‰ï¼š
- å…·ä½“æè¿°ä¸­ç­‰é—®é¢˜...

ğŸŸ¢ **æ— æ˜æ˜¾é—®é¢˜**ï¼ˆå¦‚æœæ²¡æœ‰ä¸¥é‡å’Œä¸­ç­‰é—®é¢˜ï¼‰

### ä¸‰ã€è¯„ä¼°ç»“è®º

**æ˜¯å¦é€šè¿‡**ï¼šæ˜¯/å¦

**ä¸€å¥è¯è¯„ä»·**ï¼šç”¨ä¸€å¥è¯æ€»ç»“ä½ çš„è¯„ä»·

**è¯¦ç»†ç†ç”±**ï¼š
1. ç†ç”±1...
2. ç†ç”±2...
3. ç†ç”±3...

### å››ã€æ”¹è¿›å»ºè®®

**å¿…é¡»ä¿®æ”¹**ï¼š
1. [å…·ä½“ä¿®æ”¹å»ºè®®1] - è¯´æ˜ä¸ºä»€ä¹ˆéœ€è¦è¿™æ ·æ”¹ï¼Œä»¥åŠå…·ä½“æ€ä¹ˆæ”¹
2. [å…·ä½“ä¿®æ”¹å»ºè®®2] - è¯´æ˜ä¸ºä»€ä¹ˆéœ€è¦è¿™æ ·æ”¹ï¼Œä»¥åŠå…·ä½“æ€ä¹ˆæ”¹

**å»ºè®®ä¼˜åŒ–**ï¼š
1. [å…·ä½“ä¼˜åŒ–å»ºè®®1]
2. [å…·ä½“ä¼˜åŒ–å»ºè®®2]

### äº”ã€äº®ç‚¹æ€»ç»“
- äº®ç‚¹1...
- äº®ç‚¹2...

---

## âš ï¸ å…³é”®è¦æ±‚ï¼š

1. **å…·ä½“æ˜ç¡®**ï¼šæ¯ä¸ªé—®é¢˜å’Œå»ºè®®éƒ½å¿…é¡»å…·ä½“ï¼Œä¸è¦ç”¨"ä¸é”™"ã€"å¯ä»¥æ”¹è¿›"ç­‰æ¨¡ç³Šè¯è¯­
2. **æœ‰é’ˆå¯¹æ€§**ï¼šé’ˆå¯¹{task_type}çš„ç‰¹ç‚¹è¿›è¡Œè¯„ä¼°
3. **å¯æ“ä½œ**ï¼šå»ºè®®å¿…é¡»æ˜¯å¯ä»¥ç›´æ¥æ‰§è¡Œçš„ï¼Œä¾‹å¦‚"å¢åŠ ä¸»è§’å¯¹æœªæ¥çš„ææƒ§æå†™"è€Œä¸æ˜¯"å¢åŠ æƒ…æ„Ÿæå†™"
4. **ä¸¤åˆ†æ³•**ï¼šæ˜ç¡®åŒºåˆ†"è´¨é‡é—®é¢˜"ï¼ˆæ–‡å­¦æ€§ï¼‰å’Œ"ä¸€è‡´æ€§é—®é¢˜"ï¼ˆé€»è¾‘æ€§ï¼‰
5. **è¯„åˆ†è¯´æ˜**ï¼šå¦‚æœè¯„åˆ†ä½äº8åˆ†ï¼Œå¿…é¡»åœ¨"è´¨é‡é—®é¢˜æ€»ç»“"å’Œ"é€»è¾‘ä¸€è‡´æ€§æ£€æŸ¥"ä¸­è¯´æ˜å…·ä½“åŸå› 

## è¯„åˆ†æ ‡å‡†å‚è€ƒï¼š
- 9-10åˆ†ï¼šä¼˜ç§€ï¼Œä»…æœ‰å¾®ä¸è¶³é“çš„å°é—®é¢˜
- 8-9åˆ†ï¼šè‰¯å¥½ï¼Œæœ‰å°é—®é¢˜ä½†ä¸å½±å“æ•´ä½“
- 6-8åˆ†ï¼šåŠæ ¼ï¼Œæœ‰æ˜æ˜¾é—®é¢˜éœ€è¦ä¿®æ”¹
- 4-6åˆ†ï¼šä¸åŠæ ¼ï¼Œæœ‰é‡å¤§é—®é¢˜éœ€è¦é‡å†™
- 0-4åˆ†ï¼šå®Œå…¨ä¸åˆæ ¼ï¼Œå®Œå…¨ä¸ç¬¦åˆè¦æ±‚
"""

        return prompt

    def _parse_evaluation_response(
        self,
        response: str,
        criteria: Dict[EvaluationCriterion, float],
        task_type: str,
    ) -> EvaluationResult:
        """Parse LLM evaluation response

        æ”¯æŒä¸¤ç§æ ¼å¼:
        1. JSONæ ¼å¼ (æ—§æ ¼å¼)
        2. æ–°æ ¼å¼ (è´¨é‡+ä¸€è‡´æ€§è¯„åˆ†)
        """

        try:
            # ğŸ”¥ ä¼˜å…ˆå°è¯•è§£ææ–°æ ¼å¼ (è´¨é‡+ä¸€è‡´æ€§è¯„åˆ†)
            new_format_result = self._try_parse_new_format(response, task_type)
            if new_format_result:
                return new_format_result

            # å›é€€åˆ°æ—§JSONæ ¼å¼
            return self._parse_json_format(response, criteria, task_type)

        except Exception as e:
            logger.error(f"Failed to parse evaluation response: {e}")
            # Return a default result
            return EvaluationResult(
                passed=False,  # è§£æå¤±è´¥æ—¶é»˜è®¤ä¸é€šè¿‡
                score=0.7,
                quality_score=0.7,
                consistency_score=0.7,
                dimension_scores={},
                reasons=[f"è¯„ä¼°è§£æå¤±è´¥: {str(e)}"],
                suggestions=["è¯·é‡æ–°ç”Ÿæˆ"],
                quality_issues=[],
                consistency_issues=[],
                evaluator="parse_error",
                metadata={"task_type": task_type, "error": str(e)},
            )

    def _try_parse_new_format(
        self, response: str, task_type: str
    ) -> Optional[EvaluationResult]:
        """
        å°è¯•è§£ææ–°æ ¼å¼ (è´¨é‡+ä¸€è‡´æ€§è¯„åˆ†)
        æ–°æ ¼å¼ä½¿ç”¨ç»“æ„åŒ–æ–‡æœ¬è€ŒéJSONï¼Œæ›´å®¹æ˜“è¢«LLMæ­£ç¡®ç”Ÿæˆ
        """

        import re

        # æå–ç»¼åˆè´¨é‡è¯„åˆ†
        quality_match = re.search(r"ç»¼åˆè´¨é‡è¯„åˆ†[ï¼š:]\s*(\d+(?:\.\d+)?)[/ï¼]10", response)
        quality_score = float(quality_match.group(1)) / 10.0 if quality_match else None

        # æå–ä¸€è‡´æ€§è¯„åˆ†
        consistency_match = re.search(r"ä¸€è‡´æ€§è¯„åˆ†[ï¼š:]\s*(\d+(?:\.\d+)?)[/ï¼]10", response)
        consistency_score = (
            float(consistency_match.group(1)) / 10.0 if consistency_match else None
        )

        # å¦‚æœéƒ½æ²¡æœ‰æå–åˆ°ï¼Œè¿”å›None (ä½¿ç”¨æ—§æ ¼å¼)
        if quality_score is None and consistency_score is None:
            return None

        # å¦‚æœåªæœ‰ä¸€ä¸ªæœ‰å€¼ï¼Œå¦ä¸€ä¸ªä½¿ç”¨é»˜è®¤å€¼
        if quality_score is None:
            quality_score = consistency_score if consistency_score else 0.7
        if consistency_score is None:
            consistency_score = quality_score if quality_score else 0.7

        # ğŸ”¥ æå–è´¨é‡é—®é¢˜ - æ”¹è¿›ç‰ˆ
        quality_issues = []

        # æ–¹æ³•1: ä»"è´¨é‡é—®é¢˜æ€»ç»“"éƒ¨åˆ†æå–
        quality_summary_section = re.search(
            r"\*\*è´¨é‡é—®é¢˜æ€»ç»“\*\*[ï¼š:]?\s*\n((?:[-â€¢]\s*.*?\n)+)", response, re.MULTILINE
        )
        if quality_summary_section:
            for line in quality_summary_section.group(1).split("\n"):
                line = line.strip()
                if line.startswith("-") or line.startswith("â€¢"):
                    # æå–"é—®é¢˜Xï¼š"åé¢çš„å†…å®¹
                    issue = re.sub(r"^é—®é¢˜\d+[ï¼š:]\s*", "", line.lstrip("-â€¢*").strip())
                    if issue and len(issue) > 5:
                        quality_issues.append(issue)

        # æ–¹æ³•2: å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œä»"ä¸è¶³"åˆ—æå–
        if not quality_issues:
            quality_section = re.search(
                r"###?\s*ä¸€ã€æ–‡å­¦è´¨é‡è¯„åˆ†.*?(?=###?\s*äºŒã€|###?\s*é€»è¾‘ä¸€è‡´æ€§|\Z)", response, re.DOTALL
            )
            if quality_section:
                # åœ¨è¡¨æ ¼ä¸­æŸ¥æ‰¾"ä¸è¶³"åˆ—çš„å†…å®¹
                lines = quality_section.group(0).split("\n")
                for i, line in enumerate(lines):
                    if "ä¸è¶³" in line:
                        # æ‰¾åˆ°ä¸è¶³åˆ—çš„ç´¢å¼•
                        cells = [c.strip() for c in line.split("|")]
                        try:
                            shortage_idx = cells.index("ä¸è¶³")
                            # è¯»å–ä¸‹ä¸€è¡Œçš„ä¸è¶³å†…å®¹
                            if i + 1 < len(lines):
                                next_cells = [c.strip() for c in lines[i + 1].split("|")]
                                if shortage_idx < len(next_cells):
                                    issue = next_cells[shortage_idx].strip()
                                    if issue and len(issue) > 3 and issue != "...":
                                        quality_issues.append(issue)
                        except ValueError:
                            pass

        # æ–¹æ³•3: æŸ¥æ‰¾æ–‡æœ¬ä¸­çš„è´¨é‡é—®é¢˜åˆ—è¡¨
        if not quality_issues:
            quality_list_patterns = [
                r"(?:è´¨é‡é—®é¢˜|ä¸è¶³|ç¼ºç‚¹)[ï¼š:]?\s*\n((?:[-â€¢]\s*.*?\n){1,5})",
                r"(?:è´¨é‡|æ–‡å­¦)[^ã€‚]*?é—®é¢˜[ï¼š:]\s*([^\n]+)",
            ]
            for pattern in quality_list_patterns:
                match = re.search(pattern, response, re.DOTALL)
                if match:
                    text = match.group(1)
                    for line in text.split("\n"):
                        line = line.strip()
                        if line.startswith("-") or line.startswith("â€¢"):
                            issue = line.lstrip("-â€¢*").strip()
                            if issue and len(issue) > 5:
                                quality_issues.append(issue)
                    if quality_issues:
                        break

        # ğŸ”¥ æå–ä¸€è‡´æ€§é—®é¢˜ - æ”¹è¿›ç‰ˆ
        consistency_issues = []

        # æå–ä¸¥é‡é—®é¢˜
        critical_section = re.search(
            r"ğŸ”´\s*\*\*ä¸¥é‡é—®é¢˜\*\*.*?(?=ğŸŸ¡|ğŸŸ¢|âœ…|###?\s*ä¸‰ã€|###?\s*è¯„ä¼°ç»“è®º|\Z)", response, re.DOTALL
        )
        if critical_section:
            for line in critical_section.group(0).split("\n"):
                line = line.strip()
                if line.startswith("-") or line.startswith("â€¢"):
                    issue = line.lstrip("-â€¢*").strip()
                    if issue and len(issue) > 5:
                        consistency_issues.append(f"[ä¸¥é‡] {issue}")

        # æå–ä¸­ç­‰é—®é¢˜
        medium_section = re.search(
            r"ğŸŸ¡\s*\*\*ä¸­ç­‰é—®é¢˜\*\*.*?(?=ğŸŸ¢|âœ…|###?\s*ä¸‰ã€|###?\s*è¯„ä¼°ç»“è®º|\Z)", response, re.DOTALL
        )
        if medium_section:
            for line in medium_section.group(0).split("\n"):
                line = line.strip()
                if line.startswith("-") or line.startswith("â€¢"):
                    issue = line.lstrip("-â€¢*").strip()
                    if issue and len(issue) > 5:
                        consistency_issues.append(f"[ä¸­ç­‰] {issue}")

        # ğŸ”¥ æå–æ€»ä½“è¯„ä»·
        overall_comment = ""
        comment_match = re.search(r"ä¸€å¥è¯è¯„ä»·[ï¼š:]\s*(.+?)(?:\n|$)", response)
        if comment_match:
            overall_comment = comment_match.group(1).strip()

        # ğŸ”¥ æå–è¯¦ç»†ç†ç”±
        detailed_reasons = []
        reasons_section = re.search(
            r"\*\*è¯¦ç»†ç†ç”±\*\*[ï¼š:]?\s*\n((?:\d+[\.ã€]\s*.*?\n)+)", response, re.MULTILINE
        )
        if reasons_section:
            for line in reasons_section.group(1).split("\n"):
                line = line.strip()
                # æå–ç¼–å·åˆ—è¡¨
                match = re.match(r"\d+[\.ã€]\s*(.+)", line)
                if match:
                    reason = match.group(1).strip()
                    if reason and len(reason) > 5:
                        detailed_reasons.append(reason)

        # ğŸ”¥ æå–äº®ç‚¹
        highlights = []
        highlight_section = re.search(
            r"###?\s*äº”ã€äº®ç‚¹æ€»ç»“.*?(?=---|\Z)", response, re.DOTALL
        )
        if highlight_section:
            for line in highlight_section.group(0).split("\n"):
                line = line.strip()
                if line.startswith("-") or line.startswith("â€¢") or line.startswith("â€”"):
                    highlight = line.lstrip("-â€”â€¢*").strip()
                    if highlight and len(highlight) > 5:
                        highlights.append(highlight)

        # ğŸ”¥ æå–æ”¹è¿›å»ºè®®
        suggestions = []

        # æå–"å¿…é¡»ä¿®æ”¹"
        must_fix_section = re.search(
            r"\*\*å¿…é¡»ä¿®æ”¹\*\*[ï¼š:]?\s*\n((?:\d+[\.ã€]\s*.*?\n)+)", response, re.MULTILINE
        )
        if must_fix_section:
            for line in must_fix_section.group(1).split("\n"):
                line = line.strip()
                match = re.match(r"\d+[\.ã€]\s*(.+)", line)
                if match:
                    suggestion = match.group(1).strip()
                    if suggestion and len(suggestion) > 10:
                        suggestions.append(f"[å¿…é¡»] {suggestion}")

        # æå–"å»ºè®®ä¼˜åŒ–"
        should_optimize_section = re.search(
            r"\*\*å»ºè®®ä¼˜åŒ–\*\*[ï¼š:]?\s*\n((?:\d+[\.ã€]\s*.*?\n)+)", response, re.MULTILINE
        )
        if should_optimize_section:
            for line in should_optimize_section.group(1).split("\n"):
                line = line.strip()
                match = re.match(r"\d+[\.ã€]\s*(.+)", line)
                if match:
                    suggestion = match.group(1).strip()
                    if suggestion and len(suggestion) > 10:
                        suggestions.append(f"[å»ºè®®] {suggestion}")

        # ğŸ”¥ æ„å»ºreasons
        reasons = []

        if overall_comment:
            reasons.append(overall_comment)

        if detailed_reasons:
            reasons.extend(detailed_reasons[:3])

        if highlights:
            reasons.append("äº®ç‚¹: " + "; ".join(highlights[:3]))

        # å¦‚æœæ²¡æœ‰æå–åˆ°å…·ä½“å»ºè®®ï¼Œä½¿ç”¨è´¨é‡é—®é¢˜å’Œä¸€è‡´æ€§é—®é¢˜ä½œä¸ºå»ºè®®
        if not suggestions:
            # è´¨é‡é—®é¢˜ä½œä¸ºå»ºè®®
            if quality_issues:
                suggestions.extend([f"è´¨é‡æ”¹è¿›: {issue}" for issue in quality_issues[:3]])

            # ä¸€è‡´æ€§é—®é¢˜ä½œä¸ºå»ºè®®
            if consistency_issues:
                suggestions.extend(
                    [f"ä¸€è‡´æ€§ä¿®å¤: {issue}" for issue in consistency_issues[:3]]
                )

        # ğŸ”¥ åˆ¤æ–­æ˜¯å¦é€šè¿‡ï¼šä¸¤ä¸ªåˆ†æ•°éƒ½éœ€è¦ >= passing_threshold
        passed = quality_score >= self.passing_threshold and consistency_score >= self.passing_threshold

        # æ„å»ºdimension_scores (å…¼å®¹æ—§æ ¼å¼)
        dimension_scores = {
            "æ–‡å­¦è´¨é‡": DimensionScore(
                dimension="æ–‡å­¦è´¨é‡",
                score=quality_score,
                reason=f"è´¨é‡è¯„åˆ†: {quality_score * 10:.1f}/10",
                suggestions=[f"è´¨é‡æ”¹è¿›: {issue}" for issue in quality_issues[:3]],
            ),
            "é€»è¾‘ä¸€è‡´æ€§": DimensionScore(
                dimension="é€»è¾‘ä¸€è‡´æ€§",
                score=consistency_score,
                reason=f"ä¸€è‡´æ€§è¯„åˆ†: {consistency_score * 10:.1f}/10",
                suggestions=[f"ä¸€è‡´æ€§ä¿®å¤: {issue}" for issue in consistency_issues[:3]],
            ),
        }

        return EvaluationResult(
            passed=passed,
            score=(quality_score + consistency_score) / 2,  # å¹³å‡åˆ†
            quality_score=quality_score,
            consistency_score=consistency_score,
            dimension_scores=dimension_scores,
            reasons=reasons,
            suggestions=suggestions,
            quality_issues=quality_issues,
            consistency_issues=consistency_issues,
            evaluator="llm_new_format",
            metadata={"task_type": task_type},
        )

    def _parse_json_format(
        self, response: str, criteria: Dict[EvaluationCriterion, float], task_type: str
    ) -> EvaluationResult:
        """è§£ææ—§JSONæ ¼å¼"""

        # ğŸ”¥ æ”¹è¿› JSON æå–æ–¹æ³•ï¼šå°è¯•å¤šç§æ–¹å¼
        data = None
        json_error = None
        original_response = response

        # æ–¹æ³•1: å°è¯•ç›´æ¥è§£ææ•´ä¸ªå“åº”ï¼ˆå¦‚æœçº¯JSONï¼‰
        try:
            data = json.loads(response.strip())
        except json.JSONDecodeError as e:
            json_error = str(e)

        # æ–¹æ³•2: å°è¯•æå– JSON ä»£ç å—
        if data is None:
            # æŸ¥æ‰¾ ```json ... ``` ä»£ç å—
            json_block_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', response, re.DOTALL)
            if json_block_match:
                try:
                    data = json.loads(json_block_match.group(1))
                    json_error = None
                except json.JSONDecodeError as e:
                    json_error = str(e)

        # æ–¹æ³•3: å°è¯•åŒ¹é…æœ€å¤–å±‚çš„ { ... } (ä½¿ç”¨æ‹¬å·åŒ¹é…)
        if data is None:
            try:
                # æ‰¾åˆ°ç¬¬ä¸€ä¸ª { å’Œå¯¹åº”çš„ }
                start = response.find("{")
                if start >= 0:
                    brace_count = 0
                    in_string = False
                    escape_next = False
                    for i in range(start, len(response)):
                        char = response[i]
                        if escape_next:
                            escape_next = False
                            continue
                        if char == "\\":
                            escape_next = True
                            continue
                        if char == '"' and not escape_next:
                            in_string = not in_string
                            continue
                        if not in_string:
                            if char == "{":
                                brace_count += 1
                            elif char == "}":
                                brace_count -= 1
                                if brace_count == 0:
                                    # æ‰¾åˆ°åŒ¹é…çš„æ‹¬å·
                                    json_str = response[start:i+1]
                                    data = json.loads(json_str)
                                    json_error = None
                                    break
            except (json.JSONDecodeError, ValueError) as e:
                json_error = str(e)

        # ğŸ”¥ å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œå°è¯•ä»åŸå§‹å“åº”ä¸­æå–æœ‰ç”¨ä¿¡æ¯
        if data is None:
            logger.warning(f"Failed to parse JSON response: {json_error}")
            logger.warning(f"Original response (first 500 chars): {response[:500]}")

            # ğŸ”¥ å°è¯•ä»é JSON æ ¼å¼ä¸­æå–å»ºè®®
            suggestions = self._extract_suggestions_from_text(response)
            reasons = self._extract_reasons_from_text(response)

            # ğŸ”¥ å°è¯•æå–è¯„åˆ†
            score_match = re.search(r'(?:è¯„åˆ†|å¾—åˆ†|åˆ†æ•°)[ï¼š:\s]*(\d+(?:\.\d+)?)[/ï¼]?(?:10|100)?', response)
            extracted_score = float(score_match.group(1)) / 10.0 if score_match else None
            overall_score = extracted_score if extracted_score else 0.7

            # ğŸ”¥ æ£€æŸ¥æ˜¯å¦æ˜ç¡®è¡¨ç¤º"éœ€è¦æ”¹è¿›"æˆ–ç±»ä¼¼è¡¨è¿°
            passed = overall_score >= self.passing_threshold
            needs_rewrite = re.search(r'(éœ€è¦æ”¹è¿›|è¯·é‡å†™|ä¸ç¬¦åˆè¦æ±‚|æœªè¾¾åˆ°æ ‡å‡†|å»ºè®®é‡æ–°)', response, re.IGNORECASE)
            if needs_rewrite and overall_score >= self.passing_threshold:
                overall_score = 0.7  # æ˜ç¡®è¡¨ç¤ºéœ€è¦æ”¹è¿›ï¼Œé™ä½åˆ†æ•°
                passed = False

            # ğŸ”¥ æ„å»ºæ›´è¯¦ç»†çš„å»ºè®®
            if not suggestions:
                if task_type == "ä¸–ç•Œè§‚è§„åˆ™":
                    suggestions = [
                        "è¯·ç¡®ä¿ä¸–ç•Œè§‚è§„åˆ™å®Œæ•´ä¸”è‡ªæ´½",
                        "åŠ›é‡ä½“ç³»è¦æœ‰æ˜ç¡®çš„è§„åˆ™å’Œé™åˆ¶",
                        "ä¸–ç•ŒèƒŒæ™¯è¦ä¸æ•…äº‹ç±»å‹ç›¸ç¬¦",
                        "é¿å…è§„åˆ™ä¹‹é—´çš„çŸ›ç›¾"
                    ]
                elif task_type == "äººç‰©è®¾è®¡":
                    suggestions = [
                        "è¯·ç¡®ä¿äººç‰©æ€§æ ¼é²œæ˜ä¸”æœ‰å‘å±•ç©ºé—´",
                        "äººç‰©èƒŒæ™¯è¦ä¸ä¸–ç•Œè§‚ç›¸ç¬¦",
                        "äººç‰©å…³ç³»è¦æ¸…æ™°åˆç†",
                        "ä¸»è§’è¦æœ‰æ˜ç¡®çš„åŠ¨æœºå’Œç›®æ ‡"
                    ]
                elif task_type == "å¤§çº²":
                    suggestions = [
                        "è¯·ç¡®ä¿å¤§çº²ç»“æ„å®Œæ•´",
                        "æƒ…èŠ‚å‘å±•è¦åˆä¹é€»è¾‘",
                        "è¦æœ‰æ˜ç¡®çš„ä¸‰å¹•ç»“æ„",
                        "é«˜æ½®éƒ¨åˆ†è¦è¶³å¤Ÿç²¾å½©"
                    ]
                else:
                    suggestions = [
                        "è¯·æ ¹æ®ä»»åŠ¡è¦æ±‚é‡æ–°ç”Ÿæˆå†…å®¹",
                        "ç¡®ä¿å†…å®¹ç¬¦åˆåˆ›ä½œç›®æ ‡",
                        "æ³¨æ„ä¸å‰é¢å†…å®¹çš„ä¸€è‡´æ€§"
                    ]

            # ğŸ”¥ ä»å“åº”ä¸­æå–å…·ä½“é—®é¢˜
            issues = self._extract_issues_from_text(response)
            quality_issues = issues.get("quality", [])
            consistency_issues = issues.get("consistency", [])

            # ğŸ”¥ æ„å»ºç»´åº¦åˆ†æ•°
            dimension_scores = {
                "quality": DimensionScore(
                    dimension="quality",
                    score=overall_score,
                    reason=reasons[0] if reasons else "åŸºäºæ–‡æœ¬å†…å®¹çš„è¯„ä¼°",
                    suggestions=suggestions,
                )
            }

            # ğŸ”¥ å¦‚æœæ²¡æœ‰æå–åˆ°è¯„åˆ†ï¼Œä½¿ç”¨é»˜è®¤åˆ†æ•°ä½†ä¸é€šè¿‡
            if extracted_score is None:
                overall_score = 0.7
                passed = False

            return EvaluationResult(
                passed=passed,
                score=overall_score,
                quality_score=overall_score,
                consistency_score=overall_score,
                dimension_scores=dimension_scores,
                reasons=reasons if reasons else ["JSONè§£æå¤±è´¥ï¼ŒåŸºäºæ–‡æœ¬å†…å®¹è¯„ä¼°"],
                suggestions=suggestions,
                quality_issues=quality_issues,
                consistency_issues=consistency_issues,
                evaluator="text_fallback",
                metadata={"task_type": task_type, "parse_error": json_error}
            )

        # JSON è§£ææˆåŠŸï¼Œç»§ç»­æ­£å¸¸æµç¨‹
        # Build dimension scores
        dimension_scores = {}
        total_score = 0.0
        total_weight = 0.0

        for criterion, weight in criteria.items():
            criterion_key = criterion.value
            if criterion_key in data.get("dimension_scores", {}):
                score_data = data["dimension_scores"][criterion_key]
                score = score_data.get("score", 70) / 100.0  # Convert to 0-1

                dimension_scores[criterion_key] = DimensionScore(
                    dimension=criterion_key,
                    score=score,
                    reason=score_data.get("reason", ""),
                    suggestions=score_data.get("suggestions", []),
                )

                total_score += score * weight
                total_weight += weight
            else:
                # Use default score if missing
                dimension_scores[criterion_key] = DimensionScore(
                    dimension=criterion_key,
                    score=0.7,
                    reason="æœªè¯„ä¼°",
                    suggestions=[],
                )
                total_score += 0.7 * weight
                total_weight += weight

        # Calculate overall score
        overall_score = total_score / total_weight if total_weight > 0 else 0.7

        # Collect reasons and suggestions
        all_reasons = data.get("overall_reasons", [])
        all_suggestions = data.get("suggestions", [])

        for dim_score in dimension_scores.values():
            if dim_score.reason:
                all_reasons.append(f"{dim_score.dimension}: {dim_score.reason}")
            all_suggestions.extend(dim_score.suggestions)

        # ğŸ”¥ æ—§æ ¼å¼ä¹Ÿä½¿ç”¨æ–°çš„é€šè¿‡æ ‡å‡†
        passed = overall_score >= self.passing_threshold

        return EvaluationResult(
            passed=passed,
            score=overall_score,
            quality_score=overall_score,  # æ—§æ ¼å¼åªæœ‰ä¸€ä¸ªåˆ†æ•°
            consistency_score=overall_score,  # å‡è®¾ä¸€è‡´æ€§ç›¸åŒ
            dimension_scores=dimension_scores,
            reasons=all_reasons,
            suggestions=all_suggestions,
            quality_issues=[],  # æ—§æ ¼å¼ä¸åŒºåˆ†
            consistency_issues=[],
            evaluator="llm_json_format",
            metadata={"task_type": task_type},
        )

    def _extract_suggestions_from_text(self, text: str) -> List[str]:
        """ğŸ”¥ ä»æ–‡æœ¬ä¸­æå–å»ºè®®"""
        suggestions = []

        # æŸ¥æ‰¾"å»ºè®®"ã€"æ”¹è¿›"ã€"ä¿®æ”¹"ç­‰å…³é”®è¯åé¢çš„å†…å®¹
        patterns = [
            r'(?:å»ºè®®|æ”¹è¿›|ä¿®æ”¹|éœ€è¦)[ï¼š:]\s*([^\n]+)',
            r'(?:å»ºè®®|æ”¹è¿›|ä¿®æ”¹)[1-9][ï¼š:]\s*([^\n]+)',
            r'[-â€¢]\s*(.*?(?:å»ºè®®|æ”¹è¿›|ä¿®æ”¹|ä¼˜åŒ–|è°ƒæ•´)[^\n]*)',
            r'>(\d+)[\.ã€]\s*([^\n]+)',  # HTML list
            r'(\d+)[\.ã€]\s*(.*?(?:å»ºè®®|æ”¹è¿›))',  # Numbered list
            r'ã€.*?ã€‘\s*([^\n]+)',  # Brackets
        ]

        for pattern in patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                if isinstance(match, tuple):
                    suggestion = match[1] if len(match) > 1 else match[0]
                else:
                    suggestion = match

                suggestion = suggestion.strip()
                if suggestion and len(suggestion) > 3 and suggestion not in suggestions:
                    # ç§»é™¤è¿‡é•¿çš„å»ºè®®
                    if len(suggestion) < 200:
                        suggestions.append(suggestion)

        return list(set(suggestions))[:10]  # å»é‡å¹¶é™åˆ¶æ•°é‡

    def _extract_reasons_from_text(self, text: str) -> List[str]:
        """ğŸ”¥ ä»æ–‡æœ¬ä¸­æå–åŸå› /è¯„ä»·"""
        reasons = []

        # æŸ¥æ‰¾è¯„ä»·æ€§è¯­å¥
        patterns = [
            r'(?:è¯„ä»·|è¯„ä¼°|æ€»ä½“|ç»“è®º)[ï¼š:]\s*([^\n]+)',
            r'(?:ä¼˜ç‚¹|äº®ç‚¹|å¥½å¤„)[ï¼š:]\s*([^\n]+)',
            r'(?:ç¼ºç‚¹|ä¸è¶³|é—®é¢˜)[ï¼š:]\s*([^\n]+)',
        ]

        for pattern in patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                reason = match.strip()
                if reason and len(reason) > 3 and reason not in reasons:
                    if len(reason) < 200:
                        reasons.append(reason)

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•æå–ç¬¬ä¸€æ®µ
        if not reasons:
            first_para = re.split(r'\n\n+', text.strip())[0]
            if len(first_para) < 200 and len(first_para) > 10:
                reasons.append(first_para)

        return list(set(reasons))[:5]

    def _extract_issues_from_text(self, text: str) -> Dict[str, List[str]]:
        """ğŸ”¥ ä»æ–‡æœ¬ä¸­æå–è´¨é‡é—®é¢˜"""
        quality_issues = []
        consistency_issues = []

        # æŸ¥æ‰¾è´¨é‡é—®é¢˜
        quality_patterns = [
            r'(?:è´¨é‡|æ–‡å­¦æ€§|æ–‡ç¬”|è¡¨è¾¾|æå†™)[^\n]*?(?:é—®é¢˜|ä¸è¶³|ç¼ºç‚¹|éœ€æ”¹è¿›)[ï¼š:]\s*([^\n]+)',
            r'(?:æ•…äº‹|æƒ…èŠ‚|äººç‰©|å¯¹è¯)[^\n]*?(?:é—®é¢˜|ä¸è¶³|ç¼ºç‚¹)[ï¼š:]\s*([^\n]+)',
        ]

        for pattern in quality_patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                issue = match.strip()
                if issue and len(issue) > 3 and issue not in quality_issues:
                    if len(issue) < 200:
                        quality_issues.append(issue)

        # æŸ¥æ‰¾ä¸€è‡´æ€§é—®é¢˜
        consistency_patterns = [
            r'(?:ä¸€è‡´æ€§|é€»è¾‘|çŸ›ç›¾|å†²çª)[^\n]*?(?:é—®é¢˜|ä¸è¶³|ç¼ºç‚¹|éœ€æ”¹è¿›)[ï¼š:]\s*([^\n]+)',
            r'(?:å‰å|çŸ›ç›¾|ä¸ç¬¦)[^\n]*?(?:é—®é¢˜|ä¸è¶³)[ï¼š:]\s*([^\n]+)',
            r'(?:äººç‰©|ä¸–ç•Œè§‚|è®¾å®š)[^\n]*?(?:ä¸ä¸€è‡´|çŸ›ç›¾|å†²çª)[^\n]*?(?:[ï¼š:])\s*([^\n]+)',
        ]

        for pattern in consistency_patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                issue = match.strip()
                if issue and len(issue) > 3 and issue not in consistency_issues:
                    if len(issue) < 200:
                        consistency_issues.append(issue)

        # æŸ¥æ‰¾é€šç”¨çš„"é—®é¢˜"åˆ—è¡¨
        problem_section = re.search(r'(?:é—®é¢˜|ä¸è¶³|ç¼ºç‚¹)[ï¼š:]\s*\n((?:[-â€¢].*\n)+)', text, re.MULTILINE)
        if problem_section:
            for line in problem_section.group(1).split('\n'):
                line = line.strip()
                if line.startswith('-') or line.startswith('â€¢'):
                    issue = line.lstrip('-â€¢*').strip()
                    if issue and len(issue) > 3:
                        # ç®€å•åˆ†ç±»ï¼šåŒ…å«"ä¸€è‡´"ã€"é€»è¾‘"ã€"çŸ›ç›¾"çš„å½’ç±»ä¸ºä¸€è‡´æ€§é—®é¢˜
                        if any(kw in issue for kw in ['ä¸€è‡´', 'é€»è¾‘', 'çŸ›ç›¾', 'å‰å', 'å†²çª']):
                            if issue not in consistency_issues:
                                consistency_issues.append(issue)
                        else:
                            if issue not in quality_issues:
                                quality_issues.append(issue)

        return {
            "quality": list(set(quality_issues))[:5],
            "consistency": list(set(consistency_issues))[:5]
        }

    async def _rule_based_evaluate(
        self,
        task_type: str,
        content: str,
        criteria: Dict[EvaluationCriterion, float],
        context: Optional[Dict[str, Any]] = None,
    ) -> EvaluationResult:
        """
        Perform rule-based evaluation (fallback)

        Uses simple heuristics to assess content quality
        """
        dimension_scores = {}
        total_score = 0.0
        total_weight = 0.0

        content_length = len(content)
        word_count = len(content.split())

        # Coherence: Check for reasonable length and structure
        coherence_score = min(1.0, min(content_length / 500, 1.0))
        if content_length < 100:
            coherence_score = 0.5
        dimension_scores[EvaluationCriterion.COHERENCE.value] = DimensionScore(
            dimension=EvaluationCriterion.COHERENCE.value,
            score=coherence_score,
            reason=f"å†…å®¹é•¿åº¦: {content_length}å­—ç¬¦",
            suggestions=["å†…å®¹è¿‡çŸ­ï¼Œå»ºè®®æ‰©å±•" if content_length < 200 else "é•¿åº¦é€‚ä¸­"],
        )
        total_score += coherence_score * criteria.get(EvaluationCriterion.COHERENCE, 0.2)
        total_weight += criteria.get(EvaluationCriterion.COHERENCE, 0.2)

        # Creativity: Check for variety in vocabulary
        unique_words = len(set(content.split()))
        vocabulary_diversity = unique_words / max(word_count, 1)
        creativity_score = min(1.0, vocabulary_diversity * 1.5)
        dimension_scores[EvaluationCriterion.CREATIVITY.value] = DimensionScore(
            dimension=EvaluationCriterion.CREATIVITY.value,
            score=creativity_score,
            reason=f"è¯æ±‡å¤šæ ·æ€§: {vocabulary_diversity:.2f}",
            suggestions=["å¢åŠ è¯æ±‡ä¸°å¯Œåº¦" if vocabulary_diversity < 0.5 else "è¯æ±‡ä¸°å¯Œåº¦è‰¯å¥½"],
        )
        total_score += creativity_score * criteria.get(EvaluationCriterion.CREATIVITY, 0.2)
        total_weight += criteria.get(EvaluationCriterion.CREATIVITY, 0.2)

        # Quality: Check for basic grammar indicators
        quality_score = 0.8  # Default good score
        issues = []
        if content.count("ã€‚") < content_length / 200:
            issues.append("å¥å­ç»“å°¾æ ‡ç‚¹å¯èƒ½ä¸è¶³")
            quality_score -= 0.1
        if content.count("\n") < content_length / 1000:
            issues.append("æ®µè½åˆ’åˆ†å¯èƒ½ä¸è¶³")
            quality_score -= 0.1
        dimension_scores[EvaluationCriterion.QUALITY.value] = DimensionScore(
            dimension=EvaluationCriterion.QUALITY.value,
            score=max(0.5, quality_score),
            reason="åŸºç¡€æ ¼å¼æ£€æŸ¥",
            suggestions=issues if issues else ["æ ¼å¼è‰¯å¥½"],
        )
        total_score += quality_score * criteria.get(EvaluationCriterion.QUALITY, 0.2)
        total_weight += criteria.get(EvaluationCriterion.QUALITY, 0.2)

        # Consistency and goal alignment: Default scores
        # ğŸ”¥ æ ¹æ®ä»»åŠ¡ç±»å‹æä¾›æ›´å…·ä½“çš„å»ºè®®
        task_specific_suggestions = self._get_task_specific_suggestions(task_type, content)

        for criterion in [EvaluationCriterion.CONSISTENCY, EvaluationCriterion.GOAL_ALIGNMENT]:
            if criterion in criteria:
                default_score = 0.7
                dimension_scores[criterion.value] = DimensionScore(
                    dimension=criterion.value,
                    score=default_score,
                    reason="åŸºäºè§„åˆ™çš„é»˜è®¤è¯„ä¼°",
                    suggestions=task_specific_suggestions,  # ğŸ”¥ ä½¿ç”¨ä»»åŠ¡ç‰¹å®šçš„å»ºè®®
                )
                total_score += default_score * criteria[criterion]
                total_weight += criteria[criterion]

        # Calculate overall score
        overall_score = total_score / total_weight if total_weight > 0 else 0.7

        all_reasons = [f"{d.dimension}: {d.reason}" for d in dimension_scores.values()]
        all_suggestions = []
        for d in dimension_scores.values():
            all_suggestions.extend(d.suggestions)

        # ğŸ”¥ æ›´æ–°ï¼šä½¿ç”¨æ–°çš„0.8é˜ˆå€¼å’Œåˆ†åˆ«çš„è´¨é‡/ä¸€è‡´æ€§è¯„åˆ†
        # è§„åˆ™è¯„ä¼°æ— æ³•åŒºåˆ†è´¨é‡å’Œä¸€è‡´æ€§ï¼Œä½¿ç”¨ç›¸åŒåˆ†æ•°
        return EvaluationResult(
            passed=overall_score >= self.passing_threshold,  # ğŸ”¥ ä½¿ç”¨é…ç½®çš„é˜ˆå€¼
            score=overall_score,
            quality_score=overall_score,
            consistency_score=overall_score,
            dimension_scores=dimension_scores,
            reasons=all_reasons,
            suggestions=all_suggestions,
            quality_issues=[],
            consistency_issues=[],
            evaluator="rule_based",
            metadata={"task_type": task_type},
        )

    def _get_task_evaluation_criteria(self, task_type: str) -> str:
        """
        ğŸ”¥ æ ¹æ®ä»»åŠ¡ç±»å‹è¿”å›å…·ä½“çš„è¯„ä¼°è¦ç‚¹
        è¿™äº›è¦ç‚¹ä¼šåŒ…å«åœ¨è¯„ä¼°æç¤ºè¯ä¸­ï¼Œå¼•å¯¼LLMç”Ÿæˆæ›´å…·ä½“çš„å»ºè®®
        """
        criteria_map = {
            "åˆ›æ„è„‘æš´": """
**è¯·é‡ç‚¹è¯„ä¼°ä»¥ä¸‹æ–¹é¢**ï¼š
1. **ä¸–ç•Œè§‚ä¸€è‡´æ€§**ï¼ˆæœ€é‡è¦ï¼ï¼‰ï¼šæ˜¯å¦éµå®ˆç”¨æˆ·è¾“å…¥ä¸­çš„æ—¶ä»£/æœä»£è®¾å®š
   - â›” ä¸¥é‡é”™è¯¯ï¼šå®‹æœå‡ºç°ç§¦å§‹çš‡ã€å”æœå‡ºç°æœ±å…ƒç’‹ç­‰è·¨æ—¶ä»£äººç‰©
   - â›” ä¸¥é‡é”™è¯¯ï¼šç°ä»£éƒ½å¸‚è®¾å®šå‡ºç°å¤ä»£äººç‰©
   - â›” ä¸¥é‡é”™è¯¯ï¼šæ˜ç¡®æœä»£è®¾å®šä¸‹å‡ºç°å…¶ä»–æœä»£æ ‡å¿—æ€§å…ƒç´ 
   - âœ… æ­£ç¡®ï¼šå®‹æœæ•…äº‹åªæœ‰å®‹æœäººç‰©å’Œäº‹ä»¶
   - âœ… æ­£ç¡®ï¼šç°ä»£éƒ½å¸‚æ•…äº‹ç¬¦åˆç°ä»£è®¾å®š

2. **åˆ›æ„ç‹¬ç‰¹æ€§**ï¼šç‚¹å­æ˜¯å¦æœ‰æ–°æ„ï¼Œé¿å…å¥—è·¯å’Œä¿—å¥—
   - æ˜¯å¦æ‹’ç»"åºŸæŸ´é€†è¢­""é€€å©šæµ"ç­‰è€å¥—è·¯
   - æ¯ä¸ªç‚¹å­æ˜¯å¦æœ‰è‡ªå·±çš„"çµé­‚"

3. **ç‚¹å­å¤šæ ·æ€§**ï¼šå¤šä¸ªç‚¹å­ä¹‹é—´æ˜¯å¦æœ‰æ˜æ˜¾å·®å¼‚
   - é£æ ¼æ˜¯å¦é›·åŒ
   - äººç‰©åŠ¨æœºæ˜¯å¦å¤æ‚
   - å†²çªæ˜¯å¦æ–°é¢–

4. **å®Œæ•´æ€§**ï¼šæ¯ä¸ªç‚¹å­æ˜¯å¦åŒ…å«æ ¸å¿ƒæ¦‚å¿µã€å†²çªã€æƒ…æ„Ÿé’©å­
5. **å¯è¡Œæ€§**ï¼šåˆ›æ„æ˜¯å¦å¯ä»¥è¢«æ‰©å±•æˆå®Œæ•´æ•…äº‹

**å¸¸è§ä¸¥é‡é—®é¢˜**ï¼ˆç›´æ¥åˆ¤ä¸ºä¸é€šè¿‡ï¼‰ï¼š
- âŒ è·¨æ—¶ä»£é”™è¯¯ï¼šå®‹æœæ•…äº‹å‡ºç°ç§¦å§‹çš‡ã€å”æœå‡ºç°æœ±å…ƒç’‹
- âŒ å¥—è·¯é‡å¤ï¼š3ä¸ªç‚¹å­éƒ½æ˜¯"åºŸæŸ´é€†è¢­""æ‰“è„¸å¤ä»‡"
- âŒ è®¾å®šçŸ›ç›¾ï¼šç°ä»£éƒ½å¸‚çªç„¶å‡ºç°ä¿®ä»™å…ƒç´ ä¸”æ— è§£é‡Š
""",
            "ä¸–ç•Œè§‚è§„åˆ™": """
**è¯·é‡ç‚¹è¯„ä¼°ä»¥ä¸‹æ–¹é¢**ï¼š
1. **å®Œæ•´æ€§**ï¼šä¸–ç•Œè§‚æ˜¯å¦åŒ…å«æ ¸å¿ƒè¦ç´ ï¼ˆåŠ›é‡ä½“ç³»ã€ä¸–ç•ŒèƒŒæ™¯ã€ç¤¾ä¼šè§„åˆ™ç­‰ï¼‰
2. **è‡ªæ´½æ€§**ï¼šè§„åˆ™ä¹‹é—´æ˜¯å¦å­˜åœ¨é€»è¾‘çŸ›ç›¾
3. **æ•…äº‹æ€§**ï¼šä¸–ç•Œè§‚æ˜¯å¦æœåŠ¡äºæ•…äº‹å‘å±•ï¼Œè€Œéçº¯è®¾å®šå †ç Œ
4. **åˆ›æ–°æ€§**ï¼šæ˜¯å¦æœ‰ç‹¬ç‰¹çš„åˆ›æ„ï¼Œè€Œéå¥—ç”¨å¸¸è§å¥—è·¯
5. **å¯æ‰§è¡Œæ€§**ï¼šè®¾å®šæ˜¯å¦å¯ä»¥è¢«åç»­å†…å®¹å®é™…æ‰§è¡Œ

**å¸¸è§é—®é¢˜ç¤ºä¾‹**ï¼š
- âŒ "åŠ›é‡ä½“ç³»å¾ˆå¼ºå¤§" â†’ âœ… "åŠ›é‡ä½“ç³»åˆ†5ä¸ªç­‰çº§ï¼Œæ¯çº§çš„èƒ½åŠ›å’Œé™åˆ¶éƒ½å¾ˆæ˜ç¡®"
- âŒ "ä¸–ç•Œè§‚å®Œæ•´" â†’ âœ… "åŒ…å«3ä¸ªå¤§é™†ã€5ä¸ªå›½å®¶ã€2ç§åŠ›é‡ä½“ç³»"
""",
            "äººç‰©è®¾è®¡": """
**è¯·é‡ç‚¹è¯„ä¼°ä»¥ä¸‹æ–¹é¢**ï¼š
1. **ç«‹ä½“æ€§**ï¼šäººç‰©æ˜¯å¦æœ‰é²œæ˜çš„æ€§æ ¼ã€åŠ¨æœºã€ç›®æ ‡
2. **å‘å±•ç©ºé—´**ï¼šäººç‰©æ˜¯å¦æœ‰æˆé•¿å’Œå˜åŒ–çš„å¯èƒ½æ€§
3. **ç‹¬ç‰¹æ€§**ï¼šäººç‰©æ˜¯å¦æœ‰ç‹¬ç‰¹çš„ç‰¹è´¨ï¼Œè€Œéåˆ»æ¿å°è±¡
4. **ä¸ä¸–ç•Œè§‚çš„å…³ç³»**ï¼šäººç‰©èƒŒæ™¯æ˜¯å¦ç¬¦åˆä¸–ç•Œè§‚è®¾å®š
5. **äººç‰©å…³ç³»**ï¼šäººç‰©ä¹‹é—´çš„å…³ç³»æ˜¯å¦æ¸…æ™°åˆç†

**å¸¸è§é—®é¢˜ç¤ºä¾‹**ï¼š
- âŒ "ä¸»è§’å¾ˆå‹‡æ•¢" â†’ âœ… "ä¸»è§’ä¸ºäº†æ‹¯æ•‘å¦¹å¦¹è€Œå…‹æœææƒ§ï¼Œå±•ç°äº†å†…åœ¨å†²çª"
- âŒ "äººç‰©å½¢è±¡é²œæ˜" â†’ âœ… "ä¸»è§’å¤–è¡¨å†·æ¼ ä½†å†…å¿ƒå–„è‰¯ï¼Œé€šè¿‡å†’é™©é€æ¸æ‰“å¼€å¿ƒæ‰‰"
""",
            "å¤§çº²": """
**è¯·é‡ç‚¹è¯„ä¼°ä»¥ä¸‹æ–¹é¢**ï¼š
1. **ç»“æ„å®Œæ•´æ€§**ï¼šæ˜¯å¦æœ‰æ¸…æ™°çš„å¼€ç«¯ã€å‘å±•ã€é«˜æ½®ã€ç»“å±€
2. **æƒ…èŠ‚é€»è¾‘**ï¼šäº‹ä»¶ä¹‹é—´æ˜¯å¦æœ‰åˆç†çš„å› æœå…³ç³»
3. **èŠ‚å¥æ§åˆ¶**ï¼šæƒ…èŠ‚å‘å±•æ˜¯å¦å¼ å¼›æœ‰åº¦
4. **å¸å¼•åŠ›**ï¼šæ˜¯å¦èƒ½æŒç»­å¸å¼•è¯»è€…é˜…è¯»
5. **å¯æ‰§è¡Œæ€§**ï¼šå¤§çº²æ˜¯å¦å¯ä»¥è¢«æ‹†åˆ†æˆå…·ä½“çš„ç« èŠ‚

**å¸¸è§é—®é¢˜ç¤ºä¾‹**ï¼š
- âŒ "æƒ…èŠ‚æœ‰è¶£" â†’ âœ… "ç¬¬3ç« çš„æ‚¬å¿µè®¾è®¡å¾ˆå¥½ï¼Œä½†ç¬¬5-7ç« èŠ‚å¥è¿‡æ…¢"
- âŒ "ç»“æ„å®Œæ•´" â†’ âœ… "ä¸‰å¹•ç»“æ„æ¸…æ™°ï¼Œä½†é«˜æ½®éƒ¨åˆ†å¯ä»¥æ›´æ¿€çƒˆ"
""",
            "äº‹ä»¶": """
**è¯·é‡ç‚¹è¯„ä¼°ä»¥ä¸‹æ–¹é¢**ï¼š
1. **å› æœå…³ç³»**ï¼šäº‹ä»¶çš„å‘ç”Ÿæ˜¯å¦æœ‰åˆç†çš„èµ·å› 
2. **å½±å“æ·±åº¦**ï¼šäº‹ä»¶æ˜¯å¦å¯¹åç»­æƒ…èŠ‚äº§ç”Ÿå®è´¨æ€§å½±å“
3. **å†²çªæ€§**ï¼šäº‹ä»¶æ˜¯å¦åŒ…å«è¶³å¤Ÿçš„æˆå‰§å†²çª
4. **ä¸äººç‰©çš„å…³ç³»**ï¼šäº‹ä»¶æ˜¯å¦æ¨åŠ¨äººç‰©å‘å±•
5. **ä¸ä¸–ç•Œè§‚çš„å…³ç³»**ï¼šäº‹ä»¶æ˜¯å¦ç¬¦åˆä¸–ç•Œè§‚è§„åˆ™

**å¸¸è§é—®é¢˜ç¤ºä¾‹**ï¼š
- âŒ "äº‹ä»¶ç²¾å½©" â†’ âœ… "ä¸»è§’ä¸åæ´¾çš„åˆæ¬¡å†²çªæš´éœ²äº†åŠ›é‡ä½“ç³»çš„é™åˆ¶è§„åˆ™"
- âŒ "æƒ…èŠ‚åˆç†" â†’ âœ… "äº‹ä»¶Aå¯¼è‡´äº†äº‹ä»¶Bï¼Œç¬¦åˆäººç‰©æ€§æ ¼å’Œä¸–ç•Œè§‚è®¾å®š"
""",
            "ä¼ç¬”åˆ—è¡¨": """
**è¯·é‡ç‚¹è¯„ä¼°ä»¥ä¸‹æ–¹é¢**ï¼š
1. **é‡è¦æ€§**ï¼šä¼ç¬”æ˜¯å¦ä¸ä¸»çº¿å‰§æƒ…ç›¸å…³
2. **éšè”½æ€§**ï¼šä¼ç¬”æ˜¯å¦è¶³å¤Ÿéšè”½ï¼Œä¸ä¼šè¿‡æ—©æš´éœ²
3. **å¯å›æ”¶æ€§**ï¼šä¼ç¬”æ˜¯å¦æœ‰æ˜ç¡®çš„å›æ”¶è®¡åˆ’
4. **åˆç†æ€§**ï¼šä¼ç¬”æ˜¯å¦ç¬¦åˆæ•…äº‹é€»è¾‘
5. **åˆ†å¸ƒæ€§**ï¼šä¼ç¬”æ˜¯å¦åˆç†åˆ†å¸ƒï¼Œè€Œéé›†ä¸­å †ç Œ

**å¸¸è§é—®é¢˜ç¤ºä¾‹**ï¼š
- âŒ "ä¼ç¬”å·§å¦™" â†’ âœ… "ç¬¬3ç« æåˆ°çš„ç¥ç§˜ç‰©å“å°†åœ¨ç¬¬15ç« æ­ç¤ºå…¶çœŸæ­£ç”¨é€”"
- âŒ "ä¼ç¬”åˆç†" â†’ âœ… "ä¸»è§’èº«ä¸–çš„çº¿ç´¢åˆ†å¸ƒåœ¨ç¬¬5ã€8ã€12ç« ï¼Œé€æ­¥æ­ç¤º"
""",
            "ç« èŠ‚å†…å®¹": """
**è¯·é‡ç‚¹è¯„ä¼°ä»¥ä¸‹æ–¹é¢**ï¼š
1. **ä¸å¤§çº²çš„ä¸€è‡´æ€§**ï¼šå†…å®¹æ˜¯å¦ç¬¦åˆç« èŠ‚å¤§çº²è¦æ±‚
2. **åœºæ™¯æå†™**ï¼šæ˜¯å¦æœ‰ç”ŸåŠ¨çš„åœºæ™¯æå†™
3. **å¯¹è¯è´¨é‡**ï¼šå¯¹è¯æ˜¯å¦ç¬¦åˆäººç‰©æ€§æ ¼
4. **æƒ…èŠ‚æ¨è¿›**ï¼šæ˜¯å¦æœ‰æ•ˆæ¨è¿›æ•…äº‹å‘å±•
5. **æ–‡ç¬”è´¨é‡**ï¼šè¯­è¨€æ˜¯å¦æµç•…ã€æœ‰æ„ŸæŸ“åŠ›

**å¸¸è§é—®é¢˜ç¤ºä¾‹**ï¼š
- âŒ "å†…å®¹ç²¾å½©" â†’ âœ… "æˆ˜æ–—åœºæ™¯æå†™ç´§å¼ æ¿€çƒˆï¼Œä½†äººç‰©å¯¹è¯ç•¥æ˜¾ç”Ÿç¡¬"
- âŒ "æ–‡ç¬”æµç•…" â†’ âœ… "ç¯å¢ƒæå†™ç»†è‡´ï¼Œä½†èŠ‚å¥ç•¥æ…¢ï¼Œå»ºè®®ç²¾ç®€ä¸­é—´éƒ¨åˆ†"
""",
            "ç« èŠ‚æ¶¦è‰²": """
**è¯·é‡ç‚¹è¯„ä¼°ä»¥ä¸‹æ–¹é¢**ï¼š
1. **æ”¹è¿›æ•ˆæœ**ï¼šæ¶¦è‰²æ˜¯å¦è§£å†³äº†åˆç¨¿çš„é—®é¢˜
2. **è¯­è¨€æµç•…åº¦**ï¼šè¯­è¨€æ˜¯å¦æ›´åŠ æµç•…è‡ªç„¶
3. **ç»†èŠ‚ä¸°å¯Œåº¦**ï¼šæ˜¯å¦å¢åŠ äº†æœ‰ä»·å€¼çš„ç»†èŠ‚
4. **æƒ…æ„Ÿå…±é¸£**ï¼šæ˜¯å¦å¢å¼ºäº†è¯»è€…çš„æƒ…æ„Ÿä½“éªŒ
5. **ä¸€è‡´æ€§**ï¼šæ˜¯å¦ä¿æŒäº†ä¸å‰æ–‡çš„ä¸€è‡´æ€§

**å¸¸è§é—®é¢˜ç¤ºä¾‹**ï¼š
- âŒ "æ¶¦è‰²æœ‰æ•ˆ" â†’ âœ… "å¢åŠ äº†ä¸»è§’å†…å¿ƒçš„çŸ›ç›¾æå†™ï¼Œä½¿äººç‰©æ›´ç«‹ä½“"
- âŒ "æ–‡ç¬”æå‡" â†’ âœ… "å¯¹è¯æ›´è‡ªç„¶äº†ï¼Œä½†éƒ¨åˆ†åœºæ™¯æå†™ä»éœ€åŠ å¼º"
""",
            "é£æ ¼å…ƒç´ ": """
**è¯·é‡ç‚¹è¯„ä¼°ä»¥ä¸‹æ–¹é¢**ï¼š
1. **ç‹¬ç‰¹æ€§**ï¼šé£æ ¼å…ƒç´ æ˜¯å¦å…·æœ‰è¾¨è¯†åº¦
2. **ä¸€è‡´æ€§**ï¼šé£æ ¼æ˜¯å¦åœ¨å…¨ä¹¦ä¸­ä¿æŒä¸€è‡´
3. **é€‚é…æ€§**ï¼šé£æ ¼æ˜¯å¦é€‚åˆæ•…äº‹ç±»å‹å’Œç›®æ ‡è¯»è€…
4. **æ‰§è¡Œæ€§**ï¼šé£æ ¼å…ƒç´ æ˜¯å¦èƒ½åœ¨åç»­å†…å®¹ä¸­æŒç»­æ‰§è¡Œ

**å¸¸è§é—®é¢˜ç¤ºä¾‹**ï¼š
- âŒ "é£æ ¼ç‹¬ç‰¹" â†’ âœ… "ä½¿ç”¨ç¬¬ä¸€äººç§°è§†è§’ï¼Œè¯­è¨€é£æ ¼åå‘é»‘è‰²å¹½é»˜"
- âŒ "é£æ ¼ä¸€è‡´" â†’ âœ… "é‡‡ç”¨å†·ç¡¬æ´¾ä¾¦æ¢å°è¯´é£æ ¼ï¼ŒçŸ­å¥ä¸ºä¸»ï¼Œå°‘ç”¨å½¢å®¹è¯"
""",
            "ä¸»é¢˜ç¡®è®¤": """
**è¯·é‡ç‚¹è¯„ä¼°ä»¥ä¸‹æ–¹é¢**ï¼š
1. **æ¸…æ™°åº¦**ï¼šä¸»é¢˜æ˜¯å¦æ¸…æ™°æ˜ç¡®
2. **æ·±åº¦**ï¼šä¸»é¢˜æ˜¯å¦æœ‰æ·±åº¦å’Œæ€è€ƒä»·å€¼
3. **å¯æ‰§è¡Œæ€§**ï¼šä¸»é¢˜æ˜¯å¦èƒ½åœ¨æ•…äº‹ä¸­å¾—åˆ°ä½“ç°
4. **ä¸€è‡´æ€§**ï¼šä¸»é¢˜æ˜¯å¦ä¸ä¸–ç•Œè§‚ã€äººç‰©ã€æƒ…èŠ‚ç›¸ä¸€è‡´

**å¸¸è§é—®é¢˜ç¤ºä¾‹**ï¼š
- âŒ "ä¸»é¢˜æ˜ç¡®" â†’ âœ… "ä¸»é¢˜æ˜¯'ç§‘æŠ€å‘å±•ä¸äººæ€§çš„å†²çª'ï¼Œé€šè¿‡ä¸»è§’çš„ä¸¤éš¾é€‰æ‹©ä½“ç°"
- âŒ "ä¸»é¢˜æ·±åˆ»" â†’ âœ… "æ¢è®¨äº†äººå·¥æ™ºèƒ½æ˜¯å¦åº”è¯¥æ‹¥æœ‰æƒåˆ©ï¼Œå…·æœ‰ç°å®æ„ä¹‰"
""",
            "åœºæ™¯ç‰©å“": """
**è¯·é‡ç‚¹è¯„ä¼°ä»¥ä¸‹æ–¹é¢**ï¼š
1. **åŠŸèƒ½æ€§**ï¼šåœºæ™¯/ç‰©å“æ˜¯å¦æœåŠ¡äºæƒ…èŠ‚æˆ–äººç‰©å‘å±•
2. **æå†™è´¨é‡**ï¼šæå†™æ˜¯å¦ç”ŸåŠ¨å…·ä½“
3. **ä¸€è‡´æ€§**ï¼šæ˜¯å¦ç¬¦åˆä¸–ç•Œè§‚è®¾å®š
4. **è±¡å¾æ„ä¹‰**ï¼šæ˜¯å¦å…·æœ‰è±¡å¾æˆ–éšå–»æ„ä¹‰

**å¸¸è§é—®é¢˜ç¤ºä¾‹**ï¼š
- âŒ "åœºæ™¯æå†™å¥½" â†’ âœ… "åºŸå¼ƒå®éªŒå®¤åœºæ™¯è¥é€ äº†æ‚¬ç–‘æ°›å›´ï¼Œæš—ç¤ºäº†ç§‘æŠ€çš„å±é™©"
- âŒ "ç‰©å“è®¾å®šæœ‰è¶£" â†’ âœ… "ç¥ç§˜æ€€è¡¨ä¸ä»…æ˜¯å…³é”®é“å…·ï¼Œä¹Ÿè±¡å¾ç€æ—¶é—´çš„æµé€"
""",
            "å¯¹è¯æ£€æŸ¥": """
**è¯·é‡ç‚¹è¯„ä¼°ä»¥ä¸‹æ–¹é¢**ï¼š
1. **äººç‰©å£°éŸ³**ï¼šæ¯ä¸ªè§’è‰²çš„å¯¹è¯æ˜¯å¦ç¬¦åˆå…¶æ€§æ ¼
2. **è‡ªç„¶åº¦**ï¼šå¯¹è¯æ˜¯å¦è‡ªç„¶æµç•…ï¼ŒåƒçœŸå®å¯¹è¯
3. **ä¿¡æ¯é‡**ï¼šå¯¹è¯æ˜¯å¦ä¼ è¾¾äº†å¿…è¦ä¿¡æ¯
4. **æ¨åŠ¨ä½œç”¨**ï¼šå¯¹è¯æ˜¯å¦æ¨åŠ¨æƒ…èŠ‚æˆ–å±•ç°äººç‰©

**å¸¸è§é—®é¢˜ç¤ºä¾‹**ï¼š
- âŒ "å¯¹è¯è‡ªç„¶" â†’ âœ… "ä¸»è§’çš„å£è¯­åŒ–è¡¨è¾¾ç¬¦åˆå…¶å‡ºèº«ï¼Œä½†åæ´¾è¿‡äºæ­£å¼"
- âŒ "å¯¹è¯ç”ŸåŠ¨" â†’ âœ… "å¯¹è¯å±•ç°äº†äººç‰©å…³ç³»ï¼Œä½†éƒ¨åˆ†å°è¯ç•¥æ˜¾ç”Ÿç¡¬"
""",
        }

        return criteria_map.get(task_type, """
**è¯·é‡ç‚¹è¯„ä¼°ä»¥ä¸‹æ–¹é¢**ï¼š
1. **å®Œæ•´æ€§**ï¼šå†…å®¹æ˜¯å¦å®Œæ•´
2. **å‡†ç¡®æ€§**ï¼šå†…å®¹æ˜¯å¦ç¬¦åˆä»»åŠ¡è¦æ±‚
3. **è´¨é‡**ï¼šå†…å®¹è´¨é‡æ˜¯å¦è¾¾åˆ°å¯æ¥å—æ°´å¹³
4. **ä¸€è‡´æ€§**ï¼šå†…å®¹æ˜¯å¦ä¸å‰é¢å†…å®¹ä¿æŒä¸€è‡´
5. **å¯æ‰§è¡Œæ€§**ï¼šå†…å®¹æ˜¯å¦å¯ä»¥è¢«åç»­å†…å®¹ä½¿ç”¨

**è¯·æä¾›å…·ä½“çš„ã€å¯æ“ä½œçš„å»ºè®®**ï¼Œé¿å…æ¨¡ç³Šçš„è¯„ä»·ã€‚
""")

    def _get_task_specific_suggestions(self, task_type: str, content: str) -> List[str]:
        """ğŸ”¥ æ ¹æ®ä»»åŠ¡ç±»å‹è·å–å…·ä½“çš„æ”¹è¿›å»ºè®®ï¼ˆä»…ç”¨äºè§„åˆ™è¯„ä¼°å›é€€ï¼‰"""
        content_lower = content.lower()
        suggestions = []

        if task_type == "ä¸–ç•Œè§‚è§„åˆ™":
            suggestions = [
                "è¯·ç¡®ä¿ä¸–ç•Œè§‚è§„åˆ™å®Œæ•´ä¸”è‡ªæ´½",
                "åŠ›é‡ä½“ç³»è¦æœ‰æ˜ç¡®çš„è§„åˆ™å’Œé™åˆ¶",
                "ä¸–ç•ŒèƒŒæ™¯è¦ä¸æ•…äº‹ç±»å‹ç›¸ç¬¦",
                "é¿å…è§„åˆ™ä¹‹é—´çš„çŸ›ç›¾"
            ]
            # æ£€æŸ¥å†…å®¹æ˜¯å¦è¿‡çŸ­
            if len(content) < 300:
                suggestions.append("ä¸–ç•Œè§‚è§„åˆ™è¿‡äºç®€å•ï¼Œè¯·è¯¦ç»†å±•å¼€")
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®è¦ç´ 
            required_keywords = ["åŠ›é‡", "è§„åˆ™", "é™åˆ¶", "ä¸–ç•Œ"]
            for keyword in required_keywords:
                if keyword not in content_lower:
                    suggestions.append(f"è¯·è¡¥å……å…³äº{keyword}çš„è®¾å®š")
                    break

        elif task_type == "äººç‰©è®¾è®¡":
            suggestions = [
                "è¯·ç¡®ä¿äººç‰©æ€§æ ¼é²œæ˜ä¸”æœ‰å‘å±•ç©ºé—´",
                "äººç‰©èƒŒæ™¯è¦ä¸ä¸–ç•Œè§‚ç›¸ç¬¦",
                "äººç‰©å…³ç³»è¦æ¸…æ™°åˆç†",
                "ä¸»è§’è¦æœ‰æ˜ç¡®çš„åŠ¨æœºå’Œç›®æ ‡"
            ]
            if len(content) < 300:
                suggestions.append("äººç‰©æè¿°è¿‡äºç®€å•ï¼Œè¯·è¡¥å……æ›´å¤šç»†èŠ‚")
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®è¦ç´ 
            required_keywords = ["æ€§æ ¼", "èƒŒæ™¯", "åŠ¨æœº", "ç›®æ ‡"]
            for keyword in required_keywords:
                if keyword not in content_lower:
                    suggestions.append(f"è¯·è¡¥å……äººç‰©çš„{keyword}")
                    break

        elif task_type == "å¤§çº²":
            suggestions = [
                "è¯·ç¡®ä¿å¤§çº²ç»“æ„å®Œæ•´",
                "æƒ…èŠ‚å‘å±•è¦åˆä¹é€»è¾‘",
                "è¦æœ‰æ˜ç¡®çš„ä¸‰å¹•ç»“æ„",
                "é«˜æ½®éƒ¨åˆ†è¦è¶³å¤Ÿç²¾å½©"
            ]
            # æ£€æŸ¥ç« èŠ‚ç»“æ„
            chapter_count = len(re.findall(r"ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« ", content))
            if chapter_count < 3:
                suggestions.append("å¤§çº²ç»“æ„è¿‡äºç®€å•ï¼Œè¯·è¯¦ç»†è§„åˆ’å„ç« èŠ‚å†…å®¹")

        elif task_type == "äº‹ä»¶":
            suggestions = [
                "è¯·ç¡®ä¿äº‹ä»¶ä¸ä¸–ç•Œè§‚å’Œäººç‰©ç›¸ç¬¦",
                "äº‹ä»¶è¦æœ‰èµ·æ‰¿è½¬åˆ",
                "äº‹ä»¶ä¹‹é—´è¦æœ‰é€»è¾‘å…³è”",
                "é‡è¦äº‹ä»¶è¦æœ‰ä¼ç¬”é“ºå«"
            ]

        elif task_type == "ä¼ç¬”åˆ—è¡¨":
            suggestions = [
                "ä¼ç¬”è¦ä¸ä¸»çº¿å‰§æƒ…ç›¸å…³",
                "ä¼ç¬”è¦æœ‰æ˜ç¡®çš„å›æ”¶è®¡åˆ’",
                "ä¼ç¬”åˆ†å¸ƒè¦åˆç†ï¼Œä¸è¦é›†ä¸­",
                "é‡è¦ä¼ç¬”è¦åœ¨æ—©æœŸåŸ‹è®¾"
            ]

        elif task_type in ["ç« èŠ‚å†…å®¹", "ç« èŠ‚æ¶¦è‰²"]:
            suggestions = [
                "è¯·ç¡®ä¿ç« èŠ‚å†…å®¹ä¸å¤§çº²ä¸€è‡´",
                "æ³¨æ„äººç‰©æ€§æ ¼å’Œå¯¹è¯çš„ä¸€è‡´æ€§",
                "åœºæ™¯æå†™è¦ç”ŸåŠ¨å…·ä½“",
                "ç« èŠ‚ç»“å°¾è¦æœ‰å¸å¼•åŠ›"
            ]

        else:
            suggestions = [
                "è¯·æ ¹æ®ä»»åŠ¡è¦æ±‚é‡æ–°ç”Ÿæˆå†…å®¹",
                "ç¡®ä¿å†…å®¹ç¬¦åˆåˆ›ä½œç›®æ ‡",
                "æ³¨æ„ä¸å‰é¢å†…å®¹çš„ä¸€è‡´æ€§"
            ]

        return suggestions[:8]  # é™åˆ¶æ•°é‡

    def set_passing_threshold(self, threshold: float) -> None:
        """Update the passing threshold"""
        self.passing_threshold = max(0.0, min(1.0, threshold))
        logger.info(f"Updated passing threshold to {self.passing_threshold}")

    def set_criteria(
        self,
        criteria: Dict[EvaluationCriterion, float],
    ) -> None:
        """Update evaluation criteria"""
        # Validate weights sum to approximately 1.0
        total_weight = sum(criteria.values())
        if abs(total_weight - 1.0) > 0.1:
            logger.warning(
                f"Criteria weights sum to {total_weight}, "
                f"expected ~1.0. Normalizing..."
            )
            # Normalize
            self.criteria = {
                k: v / total_weight for k, v in criteria.items()
            }
        else:
            self.criteria = criteria.copy()

        logger.info(f"Updated evaluation criteria: {list(self.criteria.keys())}")
